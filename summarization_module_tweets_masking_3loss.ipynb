{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28226,
     "status": "ok",
     "timestamp": 1763430836984,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "2GeGZw0S5ENK",
    "outputId": "5ada3356-5561-4164-8cc1-c77845a50c15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import os\n",
    "!pip install nltk tqdm\n",
    "import nltk, re, pandas as pd\n",
    "from tqdm import tqdm\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34906,
     "status": "ok",
     "timestamp": 1763430871897,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "OoyiopfW9r_G",
    "outputId": "65933253-03e4-4b3d-83bb-3cba2ff478ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "folder_path = '/content/drive/MyDrive/CS4650/project'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ozIPOsns58-0"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"\") #removed out for security"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bM_WKb9fOzIY"
   },
   "outputs": [],
   "source": [
    "device = 0 if torch.cuda.is_available() else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KEU7laYAfNuU"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6VA9msn3cD_m"
   },
   "source": [
    "# Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SaxYigG8cPVA"
   },
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "executionInfo": {
     "elapsed": 5495,
     "status": "ok",
     "timestamp": 1763430889736,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "XvYczeyDcNxP",
    "outputId": "ce9511ad-b4c8-4817-9a33-f7861d449c17"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"display(df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"tweets\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"for people who don't understand diy artattack\",\n          \"- You probably just missed the text.\",\n          \"dailymail readers being sensible as always shocker dailyfail inhuntspocket theyhatethenhs\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"class\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"figurative\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tokens\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"['for', 'people', 'who', 'do', \\\"n't\\\", 'understand', 'diy', 'artattack']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"flags\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"[0, 0, 0, 0, 0, 1, 0, 0]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"input_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"neutralize: for[0] people[0] who[0] do[0] n't[0] understand[1] diy[0] artattack[0]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"for people who don't understand diy artattack\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-cbc8a878-691c-42bd-a2c3-cf8f34da9769\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>class</th>\n",
       "      <th>tokens</th>\n",
       "      <th>flags</th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Be aware dirty step to get money staylight sta...</td>\n",
       "      <td>figurative</td>\n",
       "      <td>['Be', 'aware', 'dirty', 'step', 'to', 'get', ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>neutralize: Be[0] aware[1] dirty[0] step[0] to...</td>\n",
       "      <td>Be aware dirty step to get money staylight sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>for people who don't understand diy artattack</td>\n",
       "      <td>figurative</td>\n",
       "      <td>['for', 'people', 'who', 'do', \"n't\", 'underst...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "      <td>neutralize: for[0] people[0] who[0] do[0] n't[...</td>\n",
       "      <td>for people who don't understand diy artattack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dailymail readers being sensible as always sho...</td>\n",
       "      <td>figurative</td>\n",
       "      <td>['dailymail', 'readers', 'being', 'sensible', ...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>neutralize: dailymail[0] readers[0] being[0] s...</td>\n",
       "      <td>dailymail readers being sensible as always sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why do I get the feeling you like games?</td>\n",
       "      <td>figurative</td>\n",
       "      <td>['Why', 'do', 'I', 'get', 'the', 'feeling', 'y...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 1, 0, 0]</td>\n",
       "      <td>neutralize: Why[0] do[0] I[0] get[0] the[0] fe...</td>\n",
       "      <td>Why do I get the feeling you like games?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>- You probably just missed the text.</td>\n",
       "      <td>figurative</td>\n",
       "      <td>['-', 'You', 'probably', 'just', 'missed', 'th...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>neutralize: -[0] You[0] probably[0] just[0] mi...</td>\n",
       "      <td>- You probably just missed the text.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cbc8a878-691c-42bd-a2c3-cf8f34da9769')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-cbc8a878-691c-42bd-a2c3-cf8f34da9769 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-cbc8a878-691c-42bd-a2c3-cf8f34da9769');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-1f37b26a-ae7e-4626-bb94-bb007a71ba2e\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1f37b26a-ae7e-4626-bb94-bb007a71ba2e')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-1f37b26a-ae7e-4626-bb94-bb007a71ba2e button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                              tweets       class  \\\n",
       "0  Be aware dirty step to get money staylight sta...  figurative   \n",
       "1      for people who don't understand diy artattack  figurative   \n",
       "2  dailymail readers being sensible as always sho...  figurative   \n",
       "3           Why do I get the feeling you like games?  figurative   \n",
       "4               - You probably just missed the text.  figurative   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  ['Be', 'aware', 'dirty', 'step', 'to', 'get', ...   \n",
       "1  ['for', 'people', 'who', 'do', \"n't\", 'underst...   \n",
       "2  ['dailymail', 'readers', 'being', 'sensible', ...   \n",
       "3  ['Why', 'do', 'I', 'get', 'the', 'feeling', 'y...   \n",
       "4  ['-', 'You', 'probably', 'just', 'missed', 'th...   \n",
       "\n",
       "                                  flags  \\\n",
       "0  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "1              [0, 0, 0, 0, 0, 1, 0, 0]   \n",
       "2        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]   \n",
       "3        [0, 0, 0, 0, 0, 1, 0, 1, 0, 0]   \n",
       "4              [0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "                                          input_text  \\\n",
       "0  neutralize: Be[0] aware[1] dirty[0] step[0] to...   \n",
       "1  neutralize: for[0] people[0] who[0] do[0] n't[...   \n",
       "2  neutralize: dailymail[0] readers[0] being[0] s...   \n",
       "3  neutralize: Why[0] do[0] I[0] get[0] the[0] fe...   \n",
       "4  neutralize: -[0] You[0] probably[0] just[0] mi...   \n",
       "\n",
       "                                         target_text  \n",
       "0  Be aware dirty step to get money staylight sta...  \n",
       "1      for people who don't understand diy artattack  \n",
       "2  dailymail readers being sensible as always sho...  \n",
       "3           Why do I get the feeling you like games?  \n",
       "4               - You probably just missed the text.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path=os.path.join(folder_path, \"data/tweets/processed/mpqa_lexicon/flagged_train.csv\")\n",
    "df = pd.read_csv(path)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 153,
     "status": "ok",
     "timestamp": 1763433904403,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "EftwGuxFcGO3",
    "outputId": "4988fecf-768f-4ab3-bafe-28db0420d81f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: [('abandoned', {'type': 'weaksubj', 'polarity': 'negative'}), ('abandonment', {'type': 'weaksubj', 'polarity': 'negative'}), ('abandon', {'type': 'weaksubj', 'polarity': 'negative'}), ('abase', {'type': 'strongsubj', 'polarity': 'negative'}), ('abasement', {'type': 'strongsubj', 'polarity': 'negative'}), ('abash', {'type': 'strongsubj', 'polarity': 'negative'}), ('abate', {'type': 'weaksubj', 'polarity': 'negative'}), ('abdicate', {'type': 'weaksubj', 'polarity': 'negative'}), ('aberration', {'type': 'strongsubj', 'polarity': 'negative'}), ('abhor', {'type': 'strongsubj', 'polarity': 'negative'})]\n",
      "Types:  ['strongsubj', 'weaksubj']\n",
      "polarities:  ['both', 'negative', 'neutral', 'positive', 'weakneg']\n",
      "{'assure', 'fret', 'capriciously', 'egotistical', 'nagging', 'laconic', 'world-famous', 'fortuitous', 'resourcefulness', 'debauchery', 'revenge', 'heretic', 'accuses', 'absurd', 'ebulliently', 'torturous', 'uncommonly', 'refute', 'praising', 'undisputably', 'foe', 'dissatisfying', 'vouchsafe', 'vileness', 'exuberant', 'disrespectablity', 'glossy', 'wretch', 'aversion', 'afraid', 'shirk', 'easygoing', 'pariah', 'scared', 'despise', 'beautifully', 'fanfare', 'catastrophically', 'chatter', 'disgraceful', 'heretical', 'averse', 'overemphasize', 'pittance', 'congenial', 'daring', 'scare', 'cripple', 'falsely', 'dragons', 'disbelieve', 'repugnance', 'insufferable', 'shy', 'stammer', 'culpable', 'disparage', 'grateful', 'depressing', 'dilapidated', 'concerned', 'hero', 'inflame', 'insincerely', 'knave', 'dismally', 'graceful', 'misconceptions', 'encouraging', 'jubilation', 'misrepresentation', 'reactionary', 'staunchness', 'bravado', 'dispiriting', 'fuss', 'untimely', 'fondness', 'inconstant', 'uncompromisingly', 'exalted', 'savvy', 'unequivocal', 'presumably', 'forsee', 'skeletons', 'steadfastly', 'bonkers', 'regardlessly', 'atrocious', 'overjoyed', 'gracious', 'fanatic', 'elaborate', 'besides', 'romantically', 'antagonize', 'tranquility', 'sap', 'miserly', 'heavyhearted', 'displeasing', 'contemplate', 'contrived', 'profusely', 'seethe', 'graciousness', 'disgust', 'disadvantageous', 'fruitlessly', 'reprovingly', 'hostility', 'noteworthy', 'obliterate', 'renaissance', 'considerably', 'indefensible', 'diffidence', 'want', 'accusation', 'abhorrently', 'irritably', 'downhearted', 'falter', 'pertinacity', 'raptureously', 'deadlock', 'kind', 'babble', 'invaluable', 'sabotage', 'imperatively', 'byzantine', 'self-humiliation', 'studious', 'bafflement', 'crook', 'impel', 'inclinations', 'wickedly', 'heartily', 'preposterous', 'egregiously', 'overwhelmingly', 'civility', 'irksome', 'despoiler', 'lame-duck', 'straightforward', 'super', 'reproachful', 'extreme', 'dangerousness', 'disagreeably', 'petrify', 'catchy', 'feebleminded', 'tricky', 'avalanche', 'excel', 'wonderfully', 'uphold', 'flighty', 'appal', 'dungeons', 'sleazy', 'deprecate', 'trustworthy', 'disgustful', 'chivalry', 'anti-social', 'endure', 'freakish', 'damnation', 'trivialize', 'amiss', 'perceptive', 'intimidatingly', 'stars', 'rack', 'indomitable', 'inconvenient', 'boil', 'steadfast', 'zealot', 'patriotic', 'equivocal', 'loophole', 'peeve', 'beneficiary', 'forgivingly', 'disorganized', 'incongruously', 'self-satisfaction', 'worthwhile', 'unorthodox', 'cherub', 'dissatisfactory', 'eloquently', 'infidels', 'inquisitive', 'moral', 'unsophisticated', 'invalidity', 'dishonorablely', 'humiliating', 'degenerate', 'aggression', 'malign', 'mistake', 'chaff', 'heartbreaker', 'vindictive', 'amusement', 'despoil', 'inelegance', 'outrageousness', 'sentiment', 'applaud', 'swagger', 'stalemate', 'willingness', 'inflict', 'smartly', 'lover', 'implausible', 'sarcasm', 'winnable', 'obstinate', 'agitated', 'presumptuously', 'terrible', 'hale', 'derisiveness', 'beguile', 'flirty', 'perspicuous', 'merely', 'thwart', 'blessing', 'improper', 'pugnacity', 'arresting', 'adamantly', 'commiserate', 'distraught', 'sadden', 'ameliorate', 'debaser', 'idol', 'cowardly', 'digress', 'pretentious', 'wee', 'woo', 'titillatingly', 'disobey', 'delude', 'disillusion', 'dote', 'privilege', 'indiscriminating', 'vain', 'derisively', 'slashing', 'bewilderment', 'degradation', 'earsplitting', 'inviolate', 'bungle', 'unimaginably', 'impending', 'sickly', 'sulk', 'hardship', 'gibber', 'asinininity', 'flak', 'brutalities', 'dissuade', 'inanely', 'sanction', 'dexterous', 'satisfaction', 'irreverence', 'rebuke', 'backwardness', 'beloved', 'hopeless', 'brashly', 'kemp', 'glowing', 'dispirited', 'euphorically', 'excellency', 'bristle', 'crooked', 'enthuse', 'idealize', 'perverse', 'desiccate', 'disparagingly', 'healthful', 'inconvenience', 'senile', 'suspiciously', 'satisfactory', 'receptive', 'smart', 'frolic', 'sure', 'philosophize', 'spookiest', 'redeeming', 'stooges', 'worried', 'prattle', 'benevolent', 'feeling', 'indescribable', 'object', 'mindless', 'arrogance', 'greed', 'horrid', 'awestruck', 'dislike', 'dumbfounded', 'outwit', 'stranger', 'untenable', 'disappointing', 'yearningly', 'cozy', 'reprehensibly', 'hardly', 'unrealistic', 'endorsement', 'jeering', 'insulted', 'loathing', 'mellow', 'raving', 'naively', 'astonish', 'illuminati', 'conceited', 'opinionated', 'disavowal', 'insights', 'freakishly', 'plight', 'clever', 'imagine', 'wishing', 'irrationality', 'stunningly', 'troublemaker', 'heresy', 'lukewarm', 'harpy', 'scandalous', 'rightness', 'salvation', 'inexcusable', 'contortions', 'masterful', 'tiringly', 'irrelevant', 'worthlessness', 'outcry', 'peevishly', 'adamant', 'opponent', 'exhilarating', 'snub', 'devil', 'leery', 'selfishly', 'anger', 'disbeliever', 'tired', 'captivation', 'bitterly', 'devilry', 'excited', 'altruistically', 'hmm', 'calamity', 'passiveness', 'monster', 'unpretentious', 'embroiled', 'stimulating', 'disquietingly', 'misunderstanding', 'pity', 'articulate', 'brusque', 'respite', 'distressing', 'sagely', 'sensation', 'despairingly', 'dilly-dally', 'malevolently', 'deceptive', 'law-abiding', 'disagreeable', 'rally', 'rationalize', 'reverent', 'insistently', 'screaming', 'paramount', 'thoughtfulness', 'disaffect', 'moody', 'wilt', 'outcast', 'embroil', 'forgiven', 'sermonize', 'vociferously', 'arcane', 'sentiments', 'hypocritical', 'unfettered', 'foolhardy', 'ignominiously', 'battleground', 'enraged', 'levity', 'inarguable', 'misinformed', 'disaccord', 'nix', 'sugar-coat', 'insinuation', 'unselfish', 'malcontent', 'prodigy', 'saint', 'adroitly', 'supurbly', 'unfounded', 'shunned', 'abhor', 'ultimately', 'accede', 'disadvantage', 'annoyingly', 'ill-favored', 'miscreant', 'tyrannically', 'collusion', 'agreeableness', 'delicious', 'deride', 'discordance', 'opportune', 'yelp', 'unipolar', 'furiously', 'cataclysmic', 'fool', 'obsession', 'decry', 'hopelessly', 'admonishingly', 'insupportable', 'masterfully', 'vilify', 'viciousness', 'heartwarming', 'spellbind', 'hanker', 'stridently', 'surge', 'fright', 'ultimatums', 'futile', 'silly', 'astounding', 'outstrip', 'unquestionably', 'devious', 'liberty', 'okay', 'ape', 'perfectly', 'pundits', 'senselessly', 'jaded', 'remorse', 'surprisingly', 'underlings', 'fascinatingly', 'ungrateful', 'aspersion', 'talented', 'infirm', 'immoral', 'assess', 'unlucky', 'endorse', 'caustically', 'profound', 'taunt', 'pleasure', 'greedy', 'gleefully', 'insultingly', 'displeasure', 'disregardful', 'quicken', 'discountenance', 'discomfititure', 'ambiguity', 'fibber', 'invincible', 'injustices', 'misunderstand', 'frantically', 'puppet', 'smartest', 'unequivocally', 'desolately', 'intriguingly', 'incorruptible', 'relentlessly', 'tauntingly', 'smuttier', 'hell', 'magic', 'mischievous', 'reward', 'endurable', 'intolerant', 'sinfully', 'belittled', 'purposeful', 'misreading', 'impose', 'better-known', 'complaint', 'hypocritically', 'discouraging', 'critics', 'debase', 'flat-out', 'obstinately', 'acerbically', 'demand', 'lies', 'villify', 'torment', 'uncouth', 'scandalize', 'savagely', 'dream', 'burdensomely', 'enchanted', 'grumble', 'offenses', 'accentuate', 'scary', 'blame', 'insulting', 'misinform', 'sanctify', 'pitiful', 'endearing', 'floundering', 'immaculately', 'unpleasantries', 'displease', 'cute', 'jealousy', 'gloat', 'ill-treated', 'adorer', 'astounded', 'diabolical', 'glean', 'adulterated', 'reproach', 'dehumanize', 'faithfulness', 'gratefully', 'anti-us', 'frenzy', 'adversity', 'misunderstandings', 'pest', 'prate', 'satirical', 'obscure', 'skittish', 'dizzing', 'confidence', 'partisan', 'villainously', 'anxiety', 'blissfully', 'meaningless', 'inconsistence', 'well-intentioned', 'kick', 'rebellious', 'shortcoming', 'sadness', 'trauma', 'unfortunate', 'denounce', 'likable', 'tormented', 'distrustful', 'mercy', 'rectification', 'gainful', 'sweeping', 'brilliant', 'faith', 'superficially', 'ulterior', 'improbability', 'supremacy', 'feelings', 'tantrum', 'gracefully', 'masterpieces', 'revelatory', 'trick', 'overzealously', 'pugnacious', 'spectacular', 'manipulators', 'paradise', 'badly', 'utmost', 'well-being', 'affection', 'elegantly', 'maximize', 'exorbitantance', 'graceless', 'mortified', 'agitate', 'wellbeing', 'formidable', 'lunatic', 'dotingly', 'venom', 'deceivers', 'plagiarize', 'indecent', 'acrid', 'endanger', 'audaciousness', 'distraction', 'exhilarate', 'arrogantly', 'tremendously', 'pleasurable', 'polemize', 'inequitably', 'banish', 'repugnant', 'obsessive', 'swindle', 'incoherently', 'apprehensively', 'disallow', 'conjecture', 'needfully', 'scarily', 'bless', 'argue', 'sexy', 'sty', 'terrorize', 'woeful', 'undependability', 'cheapen', 'molestation', 'detestably', 'phenomenal', 'propaganda', 'notoriously', 'rantingly', 'astronomical', 'beatify', 'eschew', 'excellent', 'sublime', 'god-awful', 'views', 'sympathetic', 'debatable', 'weirdly', 'glowingly', 'baffling', 'oddest', 'unseemly', 'exalting', 'disobedience', 'deadbeat', 'indiscreet', 'earnest', 'humor', 'meritorious', 'belie', 'wisely', 'brat', 'unnerve', 'execrate', 'truth', 'mawkishly', 'boldly', 'astutely', 'hopefully', 'scorching', 'imbroglio', 'sensitivity', 'deduce', 'implore', 'wonder', 'laughter', 'doddering', 'incontrovertible', 'agitation', 'devote', 'faithless', 'spade', 'ferocious', 'irrationally', 'misunderstood', 'hallucinate', 'goodwill', 'disingenuous', 'foresight', 'gaggle', 'indignation', 'indubitable', 'mantra', 'pleasing', 'incontestable', 'despot', 'slanders', 'dissemble', 'amaze', 'brutalize', 'starkly', 'mastery', 'outsmart', 'deservedly', 'degenerately', 'justly', 'maltreatment', 'gauche', 'stale', 'gallingly', 'stodgy', 'euphoria', 'illuminating', 'recant', 'miserableness', 'scourge', 'abase', 'harmony', 'though', 'allegation', 'distort', 'truthfully', 'glibly', 'spooky', 'discontent', 'moving', 'closeness', 'militancy', 'veto', 'dismayingly', 'affected', 'despondently', 'devilishly', 'affirm', 'sob', 'gape', 'disparaging', 'aspiration', 'immaterial', 'rejoicingly', 'pathetically', 'superbly', 'unbearable', 'loathsome', 'insufferably', 'magnanimous', 'instigators', 'brazen', 'envious', 'indifferent', 'ill-conceived', 'hard-line', 'stinking', 'unsound', 'protest', 'shock', 'abrasive', 'deviously', 'witty', 'direly', 'commendably', 'hedge', 'generously', 'revel', 'sensational', 'downheartedly', 'maniacal', 'superlative', 'orthodoxy', 'acclamation', 'decadence', 'gaudy', 'idolize', 'glorify', 'ludicrously', 'inspiring', 'ridiculous', 'staunch', 'taunting', 'lurking', 'theoretize', 'ill-treatment', 'inapt', 'love', 'deign', 'dissenter', 'preposterously', 'dubious', 'puzzle', 'lovely', 'impair', 'mentality', 'rosy', 'devoid', 'lonely', 'acrimony', 'naughty', 'serious', 'undoubted', 'conceit', 'detestable', 'forsake', 'stunning', 'exasperating', 'worsening', 'gloriously', 'erroneously', 'jovial', 'certainly', 'prejudicial', 'misguidance', 'scolding', 'abhorrence', 'imagination', 'sheer', 'unimportant', 'unqualified', 'credence', 'severity', 'disappointingly', 'irritated', 'comforting', 'catastrophes', 'sincerely', 'infringement', 'ingratiatingly', 'ardently', 'luxuriously', 'stellar', 'enrage', 'alarming', 'unprecedented', 'distraughtness', 'blissful', 'complimentary', 'shirker', 'aloof', 'cynicism', 'demystify', 'enervate', 'sensationalize', 'vibrantly', 'egotistically', 'impugn', 'incoherence', 'ludicrous', 'squander', 'talent', 'strife', 'aspirations', 'devoted', 'impeccably', 'felicitous', 'hunger', 'unsettlingly', 'sensationally', 'superb', 'happily', 'defame', 'inaccurately', 'imminent', 'spurious', 'halfhearted', 'cursed', 'dumb', 'proud', 'atrocity', 'entrapment', 'tyrannical', 'ethical', 'exploitation', 'avidly', 'gratuitous', 'kindly', 'misbecome', 'blundering', 'perturbed', 'criticism', 'scoundrel', 'irk', 'nifty', 'reassurance', 'strict', 'astonishing', 'understand', 'urgency', 'passe', 'like', 'monstrous', 'giddy', 'oddity', 'overdone', 'traumatized', 'over-acted', 'unforgettable', 'litigious', 'irritate', 'vow', 'terrifying', 'gruesomely', 'well-received', 'prowess', 'underestimate', 'respectable', 'guile', 'oppressors', 'passionate', 'palatable', 'outright', 'fundamentally', 'racists', 'brave', 'toast', 'better-than-expected', 'hindrance', 'aggressive', 'clout', 'great', 'denigrate', 'fame', 'superficial', 'irresponsible', 'treachery', 'misdirection', 'spite', 'empathize', 'sympathize', 'pious', 'mishap', 'vengeance', 'petrified', 'defamations', 'hell-bent', 'regrettable', 'awful', 'ruthlessly', 'gratifying', 'luckily', 'anarchistic', 'hysteric', 'lambaste', 'succumb', 'well-wishers', 'banalize', 'unwieldy', 'stupid', 'viewpoints', 'reprehensible', 'imperiously', 'comrades', 'manipulative', 'innuendo', 'worrying', 'poorly', 'disfavor', 'contentious', 'definitively', 'passion', 'gladden', 'precautions', 'admiringly', 'harangue', 'shrewdness', 'sympathy', 'yep', 'outlaw', 'imminently', 'ignoble', 'horrifying', 'shoddy', 'dismay', 'illogic', 'diatribe', 'unthinkably', 'divisive', 'distressingly', 'assiduously', 'befit', 'one-sided', 'indiscriminate', 'misbehavior', 'trustingly', 'unethical', 'confess', 'alliance', 'haughty', 'bellicose', 'genius', 'aggressiveness', 'peerless', 'quarrelsome', 'irrational', 'stupified', 'conciliate', 'terror', 'vivacious', 'pestilent', 'destroyer', 'wrongly', 'thankless', 'draconic', 'painstakingly', 'eloquent', 'imprecate', 'grating', 'mire', 'persevere', 'ultra', 'meddlesome', 'fabrication', 'agitator', 'lawbreaker', 'mismanage', 'daydream', 'election-rigger', 'shrill', 'sane', 'celebration', 'agreeably', 'invalid', 'demean', 'fundamental', 'raptureous', 'indispensable', 'torridly', 'yearning', 'extoll', 'disrepute', 'travesty', 'anxious', 'whimper', 'annoying', 'obedient', 'inconsolable', 'indispensability', 'sidetrack', 'spectacularly', 'calamitous', 'conspicuously', 'incautious', 'gorgeous', 'scathing', 'scold', 'gaily', 'deceiving', 'licentiousness', 'astronomic', 'judicious', 'exploit', 'covet', 'tawdry', 'abysmal', 'shortsighted', 'thankfully', 'entrancing', 'fanaticism', 'superstition', 'lier', 'filth', 'vainly', 'degeneration', 'invalidate', 'rightly', 'expunge', 'indiscretion', 'glaring', 'dunce', 'blindingly', 'franticly', 'obedience', 'consummate', 'dreamland', 'irreconcilable', 'nefariously', 'scoldingly', 'sadly', 'adoring', 'astonishingly', 'disinterested', 'adore', 'distrusting', 'elegance', 'gaff', 'glimmering', 'fanatics', 'foully', 'obscene', 'quitter', 'sly', 'sullen', 'deplorable', 'vice', 'vindictiveness', 'miserably', 'amazingly', 'worryingly', 'favorable', 'phobic', 'realistically', 'cogitate', 'mere', 'cogent', 'nauseating', 'virulent', 'skittishly', 'dignified', 'heroic', 'befitting', 'caricature', 'coercion', 'thrill', 'imaginary', 'quasi-ally', 'merrily', 'despicably', 'anti-semites', 'apocalyptic', 'dazzled', 'flimflam', 'languid', 'leer', 'inclined', 'commendable', 'smilingly', 'miseries', 'ruffian', 'jealous', 'humorous', 'temptingly', 'languorous', 'rebuff', 'moderation', 'patiently', 'gallantly', 'adventurism', 'justify', 'conspirator', 'melodramatic', 'craziness', 'oddities', 'taint', 'trendy', 'blunt', 'devilish', 'fatuity', 'fallacy', 'infallibly', 'actuality', 'renown', 'monstrosity', 'false', 'poison', 'mislike', 'sarcastically', 'scrupulously', 'disrespectfulness', 'stern', 'astoundingly', 'slanderous', 'fearsome', 'crusade', 'arbitrary', 'avaricious', 'inimically', 'curses', 'proficiently', 'gumption', 'fractious', 'calumny', 'concerns', 'dummy', 'aware', 'discontentedly', 'generosity', 'inimical', 'aimless', 'hotheaded', 'nuance', 'stirringly', 'indignantly', 'apotheosis', 'wise', 'heavy-handed', 'intriguing', 'capricious', 'disputed', 'slanderously', 'dismayed', 'fortitude', 'undesirable', 'fitting', 'acclaimed', 'luckiness', 'cognizant', 'outrageous', 'proactive', 'wearisome', 'batty', 'insight', 'mistakenly', 'flagrantly', 'dogged', 'rectify', 'uncivil', 'savagery', 'unpleasant', 'licentiously', 'forsaken', 'mirage', 'sinister', 'slur', 'stubborn', 'worriedly', 'dashing', 'intolerable', 'innocuous', 'bland', 'burden', 'tint', 'embarrassingly', 'god-given', 'terribleness', 'disrespectful', 'spotless', 'credulous', 'quack', 'angelic', 'admonish', 'pervasive', 'careless', 'prodigious', 'cruelties', 'pugnaciously', 'desolate', 'coercive', 'enviable', 'letch', 'painstaking', 'adroit', 'unbosom', 'simplicity', 'cartoonish', 'reviled', 'disputable', 'striving', 'zenith', 'decayed', 'hallucination', 'acridly', 'brainwash', 'opportunity', 'severe', 'rattle', 'fustigate', 'carnage', 'pitiless', 'lone', 'contradict', 'falsify', 'irreverent', 'overworked', 'satirize', 'luxury', 'prize', 'wariness', 'amiability', 'inequitable', 'unspeakablely', 'appalled', 'anyway', 'counterproductive', 'lark', 'despised', 'odder', 'glum', 'mystery', 'glee', 'inoffensive', 'light-hearted', 'shadow', 'unbelievable', 'childish', 'comely', 'brute', 'cornerstone', 'surging', 'studiously', 'quarrels', 'fallaciousness', 'intimate', 'eminent', 'tingle', 'grimace', 'intolerance', 'selfish', 'romanticize', 'belief', 'radicalization', 'thinking', 'molest', 'ill-advised', 'irreparable', 'amicability', 'forgetful', 'injustice', 'bore', 'shocking', 'effortless', 'jealousness', 'inclination', 'magnanimously', 'rumors', 'self-determination', 'fond', 'inconsequentially', 'squarely', 'lewdly', 'tragically', 'dejectedly', 'covetingly', 'hard-liner', 'evils', 'incompetently', 'left-leaning', 'splendor', 'beg', 'indefatigable', 'enormously', 'magnificence', 'incorrigible', 'unassailable', 'weird', 'abyss', 'disposition', 'nasty', 'revoltingly', 'chaste', 'dejected', 'exceedingly', 'grudge', 'harrow', 'prestige', 'desert', 'scum', 'gruesome', 'downbeat', 'traumatic', 'dishonesty', 'strikingly', 'deluded', 'glory', 'impolite', 'obsessions', 'vestiges', 'nefarious', 'demonize', 'crass', 'dissident', 'proscriptions', 'unabashedly', 'imperturbable', 'indelible', 'disingenuously', 'mesmerizingly', 'scarier', 'sanguine', 'willfully', 'dreadful', 'haggard', 'amazed', 'jaunty', 'pragmatic', 'frighten', 'ill-fated', 'effusion', 'delightful', 'awesomely', 'bitterness', 'chastise', 'overkill', 'self-respect', 'sensations', 'unsettle', 'reticent', 'gawk', 'disturbed-let', 'gratify', 'shamefulness', 'terrifically', 'invidiously', 'insincerity', 'subjugate', 'jabber', 'sorely', 'neurotic', 'corrupt', 'disdainful', 'palliate', 'valiantly', 'optimal', 'horrifically', 'lecherous', 'winners', 'luck', 'mysterious', 'ridiculously', 'spookily', 'stagnate', 'chasten', 'unfairly', 'indecency', 'motivate', 'wail', 'perfidious', 'oasis', 'greatly', 'auspicious', 'rifts', 'virulently', 'watchdog', 'insolence', 'reflecting', 'unfit', 'dignity', 'procrastination', 'brilliantly', 'insecurity', 'unwarranted', 'lowly', 'break-point', 'longingly', 'haunting', 'plenty', 'deceptively', 'prejudge', 'contemptuous', 'ire', 'animosity', 'jeer', 'vulnerable', 'yeah', 'ally', 'sore', 'brains', 'loath', 'bold', 'hate', 'unpopular', 'repulsive', 'impatient', 'desperately', 'rumple', 'delightfulness', 'opinions', 'abominable', 'seedy', 'cheerful', 'disturb', 'spoonfed', 'belittle', 'plausible', 'well-managed', 'stubbornness', 'oppressively', 'vulgar', 'accolades', 'chatterbox', 'perfunctory', 'pettifog', 'amusing', 'enthusiast', 'annihilation', 'fragrant', 'dissuasive', 'fantastically', 'vitality', 'carefree', 'exceptional', 'supposing', 'heartening', 'stupendously', 'fruitless', 'lovably', 'loathly', 'magnificent', 'disturbed', 'elated', 'enthusiastic', 'infidel', 'so-cal', 'timidity', 'grim', 'insensitivity', 'exult', 'inexcusably', 'inexorably', 'whine', 'trustworthiness', 'uplifting', 'patience', 'destructive', 'mess', 'panicky', 'bemoaning', 'brood', 'regally', 'groundless', 'honor', 'skepticism', 'exhilaratingly', 'gainsayer', 'domineering', 'imprudence', 'goading', 'mercilessly', 'nemesis', 'poverty', 'rift', 'strange', 'irresistible', 'unwilling', 'unilateralism', 'adorable', 'vexingly', 'scoff', 'calamities', 'joyously', 'inundate', 'prickles', 'valor', 'virtue', 'dispirit', 'agonies', 'mistrustful', 'brash', 'redeem', 'unwillingness', 'angry', 'fatalistically', 'usurp', 'awfully', 'gruff', 'ego', 'mannerly', 'cackle', 'importune', 'abuse', 'spiteful', 'homage', 'frustration', 'chafe', 'galling', 'sentimentality', 'disillusioned', 'cliche', 'delight', 'ignorant', 'misery', 'undisputable', 'unconvincingly', 'misfit', 'venerable', 'brag', 'misstatement', 'obediently', 'limp', 'alas', 'incoherent', 'tenacity', 'repudiate', 'keenness', 'eloquence', 'disprove', 'dissatisfy', 'clearer', 'frustratingly', 'defy', 'tenaciously', 'fatalistic', 'blockhead', 'facetious', 'miscreants', 'discontented', 'unnerved', 'languorously', 'coveting', 'infer', 'vivid', 'reprimand', 'fiend', 'attentive', 'master', 'lunaticism', 'anguish', 'excessively', 'questionable', 'hospitable', 'revengefully', 'intolerablely', 'skillfully', 'wicked', 'shriek', 'vociferous', 'run-down', 'nervous', 'mawkish', 'virulence', 'frank', 'fear', 'invulnerable', 'grievously', 'impractical', 'pains', 'delusions', 'utterly', 'wonderously', 'acclaim', 'inhospitality', 'difficulty', 'fraternize', 'pro-american', 'handsome', 'heinous', 'degradingly', 'calumnies', 'penitent', 'hoax', 'shortcomings', 'trample', 'cautionary', 'uneasiness', 'fashionably', 'slander', 'mistrustfully', 'midget', 'confuse', 'mishandle', 'noble', 'disquietude', 'liar', 'truculently', 'excusable', 'atrocities', 'rascal', 'jauntily', 'knowledge', 'enchantingly', 'flatteringly', 'blistering', 'treason', 'collude', 'commend', 'inhumanity', 'coward', 'barbarian', 'fury', 'goodness', 'impatience', 'insanely', 'recommended', 'rusty', 'disappointed', 'morbid', 'deserve', 'aggravation', 'sagacity', 'apocalypse', 'beauty', 'eagerness', 'splendid', 'reprove', 'sanctity', 'wily', 'inelegant', 'terrify', 'farfetched', 'smoulder', 'dawdle', 'eviscerate', 'extol', 'effrontery', 'self-destructive', 'suave', 'torrid', 'happiness', 'sorrowfully', 'peevish', 'infamous', 'astute', 'adulate', 'cravenly', 'marvellous', 'bondage', 'stagnation', 'problematic', 'quandary', 'annihilate', 'abscond', 'shamelessly', 'rational', 'spank', 'superficiality', 'entreat', 'engaging', 'godlike', 'misgiving', 'disown', 'celebrate', 'livid', 'provoke', 'repugn', 'wretchedness', 'lawbreaking', 'idyllic', 'paltry', 'radical', 'shamelessness', 'betrayals', 'deceive', 'ultra-hardline', 'exemplar', 'irrefutable', 'inane', 'herald', 'realist', 'apostle', 'denunciation', 'inarticulate', 'twisted', 'askance', 'obtuse', 'fatuous', 'indestructible', 'ultimatum', 'demoralize', 'deserved', 'greet', 'glaringly', 'imperil', 'freak', 'callous', 'senseless', 'excitement', 'excitedly', 'mirth', 'irritation', 'horrifyingly', 'tramp', 'peril', 'ponder', 'devilment', 'nevertheless', 'flourish', 'shameless', 'dear', 'deft', 'spur', 'dispute', 'doggedly', 'disunity', 'galore', 'allure', 'marvelous', 'ache', 'aggressor', 'bonny', 'dishonestly', 'critic', 'dexterously', 'esteem', 'charisma', 'convincingly', 'hassle', 'entreatingly', 'abysmally', 'stereotypically', 'doubts', 'bleak', 'foolishness', 'gutsy', 'troublesomely', 'perish', 'enflame', 'invincibility', 'chit', 'welcome', 'scar', 'worthiness', 'martyrdom-seeking', 'wow', 'dearth', 'embarrassing', 'scowl', 'disinclined', 'mudslinger', 'accolade', 'overachiever', 'bereave', 'sentimentally', 'subjugation', 'inconsequently', 'sufferers', 'friendliness', 'deploring', 'jubilate', 'boldness', 'ambiguous', 'inevitable', 'madly', 'tremendous', 'outdo', 'truthful', 'pleasingly', 'attest', 'lionhearted', 'draconian', 'tedious', 'fascination', 'grit', 'crazily', 'inconsequential', 'irreproachable', 'brutish', 'inconsistencies', 'inviolable', 'lamentably', 'accursed', 'frustrating', 'itch', 'shrouded', 'despondence', 'indecently', 'distrust', 'patriot', 'stench', 'spitefully', 'forgetfulness', 'stink', 'extermination', 'miserable', 'skeptical', 'admonition', 'curious', 'worse', 'celebratory', 'maledict', 'mundane', 'boredom', 'enraptured', 'scarred', 'fortunate', 'chic', 'deplorably', 'timid', 'beneficial', 'confusion', 'inconsiderately', 'expansive', 'uproarous', 'caustic', 'extravagance', 'compel', 'hubris', 'beseech', 'bane', 'clumsy', 'lechery', 'hideously', 'mourner', 'massacres', 'disobedient', 'fanatically', 'perceptions', 'exuberance', 'misbegotten', 'stark', 'intuitive', 'excellently', 'deplore', 'enviousness', 'ironic', 'touches', 'prudently', 'ugliness', 'gloss', 'untrustworthy', 'sillily', 'fetid', 'grave', 'offensively', 'attractive', 'fiendish', 'anti-occupation', 'poise', 'annoy', 'consensus', 'courtesy', 'recommendation', 'hatred', 'integrity', 'revengeful', 'imploring', 'sugar-coated', 'best-performing', 'understate', 'uproar', 'madman', 'inequities', 'scandalized', 'terribly', 'underdog', 'incisive', 'malevolent', 'enmity', 'affectionate', 'liability', 'expertly', 'hesitant', 'invigorating', 'cherished', 'penalize', 'frazzle', 'agree', 'humble', 'indifference', 'disgustingly', 'justifiable', 'timidness', 'unfaithful', 'grief', 'huckster', 'pedantic', 'rejoice', 'indulge', 'barren', 'assertions', 'worthy', 'understood', 'rewarding', 'damnable', 'abhors', 'impassioned', 'fantasy', 'scornfully', 'diatribes', 'enthralled', 'admonishment', 'ignorance', 'overture', 'wondrous', 'triumphant', 'gleeful', 'thoughtless', 'blather', 'misleading', 'ardor', 'crusader', 'hopes', 'obviate', 'repentance', 'riled', 'ill-sorted', 'rumor', 'berate', 'sickening', 'pessimism', 'awkward', 'irrelevance', 'betrayer', 'prickle', 'soften', 'lawless', 'far-fetched', 'dullard', 'futility', 'oppositions', 'unconcerned', 'impudence', 'loopholes', 'flexible', 'disruption', 'rhetorical', 'mighty', 'brook', 'upsetting', 'vindictively', 'disconcerted', 'gibberish', 'dishonor', 'meek', 'horrible', 'disdainfully', 'utterances', 'wishes', 'suffering', 'assuredly', 'indulgent', 'sidetracked', 'insist', 'hardheaded', 'harshly', 'defective', 'lugubrious', 'renowned', 'troublesome', 'true', 'illusory', 'grace', 'enviably', 'exciting', 'explicitly', 'pine', 'blandish', 'hideousness', 'spellbindingly', 'impotent', 'ashamed', 'dissembler', 'runaway', 'intelligible', 'unleash', 'lustrous', 'endorser', 'lastly', 'accusingly', 'encouragingly', 'promoter', 'unnerving', 'denunciate', 'belittling', 'imperialist', 'fatefully', 'rankle', 'fallout', 'revealing', 'punch', 'deadweight', 'squirm', 'beggarly', 'ingratitude', 'striking', 'plot', 'profuse', 'vehement', 'scorchingly', 'jeopardy', 'plunder', 'upbeat', 'disheartening', 'doubt', 'taboo', 'dishearteningly', 'fervor', 'farce', 'uncivilized', 'gullible', 'puzzled', 'hilariousness', 'abasement', 'archaic', 'curse', 'divine', 'flabbergasted', 'idiocies', 'excellence', 'sanctimonious', 'brimstone', 'lush', 'fanatical', 'menacingly', 'horrify', 'recoil', 'inconsequent', 'disaster', 'flabbergast', 'allay', 'pandemonium', 'strictly', 'thoughtlessness', 'bad', 'confide', 'sorry', 'impudently', 'incredulously', 'warlike', 'cynical', 'invaluablely', 'ecstasy', 'apathetic', 'devastated', 'melancholy', 'shrilly', 'perhaps', 'delirious', 'guiltless', 'alarmingly', 'playful', 'ambivalent', 'irritating', 'truly', 'smug', 'damning', 'mulish', 'shimmer', 'charmingly', 'impeccable', 'balk', 'discredit', 'incisively', 'blameworthy', 'gaffe', 'generous', 'perplexed', 'dramatic', 'thoughtful', 'scream', 'detest', 'frighteningly', 'contradiction', 'insanity', 'inflexible', 'boundless', 'crazy', 'enrapt', 'impedance', 'pervert', 'flirt', 'understandable', 'truculent', 'bicker', 'laud', 'craven', 'pleadingly', 'unravel', 'partiality', 'wrangle', 'maliciousness', 'inattentive', 'gibe', 'rancor', 'shady', 'beastly', 'monumentally', 'doom', 'disgrace', 'liberate', 'reckless', 'destitution', 'condescending', 'forgive', 'insincere', 'ebullient', 'bungler', 'hardhearted', 'plunderer', 'illustrious', 'savaged', 'spellbound', 'discord', 'dejection', 'haughtily', 'muscle-flexing', 'recommendations', 'bombardment', 'disastrous', 'irreformable', 'gloom', 'interesting', 'oppress', 'keen', 'inordinately', 'scapegoat', 'priceless', 'appall', 'drunkard', 'inundated', 'kid', 'dismal', 'boiling', 'diametrically', 'resentful', 'shimmeringly', 'unreasonably', 'unquestionable', 'luster', 'allegations', 'melodramatically', 'gabble', 'motley', 'sham', 'unfeeling', 'entranced', 'disinterest', 'impenitent', 'marvelously', 'refutation', 'safeguard', 'vanquish', 'vile', 'racist', 'obstacle', 'traitorous', 'misread', 'seething', 'perfidity', 'worry', 'overlook', 'considerable', 'inevitably', 'nice', 'fawningly', 'remorselessness', 'stunned', 'lecher', 'ill-tempered', 'perversely', 'humorously', 'smarter', 'jolly', 'overblown', 'ennoble', 'immaculate', 'cheery', 'carp', 'self-interest', 'onerously', 'felicitate', 'daydreamer', 'drastically', 'fervent', 'perversity', 'stifling', 'fascinate', 'biased', 'nervousness', 'non-confidence', 'exalt', 'grudgingly', 'confusing', 'idiot', 'commotion', 'strident', 'amenable', 'righteously', 'arduously', 'lack', 'upheld', 'insane', 'benevolence', 'discomfit', 'distaste', 'invidious', 'favor', 'contravene', 'faithful', 'staunchly', 'mediocrity', 'censure', 'halfheartedly', 'cranky', 'luckier', 'rapturously', 'dumbfound', 'disclaim', 'laudably', 'mm', 'crafty', 'shark', 'downright', 'disturbingly', 'audacious', 'extraordinary', 'passionately', 'demonic', 'instigate', 'regretful', 'disloyal', 'spoon-feed', 'catastrophic', 'ingratiating', 'perplexing', 'doubtfully', 'fanciful', 'worsen', 'bully', 'friggin', 'bullyingly', 'impediment', 'insatiable', 'outraged', 'alarmed', 'nebulously', 'pretty', 'shabby', 'shroud', 'goddam', 'discourteously', 'fit', 'favorite', 'aggravate', 'backbite', 'incongruous', 'naive', 'calming', 'peaceable', 'likewise', 'adulation', 'perplexity', 'belligerently', 'mock', 'sensitively', 'limitless', 'negligence', 'ironies', 'castigate', 'sneer', 'skillful', 'rousing', 'disrespect', 'bombastic', 'revulsive', 'ineloquent', 'allusion', 'acrimoniously', 'annoyed', 'lorn', 'arduous', 'masterpiece', 'shamefully', 'obliged', 'repulsiveness', 'moreover', 'radically', 'liking', 'protector', 'maxi-devaluation', 'supurb', 'dread', 'richly', 'ideally', 'wince', 'heavenly', 'unparalleled', 'vent', 'uproariously', 'negligent', 'appreciativeness', 'paranoia', 'beautify', 'rail', 'edgy', 'laughable', 'remorseful', 'well-run', 'stupidly', 'beneficent', 'pleas', 'enormities', 'lech', 'destinies', 'prettily', 'sufferer', 'acrimonious', 'impudent', 'fervid', 'harsh', 'improbable', 'resent', 'deceiver', 'forlornly', 'providence', 'laughably', 'crippling', 'decadent', 'empathy', 'overwhelming', 'outbursts', 'insubstantially', 'unequal', 'considerate', 'abash', 'nosey', 'divisiveness', 'trump', 'captivating', 'jeopardize', 'lax', 'malice', 'promising', 'immodest', 'thoughtfully', 'unwillingly', 'quarrellously', 'obsess', 'asinine', 'compulsion', 'despotic', 'obviously', 'warmly', 'chagrin', 'torrent', 'rollercoaster', 'ploy', 'disinclination', 'precaution', 'invective', 'revulsion', 'rogue', 'glamorous', 'exemplary', 'dedicated', 'remarkably', 'lurk', 'morbidly', 'reminiscent', 'lonesome', 'deject', 'disquiet', 'absolute', 'despairing', 'troubling', 'kindliness', 'saintly', 'improbably', 'everlasting', 'erudite', 'debate', 'unusually', 'maniac', 'reverently', 'awesomeness', 'whips', 'indubitably', 'dismissive', 'embarrassment', 'unattractive', 'cannibal', 'unrest', 'facts', 'smuttiest', 'stifle', 'misfortune', 'worrier', 'daunting', 'joy', 'weaknesses', 'throttle', 'distortion', 'incendiary', 'indisposed', 'absent-minded', 'depraved', 'asininely', 'disrespectable', 'radiant', 'splayed-finger', 'destitute', 'haste', 'cruel', 'monotonous', 'amusingly', 'outstanding', 'abomination', 'bravery', 'discourage', 'disgustedly', 'grouse', 'dazzling', 'infallibility', 'infuriate', 'jittery', 'discriminatory', 'baby', 'inessential', 'loveless', 'prefer', 'depravedly', 'charming', 'prodigiously', 'thus', 'despotism', 'valuable', 'incredibly', 'mordant', 'believe', 'avarice', 'discouragement', 'acumen', 'frankly', 'insolently', 'grind', 'harried', 'ingenuity', 'disservice', 'gloatingly', 'jealously', 'motivated', 'bitter', 'forbidden', 'fancy', 'mournful', 'smother', 'allegorize', 'congratulate', 'indignity', 'elation', 'botch', 'plainly', 'trepidation', 'acerbic', 'undoubtedly', 'lackeys', 'blatantly', 'appreciative', 'faithfully', 'unscrupulously', 'discouragingly', 'mordantly', 'harridan', 'lackadaisical', 'positively', 'declaim', 'hm', 'dubitable', 'loathe', 'dishonorable', 'distinguished', 'debacle', 'fell', 'alluringly', 'villainous', 'gasp', 'sardonically', 'pernicious', 'rejection', 'exquisite', 'immensely', 'obscurity', 'indisputable', 'reprehensive', 'dreary', 'slime', 'renunciation', 'jeers', 'stricken', 'unorthodoxy', 'feckless', 'calamitously', 'unfriendly', 'lawlessness', 'war-like', 'disapprove', 'sloppy', 'improperly', 'plead', 'besmirch', 'ejaculate', 'insolent', 'rapture', 'absurdly', 'fortune', 'jest', 'lovable', 'indoctrination', 'admit', 'ax', 'mockeries', 'apathetically', 'elan', 'palatial', 'irreplacible', 'superfluous', 'farcical-yet-provocative', 'little-known', 'selfishness', 'fidget', 'really', 'chaos', 'dissention', 'sociable', 'swear', 'malcontented', 'sincere', 'downcast', 'incorrectly', 'thirst', 'nightmarish', 'activist', 'distastefully', 'grandeur', 'iniquitous', 'vital', 'favour', 'apologist', 'worth-while', 'exaltedly', 'one-side', 'terrified', 'therefore', 'dragoon', 'alliances', 'sunder', 'time-honored', 'prejudice', 'depressingly', 'frightful', 'rant', 'denunciations', 'self-coup', 'discern', 'agreeable', 'arousing', 'contend', 'sloppily', 'convincing', 'erratically', 'pitilessly', 'cringe', 'whimsical', 'savages', 'admittedly', 'temptation', 'honorable', 'pray', 'manly', 'compulsive', 'pacifists', 'fractiously', 'imaginative', 'undaunted', 'comprehend', 'abhorred', 'nicely', 'gall', 'inestimably', 'set-up', 'bastard', 'cruelty', 'exact', 'refreshing', 'misinterpret', 'amazement', 'discourteous', 'dissonance', 'imply', 'insensitive', 'nervously', 'condolence', 'adolescents', 'indecorum', 'lucky', 'obey', 'oppressiveness', 'disgruntled', 'inconsolably', 'objection', 'imploringly', 'perverted', 'spilling', 'seriously', 'tolerance', 'ascertainable', 'impervious', 'inopportune', 'egregious', 'immediately', 'unimaginable', 'forebodingly', 'truthfulness', 'outshine', 'baffle', 'hawkish', 'violator', 'deceitfulness', 'surely', 'aspersions', 'awed', 'despair', 'fastidious', 'merriment', 'squash', 'sumptuous', 'reverence', 'rumbling', 'banishment', 'dehumanization', 'screech', 'sin', 'smoldering', 'captivate', 'solicitously', 'sophisticated', 'furious', 'mollify', 'startle', 'gaiety', 'smolder', 'abidance', 'lividly', 'reluctance', 'doldrums', 'brazenness', 'disrupt', 'screamingly', 'humbling', 'first-rate', 'resounding', 'woe', 'apprehension', 'assassin', 'demoralizingly', 'disconsolately', 'impulsive', 'mysteriously', 'vibrant', 'indulgence', 'startlingly', 'earnestness', 'unsuspecting', 'pleasurably', 'adoringly', 'entertaining', 'mediocre', 'novel', 'dissolute', 'ascendant', 'bitingly', 'grievances', 'darling', 'unsettling', 'acridness', 'fretful', 'hopelessness', 'heedless', 'illuminate', 'insinuating', 'overbearingly', 'aberration', 'passive', 'demon', 'gainfully', 'discombobulate', 'spoil', 'stride', 'villianously', 'degrade', 'momentous', 'thrilling', 'repulsively', 'staid', 'idolized', 'antithetical', 'happy', 'ecstatically', 'contentment', 'disconsolate', 'toil', 'mad', 'valiant', 'wretchedly', 'overplay', 'nationalism', 'abusive', 'relentless', 'cry', 'ambivalence', 'dispiritedly', 'spitefulness', 'inescapable', 'bolster', 'forbid', 'inconceivably', 'excessive', 'unfazed', 'best', 'fun', 'paranoid', 'heathen', 'gawky', 'manic', 'agonize', 'ideal', 'irony', 'preaching', 'worthless', 'scathingly', 'optimism', 'brainy', 'ferocity', 'accuse', 'forgiveness', 'smile', 'supportive', 'eulogize', 'erroneous', 'genocide', 'wry', 'indelibly', 'flout', 'ignominious', 'joke', 'foreboding', 'precipitate', 'upliftingly', 'fastuous', 'grandiose', 'concede', 'blister', 'unclean', 'congratulatory', 'indiscreetly', 'obscenity', 'maliciously', 'arrogant', 'heavily', 'jeeringly', 'disapproval', 'misjudgment', 'enjoyably', 'sneering', 'heartrending', 'metaphorize', 'monumental', 'anti-proliferation', 'knowing', 'pertinaciously', 'farcical', 'devotion', 'fascinating', 'inveigle', 'lively', 'scarcely', 'agonizingly', 'excitingly', 'harassment', 'invidiousness', 'festive', 'disconcerting', 'subservience', 'forget', 'inspirational', 'infuriating', 'outrage', 'low-rated', 'filthy', 'downer', 'handily', 'impure', 'bragger', 'unprecedent', 'luxurious', 'slaves', 'daunt', 'infuriated', 'gossip', 'sneeringly', 'unabashed', 'shrewd', 'grievous', 'flourishing', 'shyly', 'impolitic', 'condescendingly', 'disbelief', 'exorbitantly', 'appealing', 'fervently', 'languor', 'blatant', 'righteousness', 'depressed', 'imbalance', 'fulminate', 'desertion', 'disappointment', 'long', 'quaint', 'lyrical', 'avariciously', 'rightful', 'seamless', 'contort', 'humility', 'wheedle', 'sardonic', 'insistent', 'disruptive', 'aver', 'surpass', 'ungovernable', 'spoon-fed', 'shameful', 'massacre', 'famished', 'ingenuously', 'merit', 'enormous', 'kook', 'bestial', 'compliant', 'obnoxiously', 'inglorious', 'shockingly', 'tyrant', 'apprehensive', 'infallible', 'distracting', 'rumours', 'vexing', 'emaciated', 'menacing', 'admirably', 'enjoin', 'belabor', 'calumniously', 'irredeemable', 'mawkishness', 'rid', 'grieve', 'wholeheartedly', 'pillory', 'fears', 'felicity', 'stinging', 'parody', 'obese', 'mislead', 'pride', 'profoundly', 'befriend', 'bereavement', 'cure-all', 'impetuous', 'gaga', 'awesome', 'affirmation', 'chastisement', 'fusty', 'lousy', 'inappropriately', 'disrespectfully', 'stressful', 'kindness', 'disturbing', 'preferences', 'ruthless', 'loveliness', 'atrophy', 'barbarously', 'debaucher', 'awe', 'indiscriminately', 'quibble', 'rubbish', 'dupe', 'autocrat', 'fearlessly', 'implausibly', 'moderate', 'unreliable', 'barbarically', 'decrepitude', 'pleading', 'tediously', 'treasure', 'bother', 'apprehensions', 'insensitively', 'joker', 'last-ditch', 'tranquil', 'inaptitude', 'lucidly', 'rotten', 'unwelcome', 'amiable', 'obsessiveness', 'smut', 'cataclysmically', 'impious', 'appreciatively', 'finely', 'smugly', 'undependable', 'regretfully', 'magical', 'misguide', 'triumph', 'bemused', 'bizarre', 'funny', 'mistakes', 'curiously', 'enslave', 'obnoxious', 'gripe', 'masters', 'bogus', 'maladjusted', 'creative', 'disdain', 'provocative', 'refuse', 'bowdlerize', 'playfully', 'refuge', 'peculiar', 'daringly', 'hypocrisy', 'disappoint', 'remorseless', 'falsehood', 'viciously', 'lifeblood', 'rightfully', 'conceivable', 'enthrall', 'lurid', 'multi-polarization', 'absolve', 'villian', 'wholesome', 'exhilaration', 'thrills', 'incapable', 'fact', 'gratitude', 'incognizant', 'glisten', 'sinful', 'awfulness', 'blaspheme', 'devastatingly', 'knowledgeable', 'positiveness', 'cherish', 'imminence', 'conciliatory', 'direness', 'heckle', 'devastate', 'skeptically', 'tantamount', 'premeditated', 'devotee', 'incomprehensible', 'worthlessly', 'hamper', 'solidarity', 'dragon', 'furthermore', 'inhuman', 'plebeian', 'indignant', 'mortify', 'sad', 'austere', 'horrendously', 'imposers', 'pillar', 'disconcertingly', 'seriousness', 'stupendous', 'frenetically', 'temerity', 'stylish', 'antiquated', 'antagonistic', 'prosecute', 'delirium', 'guilt', 'dishearten', 'salute', 'hateful', 'trickery', 'chide', 'uproarously', 'hearten', 'potent', 'prophesy', 'nag', 'sloth', 'excruciatingly', 'lewdness', 'inherent', 'venomous', 'impeach', 'prideful', 'baffled', 'merciful', 'hegemonistic', 'contradictory', 'stubbornly', 'irate', 'ghastly', 'exonerate', 'adulatory', 'evaluate', 'ingratiate', 'perspicuously', 'overstatement', 'suspicions', 'unjustifiable', 'solicitous', 'condescend', 'star', 'unkind', 'qualms', 'best-known', 'courageousness', 'yawn', 'ill-usage', 'astound', 'incivility', 'irrefutably', 'disconcert', 'beast', 'quite', 'frightening', 'dissidence', 'longing', 'incomprehension', 'adulteration', 'geezer', 'inextricably', 'wretched', 'hilarious', 'notable', 'implacable', 'facetiously', 'complicit', 'belligerence', 'overcome', 'immensurable', 'baseless', 'enthusiastically', 'horrific', 'damned', 'incisiveness', 'regal', 'urge', 'anarchism', 'prevaricate', 'confute', 'handy', 'bias', 'shimmering', 'sickeningly', 'dissent', 'lacking', 'proficient', 'candor', 'promise', 'monstrously', 'inferior', 'antipathy', 'realistic', 'scoffingly', 'triumphantly', 'unworthy', 'nightmarishly', 'meager', 'workable', 'precipitous', 'stiflingly', 'damnably', 'unfavorable', 'understatedly', 'grisly', 'grouch', 'praiseworthy', 'puzzling', 'remarkable', 'envy', 'macabre', 'peeved', 'persuasively', 'virus', 'enduring', 'defiler', 'shrewdly', 'so-called', 'engrossing', 'famine', 'well-regarded', 'miracle', 'personality', 'mightily', 'ostracize', 'patronize', 'posh', 'luxuriant', 'extremely', 'persecute', 'disoriented', 'impetuously', 'disvalue', 'fraught', 'listless', 'soothingly', 'fatuously', 'reflective', 'effusiveness', 'drain', 'utter', 'desirable', 'frustrate', 'ingrate', 'conveniently', 'inestimable', 'laudable', 'outrages', 'pleased', 'wary', 'craze', 'unrelenting', 'smouldering', 'well-connected', 'madder', 'amiabily', 'audaciously', 'malevolence', 'pro-beijing', 'incredible', 'supporter', 'distress', 'fateful', 'insightfully', 'despite', 'mockery', 'overstatements', 'sprightly', 'bliss', 'uncommon', 'evidently', 'uneasy', 'think', 'infamously', 'delectable', 'toughness', 'respectfully', 'cuplrit', 'objections', 'debauch', 'merriness', 'nuisance', 'rave', 'inconveniently', 'sympathetically', 'brutally', 'dissatisfaction', 'brutalising', 'pledge', 'frivolous', 'absolutely', 'uneasily', 'incompetence', 'destabilisation', 'willing', 'dishonest', 'overbearing', 'anti-israeli', 'almighty', 'smitten', 'unlamentable', 'brutality', 'misuse', 'preponderance', 'avid', 'disgustfully', 'abjectly', 'amicably', 'distressed', 'fascist', 'suppose', 'excoriate', 'condemnation', 'harmonious', 'exultation', 'tenacious', 'coerce', 'grudging', 'enthral', 'proscription', 'disregard', 'enormity', 'egomania', 'sinisterly', 'recommend', 'emotional', 'estranged', 'shiny', 'fixer', 'marvels', 'affront', 'swipe', 'merry', 'zealously', 'inefficacious', 'sobering', 'detrimental', 'unyielding', 'critical', 'yes', 'damn', 'hallowed', 'grouchy', 'believable', 'heartbreak', 'lascivious', 'moralize', 'officious', 'picky', 'unrelentingly', 'nonsense', 'punctual', 'waning', 'unlamentably', 'calumniate', 'forlorn', 'halcyon', 'laugh', 'miraculousness', 'fussy', 'statuesque', 'advantageous', 'horror', 'demise', 'unfaithfully', 'persuasive', 'betrayal', 'befoul', 'cataclysmal', 'fearful', 'gorgeously', 'immorality', 'highlight', 'honestly', 'arousal', 'intelligent', 'lambast', 'effortlessly', 'intrusive', 'disagreement', 'perversion', 'especially', 'fallaciously', 'mistaken', 'pinnacle', 'shyness', 'sorrowful', 'upsettingly', 'disrespecting', 'bewilderingly', 'vagrant', 'mischief', 'deprave', 'taunts', 'actual', 'notorious', 'poeticize', 'thankful', 'blithe', 'dubiously', 'overzealous', 'revile', 'grotesque', 'astonished', 'effusive', 'worth', 'cheat', 'inconsiderate', 'felt', 'remorsefully', 'confrontation', 'bargain', 'abject', 'extravagant', 'enthusiasm', 'failing', 'drastic', 'heartless', 'irreverently', 'lavishly', 'poised', 'shame', 'simplistically', 'terrific', 'defiance', 'disloyalty', 'constancy', 'miraculous', 'inextricable', 'havoc', 'stupor', 'blunders', 'remunerate', 'shatter', 'disorient', 'greatest', 'bothersome', 'hilarity', 'offensiveness', 'nettlesome', 'disarray', 'stellarly', 'ultimate', 'inappropriate', 'bountiful', 'confident', 'nastiness', 'ominous', 'rife', 'beauteous', 'mortification', 'roadblocks', 'stately', 'conspicuous', 'impressive', 'mendacious', 'horrors', 'enchanting', 'smiling', 'downside', 'unwise', 'regret', 'dastard', 'discontinuity', 'elatedly', 'finally', 'foolish', 'hopeful', 'impulsively', 'fastidiously', 'inhospitable', 'pretence', 'profanity', 'unbearablely', 'blah', 'violate', 'excitedness', 'oddly', 'tempt', 'ecstasies', 'disgruntle', 'illusions', 'dissocial', 'paradoxical', 'backward', 'flaunt', 'inexplainable', 'dextrous', 'elegant', 'ebullience', 'get-rich', 'imperious', 'bloodshed', 'flawlessly', 'relentlessness', 'righteous', 'will', 'maybe', 'respectful', 'fabulously', 'fervidly', 'puny', 'slanderer', 'pillage', 'puppets', 'surprising', 'mocking', 'indicative', 'exacerbate', 'inept', 'pacifist', 'ill-used', 'argumentative', 'mockingly', 'opinion', 'maddening', 'overstate', 'unkindly', 'vexation', 'scrutinize', 'illusion', 'cheater', 'plaything', 'majesty', 'preach', 'savage', 'ill-natured', 'lamentable', 'malicious', 'soliloquize', 'cannibalize', 'drunken', 'manifest', 'humiliation', 'plush', 'distasteful', 'staggeringly', 'exasperatingly', 'appallingly', 'wisdom', 'burn', 'enviously', 'desperate', 'futilely', 'incompetent', 'togetherness', 'perfect', 'divisively', 'goddamn', 'diabolically', 'standstill', 'preference', 'misbehave', 'misgivings', 'phobia', 'moribund', 'traumatically', 'humour', 'wickedness', 'darn', 'coax', 'hamstrung', 'illumine', 'dissonantly', 'frantic', 'eternity', 'cheerless', 'exquisitely', 'sarcastic', 'advocate', 'impertinent', 'polluters', 'quarrel', 'persuade', 'calumniation', 'accusing', 'malodorous', 'titillating', 'charm', 'overdo', 'frustrated', 'apologists', 'uncompromising', 'enjoyable', 'rue', 'procrastinate', 'unreliability', 'nastily', 'venerably', 'hope', 'vengeful', 'crave', 'shrivel', 'demeaning', 'well-informed', 'outburst', 'exaltingly', 'incessantly', 'deceitful', 'myth', 'catalyst', 'incommensurate', 'convenient', 'cliched', 'desire', 'breathtaking', 'illogically', 'disgraced', 'ranting', 'scruples', 'upliftment', 'terrifyingly', 'inklings', 'victimize', 'insightful', 'bedlamite', 'vouch', 'incorrigibly', 'embarrass', 'catastrophe', 'gifted', 'exaggeration', 'partisans', 'revere', 'exorbitant', 'hotbeds', 'growl', 'magnificently', 'oppression', 'somber', 'timidly', 'doubtless', 'defiant', 'predicament', 'indecision', 'unease', 'incapably', 'hopefulness', 'affable', 'puzzlement', 'fearfully', 'unnervingly', 'amazing', 'deceitfully', 'cloud', 'concur', 'dissidents', 'exactly', 'harmless', 'heroine', 'accusations', 'fashionable', 'defamatory', 'imbecile', 'matchless', 'self-criticism', 'please', 'presume', 'dazzle', 'epitome', 'wiles', 'cutthroat', 'trumpet', 'boggle', 'emptiness', 'backbiting', 'covetously', 'hoard', 'endear', 'bewilder', 'unfortunately', 'preferable', 'rude', 'insupportably', 'dissension', 'egotism', 'rampage', 'staggering', 'luckiest', 'dodge', 'maddeningly', 'pompous', 'condemn', 'mischievously', 'miser', 'selfinterested', 'stressfully', 'scariest', 'emasculate', 'reprehension', 'recklessly', 'inordinate', 'decrepit', 'infringements', 'indecisively', 'determination', 'downfall', 'keenly', 'preferably', 'corruption', 'breathlessness', 'weakness', 'smack', 'disapproving', 'imperative', 'loyalty', 'sugarcoated', 'defile', 'discerning', 'spurn', 'charismatic', 'eclectic', 'unjustifiably', 'fleer', 'hatefulness', 'heartfelt', 'remorselessly', 'torturously', 'zealous', 'kingmaker', 'nurture', 'misleadingly', 'dissatisfied', 'madness', 'phony', 'appreciation', 'squabbling', 'ugly', 'irresponsibly', 'infernal', 'sanctuary', 'pithy', 'deviousness', 'conspire', 'admirer', 'battle-lines', 'perturb', 'air', 'fabulous', 'anyhow', 'misrepresent', 'precious', 'irately', 'exclaim', 'exceptionally', 'mistrust', 'ravage', 'soothe', 'tantalizingly', 'tempting', 'inkling', 'solicitude', 'altruistic', 'upheaval', 'adored', 'moan', 'egocentric', 'defender', 'coupists', 'oppose', 'unacceptable', 'irredeemably', 'saintliness', 'unworkable', 'hardball', 'harmful', 'embrace', 'mindful', 'abhorrent', 'delusional', 'friction', 'glimmer', 'ought', 'scornful', 'woefully', 'bewildered', 'biases', 'friendly', 'blunder', 'agonizing', 'steadfastness', 'famed', 'mar', 'mourn', 'sumptuousness', 'unanimous', 'contemptuously', 'inspire', 'virtuous', 'entice', 'begging', 'inhumane', 'self-interested', 'vie', 'eagerly', 'imprudent', 'profess', 'relent', 'rapturous', 'impinge', 'nightmare', 'ramshackle', 'abominably', 'panic', 'dire', 'madden', 'pauper', 'rewardingly', 'lament', 'rampant', 'insidious', 'impiety', 'hug', 'gratuitously', 'flagrant', 'languish', 'contrive', 'disaffirm', 'entrenchment', 'gloomy', 'gratification', 'penetrating', 'scandalously', 'suffocate', 'superiority', 'feisty', 'marvelousness', 'defunct', 'bemoan', 'fainthearted', 'severely', 'subservient', 'surmount', 'admonisher', 'turmoil', 'vengefulness', 'glare', 'traumatize', 'doomsday', 'bullies', 'featly', 'trusting', 'indescribably', 'groundbreaking', 'must', 'courageously', 'heady', 'indeed', 'shiver', 'unwisely', 'insouciance', 'proclaim', 'bereft', 'shun', 'dramatically', 'inarguably', 'recalcitrant', 'wayward', 'zeal', 'inconceivable', 'mania', 'mournfully', 'discordant', 'disconsolation', 'coldly', 'placate', 'sacrifice', 'overdue', 'untrue', 'aggrieved', 'brashness', 'consequently', 'edify', 'wish', 'stirring', 'scummy', 'horrendous', 'disaffected', 'menace', 'admiration', 'complaining', 'ferociously', 'enchant', 'inexpiable', 'humourous', 'idiotically', 'ugh', 'disgusting', 'opposition', 'tolerable', 'shortchange', 'trivially', 'payback', 'bait', 'spiritless', 'contemptible', 'instigator', 'view', 'heartbreaking', 'fortuitously', 'impaired', 'aha', 'incessant', 'might', 'avenge', 'disgusted', 'intimidating', 'rapt', 'amicable', 'boring', 'defrauding', 'fickle', 'fidelity', 'flattering', 'ruthlessness', 'goad', 'wink', 'encouragement', 'stooge', 'pressing', 'criticisms', 'euphoric', 'obtrusive', 'supreme', 'desperation', 'barbarous', 'phenomenally', 'anarchist', 'browbeat', 'correctly', 'deprived', 'need', 'fathomless', 'infamy', 'amuse', 'wanton', 'hothead', 'bedlam', 'court', 'tact', 'devastating', 'foremost', 'pardon', 'mindlessly', 'hellion', 'aspire', 'deeply', 'weariness', 'blinding', 'worries', 'brutal', 'sympathies', 'inculcate', 'anarchy', 'asperse', 'untruthful', 'beleaguer', 'maverick', 'stew', 'yearn', 'frenetic', 'overacted', 'vehemently', 'enfeeble', 'grieving', 'hysterics', 'dastardly', 'renounce', 'unruly', 'lying', 'impasse', 'infuriatingly', 'detriment', 'impolitely', 'miracles', 'improvise', 'impregnable', 'meaningful', 'vindicate', 'complaints', 'unhappiness', 'betray', 'veritable', 'aghast', 'extraordinarily', 'unspeakable', 'despicable', 'hypocrites', 'glower', 'drunk', 'pertinacious', 'condescension', 'fictitious', 'drones', 'harmoniously', 'indoctrinate', 'misguided', 'resourceful', 'irrepressible', 'doubtful', 'tainted', 'ineptitude', 'haggle', 'belligerent', 'beautiful', 'fudge', 'entirely', 'motivation', 'astonishment', 'garish', 'pretentiously', 'cumbersome', 'wonderous', 'eager', 'monstrosities', 'hollow', 'revolting', 'clearly', 'vengefully', 'pitifully', 'enticing', 'intrude', 'joyfully', 'profusion', 'fume', 'angrily', 'unacceptablely', 'outrageously', 'reassure', 'right', 'imposition', 'impressiveness', 'derisive', 'misbecoming', 'anyways', 'tribute', 'insult', 'rile', 'teasingly', 'antagonism', 'recklessness', 'delightfully', 'boisterous', 'dungeon', 'indomitably', 'contention', 'ordeal', 'persecution', 'audacity', 'altogether', 'exceed', 'jerk', 'maladjustment', 'stereotypical', 'desultory', 'evocative', 'delighted', 'radiance', 'aggrieve', 'annoyance', 'disavow', 'boast', 'joyless', 'paucity', 'unexpectedly', 'craving', 'infatuated', 'propitiously', 'apathy', 'berserk', 'exasperate', 'wrest', 'ecstatic', 'demoralizing', 'allusions', 'unsavory', 'grin', 'emphatically', 'laughingstock', 'exhort', 'gallant', 'misjudge', 'foolishly', 'sanity', 'autocratic', 'condolences', 'feel', 'attractively', 'poignant', 'second-class', 'compensate', 'hideous', 'eminence', 'poetic', 'absurdness', 'regard', 'scandals', 'splendidly', 'calumnious', 'thoughtlessly', 'impurity', 'deploringly', 'polarisation', 'swamped', 'idiocy', 'bewail', 'fault', 'thrillingly', 'aggravating', 'harm', 'deepening', 'implode', 're-conquest', 'resplendent', 'astronomically', 'worrisome', 'peculiarly', 'jubilant', 'knew', 'electrification', 'loathsomely', 'serenity', 'stupidity', 'pathetic', 'disastrously', 'ridicule', 'earnestly', 'horribly', 'scheming', 'irresistibly', 'onerous', 'ill-mannered', 'bewitch', 'covetous', 'nebulous', 'radicals', 'compassion', 'immoderate', 'irritable', 'farcically', 'meanness', 'derision', 'forgetfully', 'immensity', 'smokescreen', 'complain', 'meddle', 'tantalizing', 'upbraid', 'dizzy', 'exaltation', 'unnaturally', 'honesty', 'lovelorn', 'offend', 'dismissively', 'stuffy', 'hedonistic', 'romantic', 'lewd', 'oblivious', 'distraughtly', 'assent', 'conscientious', 'touchy', 'superstitious', 'inferiority', 'actually', 'disquieting', 'travesties', 'martyrdom', 'expire', 'dreadfully', 'posturing', 'perfection', 'hilariously', 'cajole', 'floored', 'repulsing', 'insociable', 'lust', 'admirable', 'sworn', 'evildoer', 'outstandingly', 'quarrellous', 'rejoicing', 'mercifully', 'impressively', 'pretense', 'heck', 'adulterate', 'exceeding', 'pout', 'lurch', 'criticize', 'downfallen', 'gravely', 'struggle', 'acerbate', 'disapprobation', 'monotony', 'shambles', 'defiantly', 'harmonize', 'tolerably', 'tragic', 'disgracefully', 'brazenly', 'sacred', 'flatter', 'asunder', 'villianous', 'joyful', 'extravagantly', 'despondency', 'blindside', 'cataclysm', 'furor', 'importunate', 'bewildering', 'tortuous', 'forswear', 'jaundiced', 'haven', 'justifiably', 'strangest', 'err', 'neglected', 'hatefully', 'plausibility', 'hoodwink', 'picturesque', 'deference', 'bitch', 'fiasco', 'impatiently', 'needful', 'satisfactorily', 'praise', 'simplistic', 'perplex', 'prosper', 'extortion', 'frictions', 'kaput', 'nimble', 'skulk', 'stigmatize', 'warily', 'complacent', 'courteous', 'defamation', 'blab', 'merciless', 'cheap', 'uttermost', 'venomously', 'smutty', 'glitter', 'fascism', 'desecrate', 'lure', 'regardless', 'sensible', 'redemption', 'presumptuous', 'sparkle', 'worst', 'undignified', 'bitchy', 'angriness', 'repent', 'compelling', 'divinely', 'richness', 'suspicious', 'diabolic', 'unhappy', 'banal', 'entrap', 'tattered', 'disagree', 'twinkly', 'exultingly', 'irreversible', 'courtly', 'consternation', 'pretend', 'contempt', 'gladness', 'salacious', 'cheer', 'desolation', 'stigma', 'debasement', 'traitorously', 'self-serving', 'fallacious', 'despondent', 'troublingly', 'deprive', 'appreciate', 'cuss', 'exterminate', 'loot', 'quash', 'clear-cut', 'bickering', 'dodgey', 'anxiously', 'dauntless', 'knowingly', 'stress', 'brutalizing', 'sumptuously', 'hail', 'hardier', 'immorally', 'miraculously', 'glistening', 'vicious', 'evil', 'unbelievably', 'congratulations', 'convoluted', 'heros', 'leech', 'conspiratorial', 'famously', 'greatness', 'afflictive', 'indolent', 'admire', 'iniquity', 'broken-hearted', 'anxiousness', 'amour', 'desiccated', 'memorialize', 'emphatic', 'justification', 'humane', 'grudges', 'adventuresome', 'emotion', 'fallacies', 'neurotically', 'swore', 'fearless', 'irregardless', 'fake', 'advocacy', 'disreputable', 'wrath', 'rank', 'warmhearted', 'inefficacy', 'perseverance', 'barbarity', 'undermine', 'propitious', 'blasphemous', 'hysterically', 'consider', 'obliterated', 'dauntingly', 'repugnantly', 'excruciating', 'enjoyment', 'extemporize', 'inexplicable', 'flounder', 'anti-', 'prognosticate', 'heroically', 'relief', 'hysteria', 'foretell', 'sass', 'sully', 'contrariness', 'uproarious', 'attitude', 'nurturing', 'graciously', 'triumphal', 'contrary', 'dazed', 'privileged', 'bashful', 'burdensome', 'stylishly', 'unscrupulous', 'villains', 'majestic', 'slap', 'harms', 'forgiving', 'capriciousness', 'thinkable', 'nuances', 'oppressive', 'unwanted', 'oh', 'anti-white', 'rash', 'unconvincing', 'sensibly', 'pro-cuba', 'seductive', 'illogical', 'profane', 'raging', 'unjust', 'dissonant', 'usurper', 'woebegone', 'courage', 'embodiment', 'grossly', 'humankind', 'hapless', 'condemnable', 'deceit', 'ragged', 'tiresome', 'treasonous', 'anti-american', 'trivial', 'thank', 'paradoxically', 'scorn', 'cocky', 'agreeability', 'cross', 'glib', 'compassionate', 'dismaying', 'drab', 'braggart', 'fidgety', 'ineloquently', 'alluring', 'agony', 'humiliate', 'attraction', 'strangely', 'mudslinging', 'inescapably', 'appalling', 'ovation', 'compliment', 'scars', 'shrew', 'startling', 'gladly', 'lofty', 'thusly', 'dismalness', 'abominate', 'indelicate', 'licentious', 'blabber', 'irresolute', 'vex', 'confessions', 'unjustly', 'dogmatic', 'thorny', 'ingenuous', 'petty', 'ingenious', 'bleakly', 'lucid', 'pitiable', 'barbaric', 'biting', 'tyranny', 'solace', 'mortifying', 'ardent', 'degrading', 'domination', 'nauseatingly', 'extensively', 'flawless', 'incredulous', 'offending', 'terror-genic', 'impede', 'immoderately', 'candid', 'sage', 'repudiation', 'lackluster', 'opulent', 'fortunately', 'embroilment', 'anxieties', 'frown', 'grate', 'undue', 'coherence', 'unfair', 'chivalrous', 'frenzied', 'kooky', 'fawn', 'hysterical', 'decay', 'propagandize', 'rapport', 'little', 'scrupulous', 'frazzled', 'admiring', 'pessimistic', 'provocation', 'serene', 'understated', 'fortress', 'know', 'hamstring', 'ominously', 'surprise', 'awareness', 'grievance', 'wonderful', 'tolerantly', 'epic', 'busybody', 'losing', 'abide', 'regrettably', 'traitor', 'polluter', 'enticingly', 'chaotic', 'paupers', 'impropriety', 'obsessively', 'mesmerizing', 'resentment', 'heroize', 'glorious', 'breathtakingly', 'fantastic', 'ineptly', 'infringe', 'altruist', 'intransigent', 'lackey', 'liars', 'goof', 'heartbreakingly', 'marvel', 'bloodthirsty', 'insubstantial', 'nobly', 'carelessness', 'punish', 'lazy', 'scrutiny', 'derogatory', 'righten', 'exasperation', 'zest', 'effigy', 'glad', 'supremely', 'bloated', 'credible', 'ignominy', 'frightfully', 'salutary', 'gratifyingly', 'visionary', 'blasted', 'arouse', 'inaccurate', 'servitude', 'joyous', 'sputter', 'consent', 'erratic', 'condone', 'deception', 'explicit', 'hater', 'ingeniously', 'injudicious', 'assiduous', 'loneliness', 'effusively', 'boastful', 'hypocrite', 'insinuate', 'domineer', 'daze', 'immovable', 'seemingly', 'exuberantly', 'deny', 'upside', 'spook', 'blasphemy', 'dreadfulness', 'offbeat', 'stupify', 'butchery', 'fib', 'pledges', 'inexorable', 'awkwardness', 'dangle', 'relish', 'narrower', 'strut', 'fondly', 'traduce', 'stingingly', 'deserving', 'unthinkable', 'inflammatory', 'pratfall', 'delusion', 'feels', 'absurdity', 'rectifying', 'spookier', 'mendacity', 'embellish', 'intimacy', 'unhappily', 'glow', 'morality', 'mesmerize', 'bleakness', 'reject', 'denial', 'blameless', 'insidiously', 'indecisive', 'desirous', 'pro-peace', 'squabble', 'confession', 'obscenely', 'lavish', 'idiots', 'mangle', 'celebrated', 'plotters', 'sucker', 'willful', 'foulness', 'overact', 'idiotic', 'objectionable', 'mum', 'dizzingly', 'memorable', 'rage', 'forgave', 'inspiration', 'gainsay', 'prudent', 'jubilantly', 'courageous', 'drama', 'killjoy', 'hinder', 'slothful', 'intransigence', 'surmise', 'blurt', 'twist', 'weaken', 'hardships', 'virtuously', 'tiring', 'weakening', 'notably', 'exaggerate', 'misconception', 'gracelessly', 'neglect', 'dare', 'repay', 'shortsightedness', 'inequality', 'confused', 'spellbinding', 'gusto', 'guiltily', 'tidy', 'stereotype', 'spiritual', 'pessimistically', 'beneficially', 'simmer'}\n"
     ]
    }
   ],
   "source": [
    "lexicon_path=os.path.join(folder_path, \"data/subjectivity_clues_hltemnlp05/subjclueslen1-HLTEMNLP05.tff\")\n",
    "def load_mpqa_lexicon(path):\n",
    "    subj_dict = {}\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            entry = {}\n",
    "            for p in parts:\n",
    "                if '=' in p:\n",
    "                    key, value = p.split('=', 1)\n",
    "                    entry[key] = value\n",
    "            word = entry.get(\"word1\", \"\").lower()\n",
    "            subj_type = entry.get(\"type\", \"\")\n",
    "            polarity = entry.get(\"priorpolarity\", \"\")\n",
    "            if word:\n",
    "                subj_dict[word] = {\"type\": subj_type, \"polarity\": polarity}\n",
    "    return subj_dict\n",
    "mpqa_dict = load_mpqa_lexicon(lexicon_path)\n",
    "print(\"Sample:\", list(mpqa_dict.items())[:10])\n",
    "types = sorted({v[\"type\"] for v in mpqa_dict.values()})\n",
    "polarities = sorted({v[\"polarity\"] for v in mpqa_dict.values()})\n",
    "print(\"Types: \", types)\n",
    "print(\"polarities: \", polarities)\n",
    "subjective_words={k for k,v in mpqa_dict.items() if v.get(\"type\")==\"strongsubj\"}\n",
    "print(subjective_words)\n",
    "\n",
    "# Simple neutral replacements for some common ones\n",
    "neutral_map = {\n",
    "    \"ridiculous\": \"questionable\",\n",
    "    \"terrible\": \"poor\",\n",
    "    \"awful\": \"bad\",\n",
    "    \"horrible\": \"bad\",\n",
    "    \"great\": \"strong\",\n",
    "    \"amazing\": \"notable\",\n",
    "    \"good\": \"acceptable\",\n",
    "    \"bad\": \"weak\",\n",
    "    \"awesome\": \"notable\",\n",
    "    \"fantastic\": \"notable\",\n",
    "    \"trash\": \"low quality\",\n",
    "}\n",
    "import re\n",
    "def neutralize_one_sentence(text):\n",
    "    \"\"\"\n",
    "    Input: raw tweet string\n",
    "    Output:\n",
    "      source: \"neutralize: ...\" with <SUBJ> tags\n",
    "      target: neutralized sentence (no strong subjective words)\n",
    "      original: original text\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "\n",
    "    masked_tokens = []\n",
    "    target_tokens = []\n",
    "\n",
    "    for tok in tokens:\n",
    "        # core word without basic punctuation\n",
    "        core = re.sub(r\"[^\\w']\", \"\", tok).lower()\n",
    "\n",
    "        if core in subjective_words:\n",
    "            # keep punctuation around marker in input\n",
    "            # for example: \"ridiculous!\" -> \"<SUBJ>!\"\n",
    "            prefix_len = len(tok) - len(core)\n",
    "            if prefix_len > 0:\n",
    "                prefix = tok[:prefix_len]\n",
    "                suffix = tok[prefix_len + len(core):]\n",
    "            else:\n",
    "                prefix = \"\"\n",
    "                suffix = tok[len(core):]\n",
    "\n",
    "            masked_tok = f\"{prefix}<SUBJ>{suffix}\"\n",
    "            masked_tokens.append(masked_tok)\n",
    "\n",
    "            # target side: either soft neutral or drop the word\n",
    "            replacement = neutral_map.get(core, None)\n",
    "            if replacement is not None:\n",
    "                target_tokens.append(f\"{replacement}{suffix}\")\n",
    "            else:\n",
    "                # drop the word, keep punctuation if any\n",
    "                replacement = neutral_map.get(core, \"<NEU>\")\n",
    "                target_tokens.append(f\"{replacement}{suffix}\")\n",
    "                # else skip\n",
    "        else:\n",
    "            masked_tokens.append(tok)\n",
    "            target_tokens.append(tok)\n",
    "\n",
    "    source = \"neutralize: \" + \" \".join(masked_tokens)\n",
    "    target = \" \".join(target_tokens)\n",
    "\n",
    "    return source, target, text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "executionInfo": {
     "elapsed": 1198,
     "status": "ok",
     "timestamp": 1763433909993,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "5gD2MQfzc5JG",
    "outputId": "73ba6d0a-4d63-48b5-cd1e-1296b80fc78f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"print(\\\"Train size:\\\", len(df_train))\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"input_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"neutralize: for people who don't <SUBJ> diy artattack\",\n          \"neutralize: - You probably just missed the text.\",\n          \"neutralize: dailymail readers being <SUBJ> as always shocker dailyfail inhuntspocket theyhatethenhs\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"for people who don't <NEU> diy artattack\",\n          \"- You probably just missed the text.\",\n          \"dailymail readers being <NEU> as always shocker dailyfail inhuntspocket theyhatethenhs\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"original_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"for people who don't understand diy artattack\",\n          \"- You probably just missed the text.\",\n          \"dailymail readers being sensible as always shocker dailyfail inhuntspocket theyhatethenhs\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-6dd856ce-8fdb-4e2d-8315-a15aea702055\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "      <th>original_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutralize: Be &lt;SUBJ&gt; dirty step to get money ...</td>\n",
       "      <td>Be &lt;NEU&gt; dirty step to get money staylight sta...</td>\n",
       "      <td>Be aware dirty step to get money staylight sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutralize: for people who don't &lt;SUBJ&gt; diy ar...</td>\n",
       "      <td>for people who don't &lt;NEU&gt; diy artattack</td>\n",
       "      <td>for people who don't understand diy artattack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutralize: dailymail readers being &lt;SUBJ&gt; as ...</td>\n",
       "      <td>dailymail readers being &lt;NEU&gt; as always shocke...</td>\n",
       "      <td>dailymail readers being sensible as always sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutralize: Why do I get the &lt;SUBJ&gt; you &lt;SUBJ&gt;...</td>\n",
       "      <td>Why do I get the &lt;NEU&gt; you &lt;NEU&gt; games?</td>\n",
       "      <td>Why do I get the feeling you like games?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutralize: - You probably just missed the text.</td>\n",
       "      <td>- You probably just missed the text.</td>\n",
       "      <td>- You probably just missed the text.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6dd856ce-8fdb-4e2d-8315-a15aea702055')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-6dd856ce-8fdb-4e2d-8315-a15aea702055 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-6dd856ce-8fdb-4e2d-8315-a15aea702055');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-928304d2-8dfb-4c52-8c9a-3ac8889140a0\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-928304d2-8dfb-4c52-8c9a-3ac8889140a0')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-928304d2-8dfb-4c52-8c9a-3ac8889140a0 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                          input_text  \\\n",
       "0  neutralize: Be <SUBJ> dirty step to get money ...   \n",
       "1  neutralize: for people who don't <SUBJ> diy ar...   \n",
       "2  neutralize: dailymail readers being <SUBJ> as ...   \n",
       "3  neutralize: Why do I get the <SUBJ> you <SUBJ>...   \n",
       "4   neutralize: - You probably just missed the text.   \n",
       "\n",
       "                                         target_text  \\\n",
       "0  Be <NEU> dirty step to get money staylight sta...   \n",
       "1           for people who don't <NEU> diy artattack   \n",
       "2  dailymail readers being <NEU> as always shocke...   \n",
       "3            Why do I get the <NEU> you <NEU> games?   \n",
       "4               - You probably just missed the text.   \n",
       "\n",
       "                                       original_text  \n",
       "0  Be aware dirty step to get money staylight sta...  \n",
       "1      for people who don't understand diy artattack  \n",
       "2  dailymail readers being sensible as always sho...  \n",
       "3           Why do I get the feeling you like games?  \n",
       "4               - You probably just missed the text.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 81382\n"
     ]
    }
   ],
   "source": [
    "# example: df_raw = pd.read_csv(\"your_tweet_file.csv\")\n",
    "\n",
    "sources = []\n",
    "targets = []\n",
    "originals = []\n",
    "\n",
    "for t in df[\"tweets\"]:\n",
    "    s, y, o = neutralize_one_sentence(str(t))\n",
    "    sources.append(s)\n",
    "    targets.append(y)\n",
    "    originals.append(o)\n",
    "\n",
    "df_train = pd.DataFrame(\n",
    "    {\n",
    "        \"input_text\": sources,\n",
    "        \"target_text\": targets,\n",
    "        \"original_text\": originals,\n",
    "    }\n",
    ")\n",
    "\n",
    "display(df_train.head())\n",
    "print(\"Train size:\", len(df_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 696,
     "status": "ok",
     "timestamp": 1763433917543,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "Q5biIs_qh3xi",
    "outputId": "2ddaf94e-4b30-43bd-fb90-0aa3740d4019"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /content/drive/MyDrive/CS4650/project/data/WNC/processed/mpqa_lexicon_masking_input_target_original/source_target_pair_biased_word_train.csv\n"
     ]
    }
   ],
   "source": [
    "output_path = os.path.join(folder_path, \"data/WNC/processed/mpqa_lexicon_masking_input_target_original/source_target_pair_biased_word_train.csv\")\n",
    "output_dir = os.path.dirname(output_path)\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "df_train.to_csv(output_path, index=False)\n",
    "print(\"Saved:\", output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "f5de05e112814202944d4e9106be2083",
      "5eba54199b7b46038fb1a6008c3c77fc",
      "89ba717e73134964aa5359b627f40711",
      "ecab88c53400495aa431a50d4106ebfe",
      "517217c05785432f8edfa4220b2623c7",
      "530d3effe9514d8c8fda84074f71d674",
      "0e59e28f31034d9ea5161db8c4b3eee3",
      "bff9d52432f2470b8f2fc93404dd641a",
      "37a5e6c415f34650bbb0d57e176f1db6",
      "7d6038359c2f4a6599d63688f1e07c38",
      "ae456161b1f04b11a1bd48765661e93c"
     ]
    },
    "executionInfo": {
     "elapsed": 3109,
     "status": "ok",
     "timestamp": 1763433922996,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "sm_dKu4perfA",
    "outputId": "6a1d6be4-fa33-42a3-ffbd-193eb99f1dc5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5de05e112814202944d4e9106be2083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Embedding(32102, 768)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @title load dataset\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import T5TokenizerFast, T5ForConditionalGeneration\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=os.path.join(folder_path, \"data/WNC/processed/mpqa_lexicon_masking_input_target_original/source_target_pair_biased_word_train.csv\"))\n",
    "\n",
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# Add special tokens for safety\n",
    "special_tokens = [\"<SUBJ>\", \"<NEU>\"]  # we will rely on tokenizer splitting on \"=\" and \">\"\n",
    "\n",
    "tokenizer.add_tokens(special_tokens)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1763433925597,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "aTtx_YP4glUA",
    "outputId": "c45bb72c-98d2-46ac-d10b-e1b5fbb6b483"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_text', 'target_text', 'original_text'],\n",
      "        num_rows: 81382\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208,
     "referenced_widgets": [
      "fc98284225944bba8f0238b27731307c",
      "84334b32d531485baa04537cdcc837fa",
      "2e6316c662bd428f9b733cbfe5aa4b9c",
      "19722962d598405482928975278cad5b",
      "ceafd9537463401280fa75e3ad837c9a",
      "2258518d213342258ea5f78d8e05c1c1",
      "e896481db0674ebd8ea69c17b6c1f8a6",
      "92845f841c634c8a934513521a8d2ad7",
      "bf5a5b26b150489db52645048f1a7685",
      "16cbf39f36f84d0a994baa4de3dd342b",
      "a401e116741c4ec097ec55779b795da9"
     ]
    },
    "executionInfo": {
     "elapsed": 16717,
     "status": "ok",
     "timestamp": 1763434561580,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "POqdeZcqgoxR",
    "outputId": "233e9d03-715b-4598-a797-645c71b046e4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc98284225944bba8f0238b27731307c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/81382 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels', 'orig_text'],\n",
      "        num_rows: 81382\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# @title tokenize\n",
    "from datasets import DatasetDict\n",
    "def preprocess_fn(batch):\n",
    "    # Tokenize source (masked input)\n",
    "    model_inputs = tokenizer(\n",
    "        batch[\"input_text\"],\n",
    "        max_length=32,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Tokenize target (subjective words removed)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            batch[\"target_text\"],\n",
    "            max_length=32,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "\n",
    "    # Convert padding token in labels to -100 for CE loss masking\n",
    "    label_ids = labels[\"input_ids\"]\n",
    "    label_ids = [\n",
    "        [(tok if tok != tokenizer.pad_token_id else -100) for tok in seq]\n",
    "        for seq in label_ids\n",
    "    ]\n",
    "\n",
    "    model_inputs[\"labels\"] = label_ids\n",
    "\n",
    "    # Keep original sentence for semantic loss\n",
    "    model_inputs[\"orig_text\"] = batch[\"original_text\"]\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "tokenized_ds = dataset.map(\n",
    "    preprocess_fn,\n",
    "    batched=True,\n",
    "    remove_columns=['input_text', 'target_text', 'original_text']  # keep all or list only your 3 columns to drop after mapping\n",
    ")\n",
    "\n",
    "\n",
    "print(tokenized_ds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1763434567388,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "vscMufPtg2M_",
    "outputId": "c916a1f5-5429-4506-ccb9-c2a103cb2f22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [7163, 1737, 10, 493, 3, 32100, 13086, 1147, 12, 129, 540, 1049, 2242, 1049, 13698, 4854, 25797, 3320, 233, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [493, 3, 32101, 13086, 1147, 12, 129, 540, 1049, 2242, 1049, 13698, 4854, 25797, 3320, 233, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], 'orig_text': 'Be aware dirty step to get money staylight staywhite moralneeded @'}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_ds[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1763433996874,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "Z1Aa5sL6uSWN",
    "outputId": "abe0b1ca-1bd5-459f-986c-ed978ea203d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subjective token ids count: 1638\n"
     ]
    }
   ],
   "source": [
    "# @title build subjective words ids\n",
    "\n",
    "# Convert subjective words to token ids\n",
    "subj_token_ids = set()\n",
    "\n",
    "# You can limit this to a subset for speed if needed\n",
    "subj_list = list(subjective_words)\n",
    "\n",
    "enc = tokenizer(\n",
    "    subj_list,\n",
    "    add_special_tokens=False,\n",
    ")\n",
    "for ids in enc[\"input_ids\"]:\n",
    "    if len(ids) >= 1:\n",
    "        subj_token_ids.add(ids[0])\n",
    "\n",
    "subj_token_ids = sorted(list(subj_token_ids))\n",
    "print(\"Subjective token ids count:\", len(subj_token_ids))\n",
    "\n",
    "subj_token_ids_tensor = torch.tensor(subj_token_ids, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WamV8DCMg7ys"
   },
   "source": [
    "## finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GZ-lrjhaj1Cq"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "class NeutralRewriteTrainer(Seq2SeqTrainer):\n",
    "\n",
    "    def __init__(self, embedder, subj_lexicon, print_every=20, lambda_sem=1.0, lambda_neu=1.0, **kw):\n",
    "        super().__init__(**kw)\n",
    "        self.embedder = embedder.eval()\n",
    "        self.subj_lexicon = subj_lexicon\n",
    "        self.lambda_sem = lambda_sem\n",
    "        self.lambda_neu = lambda_neu\n",
    "\n",
    "        for p in self.embedder.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.ce_loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "        self.remove_unused_columns = False\n",
    "        self.print_every=print_every\n",
    "    # def _remove_unused_columns(self, dataset, description=None):\n",
    "    #     return dataset\n",
    "    def is_subj(self, word):\n",
    "        return word.lower() in self.subj_lexicon\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            labels=inputs[\"labels\"]\n",
    "        )\n",
    "        # print(inputs.keys())\n",
    "\n",
    "\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # CE loss\n",
    "        ce_loss = self.ce_loss_fn(\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            inputs[\"labels\"].view(-1)\n",
    "        )\n",
    "\n",
    "        # decode pred and original\n",
    "        pred_ids = logits.argmax(dim=-1)\n",
    "        pred_text = self.processing_class.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "        orig_text = inputs[\"orig_text\"]\n",
    "\n",
    "        # semantic preservation\n",
    "        with torch.no_grad():\n",
    "            emb_pred = self.embedder.encode(pred_text, convert_to_tensor=True).to(logits.device)\n",
    "            emb_orig = self.embedder.encode(orig_text, convert_to_tensor=True).to(logits.device)\n",
    "\n",
    "        cos = F.cosine_similarity(emb_pred, emb_orig)\n",
    "        semantic_loss = (1 - cos).mean()\n",
    "\n",
    "        # neutrality penalty\n",
    "        penalties = []\n",
    "        for sent in pred_text:\n",
    "            toks = sent.split()\n",
    "            subj_count = sum(self.is_subj(t) for t in toks)\n",
    "            penalties.append(subj_count / (len(toks) + 1e-9))\n",
    "\n",
    "        neutrality_penalty = torch.tensor(penalties, device=logits.device).mean()\n",
    "\n",
    "        total_loss = (\n",
    "            ce_loss\n",
    "            + self.lambda_sem * semantic_loss\n",
    "            + self.lambda_neu * neutrality_penalty\n",
    "        )\n",
    "\n",
    "        step = self.state.global_step\n",
    "        if step % self.print_every == 0:\n",
    "            print(\"\\n================ DEBUG ================\")\n",
    "            print(f\"Step {step}\")\n",
    "            print(f\"Total Loss:        {total_loss.item():.4f}\")\n",
    "            print(f\"CE Loss:           {ce_loss.item():.4f}\")\n",
    "            print(f\"Semantic Loss:     {semantic_loss.item():.4f}\")\n",
    "            print(f\"Neutrality Loss:   {neutrality_penalty.item():.4f}\")\n",
    "\n",
    "            # Print first example in batch\n",
    "            print(\"Source:\", orig_text[0])\n",
    "            print(\"Generated:\", pred_text[0])\n",
    "            print(\"=======================================\\n\")\n",
    "\n",
    "\n",
    "        return (total_loss, outputs) if return_outputs else total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3244,
     "status": "ok",
     "timestamp": 1763433543473,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "rzcb6XXmv7HX",
    "outputId": "8126ad46-b8a2-4b76-9bab-1d6f1fd905ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False, 'architecture': 'BertModel'})\n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embedder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ClRN116Hg7pJ"
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "from transformers import default_data_collator\n",
    "\n",
    "def custom_collator(features):\n",
    "    # each feature is a dict\n",
    "\n",
    "    # Extract raw text BEFORE anything touches tokenizer.pad()\n",
    "    orig_text = [f[\"orig_text\"] for f in features]\n",
    "\n",
    "    # Remove it from each feature\n",
    "    clean_features = []\n",
    "    for f in features:\n",
    "        f2 = f.copy()\n",
    "        f2.pop(\"orig_text\")         # <= remove here\n",
    "        clean_features.append(f2)\n",
    "\n",
    "    # Now let HF build tensors safely\n",
    "    batch = default_data_collator(clean_features)\n",
    "\n",
    "    # Add back raw text as Python list\n",
    "    batch[\"orig_text\"] = orig_text\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=os.path.join(folder_path, \"model_checkpoints/masked-3loss-tweets-mpqa-flan_t5-2loss-11_16\"),\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    # eval_steps=1000,\n",
    "    learning_rate=1e-5, #1e-5 to 5e-5\n",
    "    num_train_epochs=1, #3\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    logging_dir=os.path.join(folder_path, \"logs/masked-3loss-tweets-mpqa-flan_t5-2loss-11_16\"),\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = NeutralRewriteTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    embedder=embedder,\n",
    "    subj_lexicon=subjective_words,\n",
    "    lambda_sem=1.5,\n",
    "    lambda_neu=0.3,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    data_collator=custom_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2325276,
     "status": "ok",
     "timestamp": 1763436960748,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "S1zSf3hkhV-x",
    "outputId": "08768970-3867-45f0-d0da-263159f62595"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ DEBUG ================\n",
      "Step 0\n",
      "Total Loss:        0.9388\n",
      "CE Loss:           0.5827\n",
      "Semantic Loss:     0.2374\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Pope's plane makes a couple loops over NC on way to see President - would be if he flew over Pope AFB!\n",
      "Generated: Pope's plane makes a couple loops over NC on way to see President - would be if he w over'\n",
      "=======================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10173' max='10173' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10173/10173 38:44, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.673500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.613400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.567600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.563300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.537500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.543800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.532500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.520200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.514000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.502900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.516900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.510300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.506900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.508500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.517800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.502600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.509000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.511000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.509000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.507800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43m 5000 \u001b[0m\n",
      "Semantic Loss:     0.1700\n",
      "Neutrality Loss:   0.0114\n",
      "Source: live stream was amazing cant wait till the 31st to get hero on beatport but besides that work tmw peace\n",
      "Generated: live stream was  cant wait till the 31st to get <NEU> on beatport but <NEU> that work tm \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 1100\n",
      "Total Loss:        0.4855\n",
      "CE Loss:           0.3281\n",
      "Semantic Loss:     0.1049\n",
      "Neutrality Loss:   0.0000\n",
      "Source: LMAOOO that's all I've been watching on tumblr\n",
      "Generated: LMAOOO that's all I've been watching on tumblr  that     that    \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 1120\n",
      "Total Loss:        0.4096\n",
      "CE Loss:           0.1241\n",
      "Semantic Loss:     0.1904\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Did not qualify for Europa Cup thru Fifa fair play? Now they have 3 red cards in 4 Europa League games badboybilic\n",
      "Generated: Did not qualify for Europa Cup thru Fifa fair play? Now they have 3 red cards in 4 Europa League games badboybilic\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 1140\n",
      "Total Loss:        0.6976\n",
      "CE Loss:           0.3069\n",
      "Semantic Loss:     0.2605\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Made me laugh. Perigo is my favourite word when in Portugal. Gets me out of trouble\n",
      "Generated: Made me <NEU> Perigo is my favourite word when in Portugal. Gets me out of trouble. Made.. Made Made Made. Made\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 1160\n",
      "Total Loss:        1.4315\n",
      "CE Loss:           1.0231\n",
      "Semantic Loss:     0.2723\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Yes, by all meanscomment on my posts with things that are irrelevant to the post. Thats the s*** I like.\n",
      "Generated: <NEU> by all means...comment on my posts with things that are <NEU> to the post. Thats the <NEU><NEU> by\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 1180\n",
      "Total Loss:        0.7008\n",
      "CE Loss:           0.2697\n",
      "Semantic Loss:     0.2861\n",
      "Neutrality Loss:   0.0066\n",
      "Source: 'Sicario' Director Denis Villeneuve Says He Hates Senseless Violence In Film celeb news\n",
      "Generated: 'Sicario' Director Denis Villeneuve Says He Hates <NEU> Violence In Film celeb newskes     \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 1200\n",
      "Total Loss:        0.4291\n",
      "CE Loss:           0.1289\n",
      "Semantic Loss:     0.2001\n",
      "Neutrality Loss:   0.0000\n",
      "Source: never mind, the website is sold out of them\n",
      "Generated: never mind, the website is sold out of thems.. ever never never never never never never but ever never never never never never never never never\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 1220\n",
      "Total Loss:        0.5574\n",
      "CE Loss:           0.1747\n",
      "Semantic Loss:     0.2552\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Ok I know I'm 10 years late but prisonbreak is so good late\n",
      "Generated: Ok I <NEU> I'm 10 years late but prisonbreak is so good late.. Ok  Ok Ok Ok Ok Ok Ok\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 1240\n",
      "Total Loss:        0.8330\n",
      "CE Loss:           0.4018\n",
      "Semantic Loss:     0.2860\n",
      "Neutrality Loss:   0.0069\n",
      "Source: \"People love 'The Wall,'\" - (likely talking about people's fondness for the classic Pink Floyd work). lssc\n",
      "Generated: \"People <NEU> 'The Wall,'\" - (likely talking about people's <NEU> for the classic Pink Floyd...\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 1260\n",
      "Total Loss:        1.0365\n",
      "CE Loss:           0.6119\n",
      "Semantic Loss:     0.2831\n",
      "Neutrality Loss:   0.0000\n",
      "Source: It looks to me like may be dead. :O\n",
      "Generated: It looks to me <NEU> may be dead. :O   It   It It It It     Ach  It\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 1280\n",
      "Total Loss:        0.8497\n",
      "CE Loss:           0.5751\n",
      "Semantic Loss:     0.1831\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Good Bye Lenin! RT juxtaprogressive same people who stand on pro-\"choice\" want a socialist leader bluescare bernie\n",
      "Generated: Good Bye Lenin! RT juxtaprogressive same people who stand on pro-\"choice\"...t\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 1300\n",
      "Total Loss:        0.2591\n",
      "CE Loss:           0.0433\n",
      "Semantic Loss:     0.1438\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Good thing the Giants cut James Jones so he could go catch TDs for the Packers\n",
      "Generated: Good thing the Giants cut James Jones so he could go catch TDs for the Packers  ... respectively. Goodhold Good.\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 1320\n",
      "Total Loss:        0.5320\n",
      "CE Loss:           0.2468\n",
      "Semantic Loss:     0.1902\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Work was cool . Got to trim a crazy ass pit bull's nails and play with a dog named Doogie that has sever diarrhea. notallpitsarebad\n",
      "Generated: Work was cool . Got to trim a <NEU> ass pit bull's nails and play with a dog named Doo is \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 1340\n",
      "Total Loss:        0.5624\n",
      "CE Loss:           0.3448\n",
      "Semantic Loss:     0.1451\n",
      "Neutrality Loss:   0.0000\n",
      "Source: The ability to observe without evaluating is the highest form of intelligence. ~ Jiddu Krishnamurti\n",
      "Generated: The ability to observe without evaluating is the highest form of intelligence.  Jiddu Krishnamurti \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 1360\n",
      "Total Loss:        0.8100\n",
      "CE Loss:           0.4622\n",
      "Semantic Loss:     0.2319\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Huh. sends a lot of \"we really care!\" emails, yet doesn't reply to customer service inquiries about the lack of shipment...\n",
      "Generated: Huh. sends a lot of \"we <NEU> care!\" emails, yet doesn't reply to customer service inquiries the the <NEU> the\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 1380\n",
      "Total Loss:        0.8646\n",
      "CE Loss:           0.5118\n",
      "Semantic Loss:     0.2352\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Disney Classics.... via /r/funny lol haha humor lmao lmfao hilarious laugh #\n",
      "Generated: Disney Classics.... via /r/funny lol haha <NEU> lmao lmfal\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 1400\n",
      "Total Loss:        0.6736\n",
      "CE Loss:           0.4468\n",
      "Semantic Loss:     0.1512\n",
      "Neutrality Loss:   0.0000\n",
      "Source: None of these people get very telling of cognitive deficiencies  I feel publicopinion\n",
      "Generated: None of these people get very telling of cognitive deficiencies  I <NEU> publicopinion  None None None None None\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 1420\n",
      "Total Loss:        0.8072\n",
      "CE Loss:           0.4188\n",
      "Semantic Loss:     0.2589\n",
      "Neutrality Loss:   0.0000\n",
      "Source: I detecting \n",
      "Generated: I detecting        Iward I I I I  I I     I I I I I I\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 1440\n",
      "Total Loss:        0.4791\n",
      "CE Loss:           0.2913\n",
      "Semantic Loss:     0.1252\n",
      "Neutrality Loss:   0.0000\n",
      "Source: life has a funny way of helping you out when u think everything's gone wrong, and everything blows up in your face\n",
      "Generated: life has a <NEU> way of helping you out when u <NEU> everything's gone wrong, and everything blows up in the faces\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 1460\n",
      "Total Loss:        0.3007\n",
      "CE Loss:           0.0467\n",
      "Semantic Loss:     0.1694\n",
      "Neutrality Loss:   0.0000\n",
      "Source: schwarb played lots of AA and AAA that's why\n",
      "Generated: schwarb played lots of AA and AAA that's why    this           \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 1480\n",
      "Total Loss:        0.5797\n",
      "CE Loss:           0.2238\n",
      "Semantic Loss:     0.2373\n",
      "Neutrality Loss:   0.0000\n",
      "Source: There is a government in Pakistan, there is a government. Surely there is a government. pakistan floods\n",
      "Generated: There is a government in Pakistan, there is a government. <NEU> there is a government. pakistan floods \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 1500\n",
      "Total Loss:        0.6392\n",
      "CE Loss:           0.4430\n",
      "Semantic Loss:     0.1309\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Jhonson will get to his 400th Test wicket on James Anderson's birthday!\n",
      "Generated: Jhonson <NEU> get to his 400th Test wicket on James Anderson's birthday!!! J J J\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 1520\n",
      "Total Loss:        0.5734\n",
      "CE Loss:           0.1994\n",
      "Semantic Loss:     0.2494\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Tonko requests ceremony for American Pharoah: Should American Pharoah be honored at the White albany ny news\n",
      "Generated: Tonko requests ceremony for American Pharoah: Should American Pharoah be honored at the White... albanyb\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 1540\n",
      "Total Loss:        0.4413\n",
      "CE Loss:           0.1952\n",
      "Semantic Loss:     0.1628\n",
      "Neutrality Loss:   0.0063\n",
      "Source: teamsleepy came up with a road trip hash tag. bosstldr tldr twitter\n",
      "Generated: teamsleepy came up with a road trip hash tag. bosstldr tldr twitterteamhead \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 1560\n",
      "Total Loss:        0.4074\n",
      "CE Loss:           0.0406\n",
      "Semantic Loss:     0.2419\n",
      "Neutrality Loss:   0.0132\n",
      "Source: Offering a kid a scholarship so he can better himself is quite shameful.\n",
      "Generated: Offering a <NEU> a scholarship so he can better himself is <NEU> <NEU>       offering  Offering Offering\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 1580\n",
      "Total Loss:        0.7100\n",
      "CE Loss:           0.2610\n",
      "Semantic Loss:     0.2994\n",
      "Neutrality Loss:   0.0000\n",
      "Source: [looks to the right] Oh no how would I ever cope with such a very serious threat...\n",
      "Generated: [looks to the <NEU> <NEU> no how would I ever cope with such a very <NEU> threat...   today [ [\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 1600\n",
      "Total Loss:        0.7796\n",
      "CE Loss:           0.2846\n",
      "Semantic Loss:     0.3300\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Love a long gray ponytail &amp; breeches - on a dude!! horseshow horses hunters\n",
      "Generated: <NEU> a <NEU> gray ponytail &amp; breeches - on a dude!! horseshow horsest spots\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 1620\n",
      "Total Loss:        0.5858\n",
      "CE Loss:           0.3954\n",
      "Semantic Loss:     0.1269\n",
      "Neutrality Loss:   0.0000\n",
      "Source: I have the superpower of instantly killing girls I message. It's so peculiar!\n",
      "Generated: I have the superpower of instantly killing girls I message. It's so <NEU>! I I I I I I I I I\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 1640\n",
      "Total Loss:        0.7423\n",
      "CE Loss:           0.5651\n",
      "Semantic Loss:     0.1182\n",
      "Neutrality Loss:   0.0000\n",
      "Source: And because of the heinous nature of her alleged crime, will this 16 year old be tried as an adult??\n",
      "Generated: And because of the <NEU> nature of her alleged crime, <NEU> this 16 year old be tried as an adult?? And  And and and\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 1660\n",
      "Total Loss:        0.5577\n",
      "CE Loss:           0.3069\n",
      "Semantic Loss:     0.1672\n",
      "Neutrality Loss:   0.0000\n",
      "Source: It's not \"Tharak Pan\" but for ur kind information . Your mind and you, yourself need to grow up yet  \n",
      "Generated: It's not \"Tharak Pan\" but for ur <NEU> information . Your mind and you, yourself .\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 1680\n",
      "Total Loss:        0.6759\n",
      "CE Loss:           0.3939\n",
      "Semantic Loss:     0.1879\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Scaling Mt Everest looks so exhilarating and like a lot of fun\n",
      "Generated: Scaling Mt Everest looks so <NEU> and <NEU> a lot of <NEU>     Scal  Scal \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 1700\n",
      "Total Loss:        0.8635\n",
      "CE Loss:           0.4454\n",
      "Semantic Loss:     0.2787\n",
      "Neutrality Loss:   0.0000\n",
      "Source: . Thank you! The funny part is that my 4 y/o is wearing those same pants his brother wore..TODAY! ha! fallin3sofab\n",
      "Generated: . <NEU> you! The <NEU> part is that my 4 y/o is wearing those same pants his brother wore.TAL\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 1720\n",
      "Total Loss:        0.7168\n",
      "CE Loss:           0.4439\n",
      "Semantic Loss:     0.1819\n",
      "Neutrality Loss:   0.0000\n",
      "Source: it is hilarious to see a religious kook say this.\n",
      "Generated: it is <NEU> to see a religious <NEU> say this.  it it it it it it it it it it it it it it it\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 1740\n",
      "Total Loss:        0.5314\n",
      "CE Loss:           0.2368\n",
      "Semantic Loss:     0.1953\n",
      "Neutrality Loss:   0.0057\n",
      "Source: is 10 years younger than me, I'm an engineer and his contract is worth more than me selling all my organs!! mad_world\n",
      "Generated: is 10 years younger than me, I'm an engineer and his contract is <NEU> more than me selling all my organs!! mad\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 1760\n",
      "Total Loss:        0.8005\n",
      "CE Loss:           0.4818\n",
      "Semantic Loss:     0.2125\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Multi-millionaire connected to F1 complains about high prices.\n",
      "Generated: Multi-millionaire connected to F1 complains about high prices. Multi Multi Multi Multi Multi Multi Multi Multi Multi Multi Multi Multi Multi\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 1780\n",
      "Total Loss:        0.4462\n",
      "CE Loss:           0.2170\n",
      "Semantic Loss:     0.1528\n",
      "Neutrality Loss:   0.0000\n",
      "Source: you would need some Viagra if your dating a stoprush hate skank! Haha They sponsor Rush! tcot pjnet\n",
      "Generated: you would <NEU> some Viagra if your dating a stoprush <NEU> skank! Haha They sponsor Rush! :!\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 1800\n",
      "Total Loss:        0.4538\n",
      "CE Loss:           0.1836\n",
      "Semantic Loss:     0.1802\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Yes, fnb has invested in more bs marketing; their social media campaigns now include\n",
      "Generated: <NEU> fnb has invested in more bs marketing; their social media campaigns now includeryssyssionsessor\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 1820\n",
      "Total Loss:        0.5360\n",
      "CE Loss:           0.2778\n",
      "Semantic Loss:     0.1712\n",
      "Neutrality Loss:   0.0046\n",
      "Source: ROFLMAO yeah, because ADDING is not an INCREASE\n",
      "Generated: ROFLMAO <NEU> because ADDING is not an INCREASE RORO..\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 1840\n",
      "Total Loss:        0.5410\n",
      "CE Loss:           0.2643\n",
      "Semantic Loss:     0.1845\n",
      "Neutrality Loss:   0.0000\n",
      "Source: ...just throw lots of money at it, money fixes everything.\n",
      "Generated: ...just throw lots of money at it, money fixes everything.s.               \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 1860\n",
      "Total Loss:        0.4674\n",
      "CE Loss:           0.2602\n",
      "Semantic Loss:     0.1381\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Lol looks like PTI opened up a laundry service... RAW daagh Bhi nikal jatay hain Kia? :p leavemqmjoinpti\n",
      "Generated: Lol looks <NEU> PTI opened up a laundry service... RAW daagh Bhi nikal jatay haiy\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 1880\n",
      "Total Loss:        1.0112\n",
      "CE Loss:           0.7173\n",
      "Semantic Loss:     0.1959\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Great working w/wonderfully funny &amp; talented Too bad about his looks, huh? ;)\n",
      "Generated: <NEU> working w/wonderfully <NEU> &amp; <NEU> Too  about his looks, huh?<NEU> working\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 1900\n",
      "Total Loss:        0.8724\n",
      "CE Loss:           0.3366\n",
      "Semantic Loss:     0.3572\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Straight dude excusing gross behavior from another bro. I'm so, so, so shocked. WOW WOW WOW\n",
      "Generated: Straight dude excusing gross behavior from another bro. I'm so, so, so,. <NEU> <NEU> <NEU>   either\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 1920\n",
      "Total Loss:        0.4251\n",
      "CE Loss:           0.0932\n",
      "Semantic Loss:     0.2212\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Maybe they fucked but not together and even if they did, it produced no offspring humor\n",
      "Generated: <NEU> they fucked but not together and even if they did, it produced no offspring <NEU>.?! .sted thoughsted?!\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 1940\n",
      "Total Loss:        0.2233\n",
      "CE Loss:           0.0406\n",
      "Semantic Loss:     0.1218\n",
      "Neutrality Loss:   0.0000\n",
      "Source: This guy is parked out the front of the southport Court House on the goldcoast.\n",
      "Generated: This guy is parked out the front of the southport <NEU> House on the goldcoast...... This.. this this\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 1960\n",
      "Total Loss:        0.5709\n",
      "CE Loss:           0.2536\n",
      "Semantic Loss:     0.2115\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Wow today has been wonderful\n",
      "Generated: <NEU> today has been <NEU>  ss <NEU>th<NEU><NEU><NEU><NEU><NEU><NEU><NEU><NEU><NEU><NEU><NEU><NEU><NEU><NEU><NEU><NEU>\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 1980\n",
      "Total Loss:        0.7472\n",
      "CE Loss:           0.4645\n",
      "Semantic Loss:     0.1885\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Robin Ventura in such a hurry to go check on his franchise pitcher after he takes a line drive off his leg.\n",
      "Generated: Robin Ventura in such a hurry to go check on his franchise pitcher after he takes a line drive off his leg...\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 2000\n",
      "Total Loss:        0.9416\n",
      "CE Loss:           0.6648\n",
      "Semantic Loss:     0.1845\n",
      "Neutrality Loss:   0.0000\n",
      "Source:  hippie peace guitar pick earrings handmade\n",
      "Generated:  hippie peace guitar pick earrings handmade                     \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 2020\n",
      "Total Loss:        0.4167\n",
      "CE Loss:           0.0200\n",
      "Semantic Loss:     0.2645\n",
      "Neutrality Loss:   0.0000\n",
      "Source: And that is causing the site to have too many connections.\n",
      "Generated: And that is causing the site to have too many connections. and and and And and And And and And And And And And And\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 2040\n",
      "Total Loss:        0.8006\n",
      "CE Loss:           0.5049\n",
      "Semantic Loss:     0.1972\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Every group is a squad and every squad seems to be on point squad onpoint\n",
      "Generated: Every group is a squad and every squad seems to be on point squad onpoints!! Every Every! Every Every every Every Every Every\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 2060\n",
      "Total Loss:        0.5330\n",
      "CE Loss:           0.2361\n",
      "Semantic Loss:     0.1980\n",
      "Neutrality Loss:   0.0000\n",
      "Source: is when someone writes \" YOUR and idiot \" letsgetwordy\n",
      "Generated: is when someone writes \" YOUR and <NEU> \" letsgetwordy   isores is isis isisis isitsisis is is\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 2080\n",
      "Total Loss:        0.7036\n",
      "CE Loss:           0.3944\n",
      "Semantic Loss:     0.2061\n",
      "Neutrality Loss:   0.0000\n",
      "Source: so we suffer for a symbolic sin? Makes sense...\n",
      "Generated: so we suffer for a symbolic <NEU> Makes sense...  so so so so so so so so so so so so so so so\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 2100\n",
      "Total Loss:        0.6812\n",
      "CE Loss:           0.3121\n",
      "Semantic Loss:     0.2461\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Quality game that from Rovers &amp; Wanderers\n",
      "Generated: Quality game that from Rovers &amp; Wanderers    Quality Qualityau  Quality ... Quality Quality  Quality Quality Quality\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 2120\n",
      "Total Loss:        0.4816\n",
      "CE Loss:           0.1488\n",
      "Semantic Loss:     0.2219\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Hahah late post my gympartner she work out at a differentgym  silly girl\n",
      "Generated: Hahah late post my gympartner she work out at a differentgym  <NEU> girl   Ha  ::\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 2140\n",
      "Total Loss:        0.3686\n",
      "CE Loss:           0.1244\n",
      "Semantic Loss:     0.1482\n",
      "Neutrality Loss:   0.0729\n",
      "Source: Before uttering such remarks, be sure, it doesn't hit you back tactless\n",
      "Generated: Before uttering such remarks, be <NEU> it doesn't hit you back tactless Before \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 2160\n",
      "Total Loss:        0.7248\n",
      "CE Loss:           0.4682\n",
      "Semantic Loss:     0.1711\n",
      "Neutrality Loss:   0.0000\n",
      "Source: I'm on a bus idling in traffic because of a pipeline protest.\n",
      "Generated: I'm on a bus idling in traffic because of a pipeline <NEU>  I I I I I I I I I\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 2180\n",
      "Total Loss:        0.8191\n",
      "CE Loss:           0.3442\n",
      "Semantic Loss:     0.3166\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Use it. Love it.\n",
      "Generated: Use it. <NEU> it... .. Use Use Use Use Use   Use Use    Use Use Use Use Use Use\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 2200\n",
      "Total Loss:        0.4384\n",
      "CE Loss:           0.1407\n",
      "Semantic Loss:     0.1985\n",
      "Neutrality Loss:   0.0000\n",
      "Source: England women must have all their partners with them! poorbatting womensashes\n",
      "Generated: England women <NEU> have all their partners with them! poorbatting womensashes!! England! England England England Englandeng\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 2220\n",
      "Total Loss:        0.6894\n",
      "CE Loss:           0.4451\n",
      "Semantic Loss:     0.1628\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Hilarious to watch leftie liberals getting all wound up about Donald Trump offending John McCain whom they hated only recently.\n",
      "Generated: <NEU> to watch leftie liberals getting all wound up about Donald <NEU> <NEU> John McCain whom they hated only recently...but\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 2240\n",
      "Total Loss:        0.3764\n",
      "CE Loss:           0.1320\n",
      "Semantic Loss:     0.1629\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Mascara blusher eyeliner fake lashes eyeshadow. Meanwhile I'm 21 and only draw my brows and put on double eyelid sticker.\n",
      "Generated: Mascara blusher eyeliner <NEU> lashes eyeshadow. Meanwhile I'm 21 and only draw my brows and  on my eye\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 2260\n",
      "Total Loss:        0.4440\n",
      "CE Loss:           0.2006\n",
      "Semantic Loss:     0.1607\n",
      "Neutrality Loss:   0.0078\n",
      "Source: Good morning! late traffic butproductive\n",
      "Generated: Good morning! late traffic butproductiveday.goodgggood. Goodg Goodgg Goodg Good Good Good Good Good Good Good Good\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 2280\n",
      "Total Loss:        0.6796\n",
      "CE Loss:           0.3853\n",
      "Semantic Loss:     0.1962\n",
      "Neutrality Loss:   0.0000\n",
      "Source: A5 Give your students lots of worksheets and have them grade them themselves. Makes for easy work! ;-) oklaed\n",
      "Generated: A5 Give your students lots of worksheets and have them grade them themselves. Makes for easy work! ;-) ohom\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 2300\n",
      "Total Loss:        0.5664\n",
      "CE Loss:           0.2279\n",
      "Semantic Loss:     0.2256\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Wine can keep me up all night! passion\n",
      "Generated: Wine can keep me up all night! <NEU>             |    Wine 2001 \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 2320\n",
      "Total Loss:        0.4317\n",
      "CE Loss:           0.2418\n",
      "Semantic Loss:     0.1266\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Enforce mandatory drug tests for all Members of Parliament. - Petitions cannabis drugs\n",
      "Generated: Enforce mandatory drug tests for all Members of Parliament. - Petitions cannabis drugsmputation En En En En En En En En En  En En\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 2340\n",
      "Total Loss:        0.4834\n",
      "CE Loss:           0.1583\n",
      "Semantic Loss:     0.2168\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Schools Wi-Fi Making Son Sick, Parents Say In Lawsuit business legal politics\n",
      "Generated: Schools Wi-Fi Making Son Sick, Parents Say In Lawsuit business legal politics  School  School  School School... School  School\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 2360\n",
      "Total Loss:        0.4233\n",
      "CE Loss:           0.1080\n",
      "Semantic Loss:     0.2102\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Does boy have a permit to avoid overzealous shutdown? side-eye\n",
      "Generated: Does boy have a permit to avoid <NEU> shutdown? side-eye  Does Doeshood Does  Does Does Does Does Does Does\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 2380\n",
      "Total Loss:        0.5628\n",
      "CE Loss:           0.3695\n",
      "Semantic Loss:     0.1288\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Pepsi removes aspartame from diet cola - but only in the USA. Do lives matter less outside the USA\n",
      "Generated: Pepsi removes aspartame from diet cola - but only in the USA. Do lives matter less outside the US \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 2400\n",
      "Total Loss:        0.4217\n",
      "CE Loss:           0.2937\n",
      "Semantic Loss:     0.0853\n",
      "Neutrality Loss:   0.0000\n",
      "Source: And yet licensed techs and unlicensed assistants are always assumed, BY VETERINARIANS, to be the same.\n",
      "Generated: And yet licensed techs and unlicensed assistants are always assumed, BY VETERINARIANS, to be the same And\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 2420\n",
      "Total Loss:        0.2180\n",
      "CE Loss:           0.0354\n",
      "Semantic Loss:     0.1217\n",
      "Neutrality Loss:   0.0000\n",
      "Source: if your anything like Thomas you going to have a blast!\n",
      "Generated: if your anything <NEU> Thomas you going to have a blast!.              \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 2440\n",
      "Total Loss:        0.4469\n",
      "CE Loss:           0.1982\n",
      "Semantic Loss:     0.1658\n",
      "Neutrality Loss:   0.0000\n",
      "Source: have very few ad breaks during rwc2015 anyway... sickofads justshowtherugby\n",
      "Generated: have very few ad breaks during rwc2015 anyany sickofads justshowtherugby! hasve have have\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 2460\n",
      "Total Loss:        0.3595\n",
      "CE Loss:           0.1159\n",
      "Semantic Loss:     0.1625\n",
      "Neutrality Loss:   0.0000\n",
      "Source: husband of late TV reporter advocates aneurysm awareness\n",
      "Generated: husband of late TV reporter advocates aneurysm <NEU>s mother  husband husband husband husband husband  husband husband husband husband husband husband husband\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 2480\n",
      "Total Loss:        0.4266\n",
      "CE Loss:           0.1689\n",
      "Semantic Loss:     0.1718\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Don't worry australia understands you learnit\n",
      "Generated: Don't <NEU> australia understands you learnit'n' Don Don Don Don Don Don Don Don Don Don Don Don Don Don Don Don\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 2500\n",
      "Total Loss:        0.4768\n",
      "CE Loss:           0.3111\n",
      "Semantic Loss:     0.1105\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Do you think whoever finds this dentist gets to put his head on their wall?-a cecilthelion iwishitatehim\n",
      "Generated: Do you <NEU> whoever finds this dentist gets to put his head on their wall?-a cecilthelion iwish\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 2520\n",
      "Total Loss:        0.9739\n",
      "CE Loss:           0.6275\n",
      "Semantic Loss:     0.2202\n",
      "Neutrality Loss:   0.0536\n",
      "Source: Omg, who knew!!! MM is a CB.. not midfield  nffc everyonebutdougie\n",
      "Generated: Omg, who k <NEU> is a CB.. not midfield  nffc\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 2540\n",
      "Total Loss:        0.4871\n",
      "CE Loss:           0.2181\n",
      "Semantic Loss:     0.1794\n",
      "Neutrality Loss:   0.0000\n",
      "Source: How do you tell people what NOT to put on the internet...? Put it on the internet.\n",
      "Generated: How do you tell people what NOT to put on the internet...? Put it on the internet.. how. How How How How How How\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 2560\n",
      "Total Loss:        0.4424\n",
      "CE Loss:           0.1270\n",
      "Semantic Loss:     0.2088\n",
      "Neutrality Loss:   0.0074\n",
      "Source: Rand gives Jeb! an unexpected and endearing moment of candor on pot, of all things. ididinhale gopdebate\n",
      "Generated: Rand gives Jeb! an unexpected and <NEU> moment of <NEU> on pot, of all things. ididinhale gobate\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 2580\n",
      "Total Loss:        0.6242\n",
      "CE Loss:           0.2885\n",
      "Semantic Loss:     0.2237\n",
      "Neutrality Loss:   0.0000\n",
      "Source: reuters: Republican presidential candidate Perry suspends 2016 campaign politics\n",
      "Generated: reuters: Republican presidential candidate Perry suspends 2016 campaign politics                \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 2600\n",
      "Total Loss:        0.9488\n",
      "CE Loss:           0.4231\n",
      "Semantic Loss:     0.3505\n",
      "Neutrality Loss:   0.0000\n",
      "Source: way to go Mayor!\n",
      "Generated: way to go Mayor! way way way way way way way way way way way way way way way way way way way way way way way way\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 2620\n",
      "Total Loss:        0.5154\n",
      "CE Loss:           0.1612\n",
      "Semantic Loss:     0.2361\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Merkel,atoning for past,is threatening EU &amp; throwing Germany's weight about. Jews are 1st victims of Muslims!\n",
      "Generated: Merkel,atoning for past,is threatening EU &amp; throwing Germany's weight about. Jews are 1st \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 2640\n",
      "Total Loss:        0.3410\n",
      "CE Loss:           0.1152\n",
      "Semantic Loss:     0.1505\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Just got a spam email advertising \"Painless Profreading\" courses.\n",
      "Generated: Just got a spam email advertising \"Painless Profreading\" courses.s..... Just Just Just Just\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 2660\n",
      "Total Loss:        0.5767\n",
      "CE Loss:           0.3041\n",
      "Semantic Loss:     0.1817\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Taunton man faces murder charge for 2013 killing tauntonma news\n",
      "Generated: Taunton man faces murder charge for 2013 killing tauntonma newshhwt taut Tau taut tau taut\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 2680\n",
      "Total Loss:        0.5026\n",
      "CE Loss:           0.2658\n",
      "Semantic Loss:     0.1579\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Good thing all those good guys with guns are preventing so many! RT There have been 204 mass shootings in 2015 so far.\n",
      "Generated: Good thing all those good guys with guns are preventing so many! RT There have been 204 mass shootings in 2015 so far \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 2700\n",
      "Total Loss:        0.5539\n",
      "CE Loss:           0.3033\n",
      "Semantic Loss:     0.1671\n",
      "Neutrality Loss:   0.0000\n",
      "Source: This is from the times when sex was safe and racing was dangerous.\n",
      "Generated: This is from the times when sex was safe and racing was dangerous.. this. This. This This This This This This\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 2720\n",
      "Total Loss:        0.6897\n",
      "CE Loss:           0.3384\n",
      "Semantic Loss:     0.2328\n",
      "Neutrality Loss:   0.0069\n",
      "Source: Ben Carson talking about a brain gopdebate\n",
      "Generated: Ben Carson talking about a brain gopdebate Ben Ben Ben Ben Ben Ben Ben Ben Ben Ben Ben Ben Ben\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 2740\n",
      "Total Loss:        0.6032\n",
      "CE Loss:           0.2758\n",
      "Semantic Loss:     0.2183\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Kamaal ka logic hai. RW terror condemnable in all forms. Btw is Aurangzeb Rd being changed to Hindu name?\n",
      "Generated: Kamaal ka logic hai. RW <NEU> <NEU> in all forms. Btw is Aurangzeb R \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 2760\n",
      "Total Loss:        0.4469\n",
      "CE Loss:           0.2476\n",
      "Semantic Loss:     0.1312\n",
      "Neutrality Loss:   0.0083\n",
      "Source: Things I learn while I stuff my face.....smh  true love and cookies @ The Chocolate Bar\n",
      "Generated: Things I learn while I stuff my face.....smh  <NEU> <NEU> and cookies @ The Chocolate Bar  Things  Things Things\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 2780\n",
      "Total Loss:        0.2880\n",
      "CE Loss:           0.0123\n",
      "Semantic Loss:     0.1838\n",
      "Neutrality Loss:   0.0000\n",
      "Source: I love working long weekends\n",
      "Generated: I <NEU> working <NEU> weekends       I  I I  I I I  I  I I I I I \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 2800\n",
      "Total Loss:        0.4100\n",
      "CE Loss:           0.2200\n",
      "Semantic Loss:     0.1267\n",
      "Neutrality Loss:   0.0000\n",
      "Source: The Ford Focus in front of me in the drive thru has been texting so much she didn't realize there's no one in front of her. move\n",
      "Generated: The Ford Focus in front of me in the drive thru has been texting so much she didn't realize there's no one in the of me\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 2820\n",
      "Total Loss:        0.4414\n",
      "CE Loss:           0.2290\n",
      "Semantic Loss:     0.1416\n",
      "Neutrality Loss:   0.0000\n",
      "Source: When you're so bloody sarcastic that you need to mention \"no not being sarcastic\".\n",
      "Generated: When you're so bloody <NEU> that you <NEU> to mention \"no not being <NEU> When when  When when When \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 2840\n",
      "Total Loss:        0.6640\n",
      "CE Loss:           0.3308\n",
      "Semantic Loss:     0.2221\n",
      "Neutrality Loss:   0.0000\n",
      "Source: A new post every Saturday! Check out the latest now! blog humour humor\n",
      "Generated: A new post every Saturday! Check out the latest now! blog <NEU> <NEU>          A A A\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 2860\n",
      "Total Loss:        0.5226\n",
      "CE Loss:           0.1721\n",
      "Semantic Loss:     0.2319\n",
      "Neutrality Loss:   0.0089\n",
      "Source: Yet another office hour flooded with students whoactuallycomes notmanyifany\n",
      "Generated: Yet another office hour flooded with students whoactuallycomes notmanyifanyt Yet yet Yet Yet Yet Yet Yet Yet\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 2880\n",
      "Total Loss:        0.7288\n",
      "CE Loss:           0.2238\n",
      "Semantic Loss:     0.3367\n",
      "Neutrality Loss:   0.0000\n",
      "Source: finally got a follow...sheesh late nolove\n",
      "Generated: <NEU> got a follow...sheesh late nolovey!s...):s...ssessor though<NEU>sten\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 2900\n",
      "Total Loss:        0.3311\n",
      "CE Loss:           0.0390\n",
      "Semantic Loss:     0.1947\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Intelligence official: Not our job to warn OPM of cyber threat politics\n",
      "Generated: Intelligence official: Not our job to warn OPM of cyber threat politics Intel  Intel Intel Intel Intel Intel Intel Intel Intel Intel Intel Intel\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 2920\n",
      "Total Loss:        0.4414\n",
      "CE Loss:           0.1111\n",
      "Semantic Loss:     0.2202\n",
      "Neutrality Loss:   0.0000\n",
      "Source: This is fascinating... What one college discovered when it stopped accepting SAT/ACT scores college education\n",
      "Generated: This is <NEU> What one college discovered when it stopped accepting SAT/ACT scores college education! this. this this This grade This This\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 2940\n",
      "Total Loss:        0.4880\n",
      "CE Loss:           0.2274\n",
      "Semantic Loss:     0.1737\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Company profile LATE TALKER CONSULTING, LLC - late talker consulting, llc\n",
      "Generated: Company profile LATE TALKER CONSULTING, LLC - late talker consulting, llc  Company Company Company \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 2960\n",
      "Total Loss:        0.6603\n",
      "CE Loss:           0.2759\n",
      "Semantic Loss:     0.2563\n",
      "Neutrality Loss:   0.0000\n",
      "Source: good work Royal Mail, nice to see privatisation is working\n",
      "Generated: good work Royal Mail, <NEU> to see privatisation is workingth  good. thumb... or god good    goodgg good good among\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 2980\n",
      "Total Loss:        0.7504\n",
      "CE Loss:           0.4774\n",
      "Semantic Loss:     0.1820\n",
      "Neutrality Loss:   0.0000\n",
      "Source: YES!!!! Love it when you soooo perky maybeeeee\n",
      "Generated:  <NEU> it when you soooo perky maybeeeee Yy          \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 3000\n",
      "Total Loss:        0.5103\n",
      "CE Loss:           0.3385\n",
      "Semantic Loss:     0.1145\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Walk and wait in the sun until the \"authorities\" have the kindness to deal with them.\n",
      "Generated: Walk and wait in the sun until the \"authorities\" have the <NEU> to deal with them..  . Walk Walk Walk Walk Walk\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 3020\n",
      "Total Loss:        0.5542\n",
      "CE Loss:           0.3304\n",
      "Semantic Loss:     0.1491\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Really, apple ends their big presentation with a lousy band whose song includes the lyric \"Take that money / watch it burn\"?\n",
      "Generated: <NEU> apple ends their big presentation with a <NEU> band whose song includes the lyric \"Take that money / watch it/\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 3040\n",
      "Total Loss:        0.4130\n",
      "CE Loss:           0.1075\n",
      "Semantic Loss:     0.2026\n",
      "Neutrality Loss:   0.0054\n",
      "Source: Donald Trump's \"Merry Christmas\" Has Nothing to Do With Christmas politics VIA\n",
      "Generated: Donald Trump's \"<NEU> Christmas\" Has Nothing to Do With Christmas politics VIA   Donald Donald  Donald Donald Donald  Donald Donald\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 3060\n",
      "Total Loss:        0.5995\n",
      "CE Loss:           0.2408\n",
      "Semantic Loss:     0.2391\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Dr Ben Carson is in 2nd place in Iowa...damn those racist Republicans\n",
      "Generated: Dr Ben Carson is in 2nd place in Iowa...damn those <NEU> Republicans    Dr  Dr Dr Dr\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 3080\n",
      "Total Loss:        0.3236\n",
      "CE Loss:           0.0750\n",
      "Semantic Loss:     0.1657\n",
      "Neutrality Loss:   0.0000\n",
      "Source: E, di wow! \"A miracle: Manila not in list of cities with worlds worst traffic\" via\n",
      "Generated: E, di <NEU> \"A <NEU> Manila not in list of cities with worlds <NEU> traffic\" via s    E\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 3100\n",
      "Total Loss:        0.3261\n",
      "CE Loss:           0.1347\n",
      "Semantic Loss:     0.1276\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Blah pendant, sarcastic, black and white, unique gifts, black pendant nec\n",
      "Generated: <NEU> pendant, <NEU> black and white, unique gifts, black pendant nec...t...t pendant! !NM...<NEU>...). please-11t\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 3120\n",
      "Total Loss:        0.5068\n",
      "CE Loss:           0.3259\n",
      "Semantic Loss:     0.1206\n",
      "Neutrality Loss:   0.0000\n",
      "Source: WTF man, i thought speedrunning was serious business ? Stop having fun !\n",
      "Generated: WTF man, i thought speedrunning was <NEU> business ? Stop having <NEU> !         W \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 3140\n",
      "Total Loss:        0.5613\n",
      "CE Loss:           0.3265\n",
      "Semantic Loss:     0.1565\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Turning Iowa huckabee One Conservative at a Time gop gopdebate ccot tcot teaparty imwithhuck lnyhbt\n",
      "Generated: Turning Iowa huckabee One Conservative at a Time gop gopdebate ccot tcot\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 3160\n",
      "Total Loss:        0.8037\n",
      "CE Loss:           0.4875\n",
      "Semantic Loss:     0.2108\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Great humour from bournemouth fans singing All we want is a decent referee (to the We all live in a yellow submarine, Beatles song)\n",
      "Generated: <NEU> <NEU> from bournemouth fans singing All we <NEU> is a decent referee (to the We all live in thea \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 3180\n",
      "Total Loss:        0.5519\n",
      "CE Loss:           0.2289\n",
      "Semantic Loss:     0.2153\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Better to remain silent and be thought a fool, than to speak and remove all doubt.\n",
      "Generated: Better to remain silent and be thought a <NEU> than to speak and remove all <NEU>  be   Better Bett Better   Better\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 3200\n",
      "Total Loss:        0.4634\n",
      "CE Loss:           0.0982\n",
      "Semantic Loss:     0.2434\n",
      "Neutrality Loss:   0.0000\n",
      "Source: But But Hockey doesn't belong in the desert.\n",
      "Generated: But But Hockey doesn't belong in the <NEU>  But  But But].  But But , But Notre But But Too The],\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 3220\n",
      "Total Loss:        0.3774\n",
      "CE Loss:           0.2391\n",
      "Semantic Loss:     0.0922\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Ya America doesn't have a gun problem...\n",
      "Generated: Ya America doesn't have a gun problem...s  Ya Ya  Ya Ya Ya Ya Ya Ya Ya Ya Ya Ya Ya Ya Ya\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 3240\n",
      "Total Loss:        0.5203\n",
      "CE Loss:           0.3243\n",
      "Semantic Loss:     0.1279\n",
      "Neutrality Loss:   0.0139\n",
      "Source: Hope. photo photography nature late\n",
      "Generated: <NEU> photo photography nature latet  (2011)ndig<NEU>mp tolerance 000mtrage<NEU>longedndig identifiable/11<NEU><NEU><NEU><NEU>mtrage<NEU><NEU><NEU><NEU>\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 3260\n",
      "Total Loss:        0.5933\n",
      "CE Loss:           0.2250\n",
      "Semantic Loss:     0.2455\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Strange not buying a homecoming dress for the first time in 8 years.\n",
      "Generated: <NEU> not buying a homecoming dress for the first time in 8 years...,<NEU>TS<NEU><NEU> ATM<NEU>ties:\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 3280\n",
      "Total Loss:        0.9247\n",
      "CE Loss:           0.5634\n",
      "Semantic Loss:     0.2408\n",
      "Neutrality Loss:   0.0000\n",
      "Source: The Domestic Bliss Of Jerry And Pawn - Gary Newsom : via iartg bookboost 99cents humor thailand\n",
      "Generated: The Domestic <NEU> Of Jerry And Pawn - Gary Newsom : via iartg bookboost 99cents .ta\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 3300\n",
      "Total Loss:        0.5695\n",
      "CE Loss:           0.3054\n",
      "Semantic Loss:     0.1760\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Gov taxes cigs hugely ostensibly for health reasons which drives ever more ppl to counterfeit goods which are dangerous fakebritain\n",
      "Generated: Gov taxes cigs hugely ostensibly for health reasons which drives ever more ppl to counterfeit goods which s\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 3320\n",
      "Total Loss:        0.2820\n",
      "CE Loss:           0.0137\n",
      "Semantic Loss:     0.1789\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Thanks Had to go to emergency pain due to slip and trip at springfield store\n",
      "Generated: Thanks Had to go to emergency pain due to slip and trip at springfield store Thanks Thanks Thanks Thanks thanks Thanks Thanks Thanks Mul Thanks Thanks Thanks\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 3340\n",
      "Total Loss:        0.3421\n",
      "CE Loss:           0.0082\n",
      "Semantic Loss:     0.2226\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Bush: Opening overseas markets to US oil, gas key to US boom news politics\n",
      "Generated: Bush: Opening overseas markets to US oil, gas key to US boom news politics  Bush Bush Bush Bush Bush Bush Bush Bush Bush\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 3360\n",
      "Total Loss:        0.6652\n",
      "CE Loss:           0.4129\n",
      "Semantic Loss:     0.1670\n",
      "Neutrality Loss:   0.0060\n",
      "Source: Missing California hiker found alive after 9 days lost in Sierra Nevada politics fox\n",
      "Generated: Missing California hiker found alive after 9 days lost in Sierra Nevada politics fox    Miss miss Miss Miss Miss Miss Miss\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 3380\n",
      "Total Loss:        0.4300\n",
      "CE Loss:           0.1446\n",
      "Semantic Loss:     0.1903\n",
      "Neutrality Loss:   0.0000\n",
      "Source: \"I'm helping you deliver your baby with another woman.\" underthedome cbs yourbrainontv\n",
      "Generated: \"I'm helping you deliver your <NEU> with another woman.\" underthedome cbs yourbrainontvr \"\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 3400\n",
      "Total Loss:        0.5152\n",
      "CE Loss:           0.3040\n",
      "Semantic Loss:     0.1227\n",
      "Neutrality Loss:   0.0903\n",
      "Source: Do you know how to heal depression without drugs\n",
      "Generated: Do you <NEU> how to heal depression without drugs  Do Do Do Do Do Do Do Do Do Do Do Do Do Do Do Do Do Do\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 3420\n",
      "Total Loss:        0.6612\n",
      "CE Loss:           0.5185\n",
      "Semantic Loss:     0.0951\n",
      "Neutrality Loss:   0.0000\n",
      "Source: ahhhh...the famous PPT with paragraphs of text being read word for word to you. Nothing better... txed\n",
      "Generated: ahhhh...the famous PPT with paragraphs of text being read word for word to you. Nothing better... txt\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 3440\n",
      "Total Loss:        0.8019\n",
      "CE Loss:           0.4945\n",
      "Semantic Loss:     0.2049\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Wtf my teacher is black lol\n",
      "Generated: Wtf my teacher is black lol  W W  W W W W W W W W W W W W W W\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 3460\n",
      "Total Loss:        0.3210\n",
      "CE Loss:           0.0428\n",
      "Semantic Loss:     0.1854\n",
      "Neutrality Loss:   0.0000\n",
      "Source: I can't wait to go to work tomorrow.\n",
      "Generated: I can't wait to go to work tomorrow. I I I I I I I I I I I I I I I I I I\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 3480\n",
      "Total Loss:        0.4263\n",
      "CE Loss:           0.1718\n",
      "Semantic Loss:     0.1697\n",
      "Neutrality Loss:   0.0000\n",
      "Source: ACLU Asking Judge to Stop Honolulu Homeless Sweeps news\n",
      "Generated: ACLU Asking Judge to Stop Honolulu Homeless Sweeps newss  AC AC AC  AC AC AC AC\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 3500\n",
      "Total Loss:        0.3394\n",
      "CE Loss:           0.1015\n",
      "Semantic Loss:     0.1586\n",
      "Neutrality Loss:   0.0000\n",
      "Source: ICYMI and will be parking cars at the Reagan Library cnn gopdebate next week..\n",
      "Generated: ICYMI and <NEU> be parking cars at the Reagan Library cnn gopdebate next week.. I   I\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 3520\n",
      "Total Loss:        0.4359\n",
      "CE Loss:           0.1992\n",
      "Semantic Loss:     0.1578\n",
      "Neutrality Loss:   0.0000\n",
      "Source: ibelieveblacklivesmatter bcus racist bigots are bitching about Americans invoking their 1st Amendment RIGHTS\n",
      "Generated: ibelieveblacklivesmatter bcus <NEU> bigots are bitching about Americans invoking their 1st \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 3540\n",
      "Total Loss:        0.5660\n",
      "CE Loss:           0.1785\n",
      "Semantic Loss:     0.2567\n",
      "Neutrality Loss:   0.0083\n",
      "Source: Yes, Obama and Hillary, the GUN caused the virginiashooting &amp; the GAY BLACK GUY had nothing to do with it\n",
      "Generated: <NEU> Obama and Hillary, the GUN caused the virginiashooting &amp; the GAY BLACK GUY  to do with\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 3560\n",
      "Total Loss:        0.6863\n",
      "CE Loss:           0.4210\n",
      "Semantic Loss:     0.1768\n",
      "Neutrality Loss:   0.0000\n",
      "Source: \"Righteous Christians conspire to deceive the Pope\"\n",
      "Generated: <NEU> Christians <NEU> to <NEU> the Pope\"s \"s \"s \"...\"olchtig...)...\"nite...). \"sse 9/11 sticky \"\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 3580\n",
      "Total Loss:        0.5353\n",
      "CE Loss:           0.3444\n",
      "Semantic Loss:     0.1199\n",
      "Neutrality Loss:   0.0368\n",
      "Source: DVD player making a weird noise and not even had it a year yet brilliant\n",
      "Generated: DVD player making a <NEU> noise and not even had it a year yet <NEU>    DVD  DVD DVD DVD DVD DVD\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 3600\n",
      "Total Loss:        0.4789\n",
      "CE Loss:           0.1351\n",
      "Semantic Loss:     0.2292\n",
      "Neutrality Loss:   0.0000\n",
      "Source: In india's Bihar state, training teachers to deliver quality education literacyday\n",
      "Generated: In india's Bihar state, training teachers to deliver quality education literacydayth In In In In In In\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 3620\n",
      "Total Loss:        0.3521\n",
      "CE Loss:           0.0574\n",
      "Semantic Loss:     0.1965\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Oh the ironing: It turned out a kid as playing hide and seek with the baby. You were 100% correct.\n",
      "Generated: <NEU> the ironing: It turned out a <NEU> as playing hide and seek with the <NEU> You were 100% correct..  the\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 3640\n",
      "Total Loss:        0.5657\n",
      "CE Loss:           0.1717\n",
      "Semantic Loss:     0.2626\n",
      "Neutrality Loss:   0.0000\n",
      "Source: It's like rain on your wedding day.\n",
      "Generated: It's <NEU> rain on your wedding day. It It It It It It It It It It It It It It It It It It\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 3660\n",
      "Total Loss:        0.6301\n",
      "CE Loss:           0.3794\n",
      "Semantic Loss:     0.1672\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Funny--\"it's the end of the world as we know it\" comes to mind every time I think of donaldtrump rem\n",
      "Generated: Funny--\"it's the end of the world as we <NEU> it\" comes to mind every time I <NEU> of don'de\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 3680\n",
      "Total Loss:        0.4197\n",
      "CE Loss:           0.2336\n",
      "Semantic Loss:     0.1241\n",
      "Neutrality Loss:   0.0000\n",
      "Source: In the wake of recent rainfall,Nawab kala bagh's haveli has drowned\n",
      "Generated: In the wake of recent rainfall,Nawab kala bagh's haveli has drowned In In\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 3700\n",
      "Total Loss:        0.2542\n",
      "CE Loss:           0.0099\n",
      "Semantic Loss:     0.1629\n",
      "Neutrality Loss:   0.0000\n",
      "Source: I need a trigger warning before I read articles about the need for trigger warnings. endlesscycle\n",
      "Generated: I <NEU> a trigger warning before I read articles about the <NEU> for trigger warnings. endlesscycley    I I I I\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 3720\n",
      "Total Loss:        0.3406\n",
      "CE Loss:           0.1131\n",
      "Semantic Loss:     0.1517\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Though I understand why they did it, I was thinking exactly the same thing as burningman2015\n",
      "Generated: <NEU> I <NEU> why they did it, I was <NEU> <NEU> the same thing as burningman2015.!11. 2006.. \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 3740\n",
      "Total Loss:        0.3645\n",
      "CE Loss:           0.1407\n",
      "Semantic Loss:     0.1492\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Arab 'Chinese' confederate flag jewelry sponsored by Spongebob and Patrick growinguparab murica\n",
      "Generated: Arab 'Chinese' confederate flag jewelry sponsored by Spongebob and Patrick growinguparab murica\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 3760\n",
      "Total Loss:        0.2536\n",
      "CE Loss:           0.0615\n",
      "Semantic Loss:     0.1267\n",
      "Neutrality Loss:   0.0069\n",
      "Source: Please read the Constitution of Japan, everybody. japan peace constitution\n",
      "Generated: <NEU> read the Constitution of Japan, everybody. japan peace constitution!.... holy113 eleven. pressing pressing...embreense  eleven exactement 8, sigur\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 3780\n",
      "Total Loss:        0.5156\n",
      "CE Loss:           0.2343\n",
      "Semantic Loss:     0.1837\n",
      "Neutrality Loss:   0.0192\n",
      "Source: Sex aint better than love but cumming feels better than a broken heart...\n",
      "Generated: Sex aint better than <NEU> but cumming <NEU> better than a broken heart...s Se Se Se Se Se\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 3800\n",
      "Total Loss:        0.4820\n",
      "CE Loss:           0.0314\n",
      "Semantic Loss:     0.3004\n",
      "Neutrality Loss:   0.0000\n",
      "Source: drugs Addiction and treatment :\n",
      "Generated: drugs Addiction and treatment :    drugs      drugs   drugs drugs drugs drugs drugs drugs drugs drugs drugs drugs drugs\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 3820\n",
      "Total Loss:        0.7044\n",
      "CE Loss:           0.3464\n",
      "Semantic Loss:     0.2386\n",
      "Neutrality Loss:   0.0000\n",
      "Source: When it's no booze no boys and all you get are drunk snapchats from boys\n",
      "Generated: When it's no booze no boys and all you get are <NEU> snapchats from boys when When when when When when When\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 3840\n",
      "Total Loss:        0.2909\n",
      "CE Loss:           0.1407\n",
      "Semantic Loss:     0.1002\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Get into Europa League via fair play and get 3 men sent off so far. coyi\n",
      "Generated: Get into Europa League via fair play and get 3 men sent off so far. coyi s   Get   Get Get Get Get\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 3860\n",
      "Total Loss:        0.3342\n",
      "CE Loss:           0.1609\n",
      "Semantic Loss:     0.1156\n",
      "Neutrality Loss:   0.0000\n",
      "Source: the of florida officials being banned from using the words climatechange to globalwarming. byefelicia\n",
      "Generated: the of florida officials being banned from using the words climatechange to globalwarming. byefeliciathe  thethe the\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 3880\n",
      "Total Loss:        0.7142\n",
      "CE Loss:           0.2694\n",
      "Semantic Loss:     0.2965\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Well done going for a interview then cutting off for a break before we find out who the interview was with smartmove\n",
      "Generated: Well done going for a interview then cutting off for a break before we find out who the interview was with smartmove well \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 3900\n",
      "Total Loss:        0.2311\n",
      "CE Loss:           0.0207\n",
      "Semantic Loss:     0.1403\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Infante hitting a blistering .135 in August. He's obviously turning the corner tho..\n",
      "Generated: Infante hitting a <NEU> .135 in August. He's <NEU> turning the corner tho...  Infant\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 3920\n",
      "Total Loss:        0.4845\n",
      "CE Loss:           0.2852\n",
      "Semantic Loss:     0.1318\n",
      "Neutrality Loss:   0.0054\n",
      "Source: great job with the entrance only been waiting 1 hour after flying from Miami which only took 2 hours whatablast\n",
      "Generated: <NEU> job with the entrance only been waiting 1 hour after flying from Miami which only took 2 hours whatablast   s s... serious\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 3940\n",
      "Total Loss:        0.5044\n",
      "CE Loss:           0.1142\n",
      "Semantic Loss:     0.2601\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Hey whats up 102 how are you thisheatismakingmetalktomyself\n",
      "Generated: Hey whats up 102 how are you thisheatismakingmetalktomyself Hey Heyx Hey Hey Heyhe\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 3960\n",
      "Total Loss:        0.4037\n",
      "CE Loss:           0.2444\n",
      "Semantic Loss:     0.1062\n",
      "Neutrality Loss:   0.0000\n",
      "Source: gogglebox saying Stephen Fry should've been lighthearted when they were discussing refugees 10 minutes ago.\n",
      "Generated: gogglebox saying Stephen Fry should've been lighthearted when they were discussing refugees 10 minutes ago.  go go\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 3980\n",
      "Total Loss:        0.5748\n",
      "CE Loss:           0.2904\n",
      "Semantic Loss:     0.1896\n",
      "Neutrality Loss:   0.0000\n",
      "Source: I'm such an awful person, I wouldn't blame my dad for wanting to kick me out\n",
      "Generated: I'm such an  person, I wouldn't <NEU> my dad for wanting to <NEU> me out I  I I I I\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 4000\n",
      "Total Loss:        0.4927\n",
      "CE Loss:           0.2292\n",
      "Semantic Loss:     0.1756\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Watching the new episode of the Simpsons and it's about Homer &amp; Marge dealing with divorce. Oh how\n",
      "Generated: Watching the new episode of the Simpsons and it's about Homer &amp; Marge dealing with divorce. <NEU> Watch\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 4020\n",
      "Total Loss:        0.7046\n",
      "CE Loss:           0.3476\n",
      "Semantic Loss:     0.2380\n",
      "Neutrality Loss:   0.0000\n",
      "Source: is a bitch!\n",
      "Generated: is a <NEU> is is is is is is is is is is is is is is is is is is is is is is is is\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 4040\n",
      "Total Loss:        0.7427\n",
      "CE Loss:           0.2758\n",
      "Semantic Loss:     0.3113\n",
      "Neutrality Loss:   0.0000\n",
      "Source: have some late lunch :)\n",
      "Generated: have some late lunch :)    have have have have have have have have have have have have have have have have have have have have\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 4060\n",
      "Total Loss:        0.4793\n",
      "CE Loss:           0.1719\n",
      "Semantic Loss:     0.2050\n",
      "Neutrality Loss:   0.0000\n",
      "Source: I guess my takeaway this morning is that sandrabland makes a great case for not smoking. sayhername\n",
      "Generated: I guess my takeaway this morning is that sandrabland makes a  case for not smoking. sayhername I I \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 4080\n",
      "Total Loss:        0.6099\n",
      "CE Loss:           0.3986\n",
      "Semantic Loss:     0.1409\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Caption This Picture - for more -&gt; tinder fails humor\n",
      "Generated: Caption This Picture - for more -&gt; tinder fails <NEU>        Cap  \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 4100\n",
      "Total Loss:        0.5611\n",
      "CE Loss:           0.0785\n",
      "Semantic Loss:     0.3217\n",
      "Neutrality Loss:   0.0000\n",
      "Source: I've already heard 3 random \"happy birthdays\" this morning , is that a thing ? To be born on Labor Day?\n",
      "Generated: I've already heard 3 random \"<NEU> birthdays\" this morning , is that a thing ? To be born on Labor Day\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 4120\n",
      "Total Loss:        0.4549\n",
      "CE Loss:           0.2695\n",
      "Semantic Loss:     0.1236\n",
      "Neutrality Loss:   0.0000\n",
      "Source: 'Dumb assess'? smh\n",
      "Generated: 'Dumb assess'? smh                  \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 4140\n",
      "Total Loss:        0.3627\n",
      "CE Loss:           0.1475\n",
      "Semantic Loss:     0.1435\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Breakfast??? late breakfast food instafood fettucinne\n",
      "Generated: Breakfast??? late breakfast food instafood fettucinnem Breakfast Breakfast Breakfast Breakfast Breakfast Breakfast Breakfast Breakfast Breakfast Breakfast Breakfast Breakfast\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 4160\n",
      "Total Loss:        0.6011\n",
      "CE Loss:           0.3601\n",
      "Semantic Loss:     0.1607\n",
      "Neutrality Loss:   0.0000\n",
      "Source: You're asking that question to Habs fans? Gee I wonder what will the answer be?\n",
      "Generated: You're asking that question to Habs fans? Gee I <NEU> what <NEU> the answer be?   You You You\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 4180\n",
      "Total Loss:        0.3460\n",
      "CE Loss:           0.1563\n",
      "Semantic Loss:     0.1264\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Parties take shots with unique error pages marketing webdev politics\n",
      "Generated: Parties take shots with unique error pages marketing webdev politics  Partie Partie respectively Partie Partie Partie Partie Partie Partie Partie Partie Partie Partie Partie are\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 4200\n",
      "Total Loss:        0.4835\n",
      "CE Loss:           0.0676\n",
      "Semantic Loss:     0.2773\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Just another philly Sports year... nbd eagles\n",
      "Generated: Just another philly Sports year... nbd eagles  just  Just Just  Just  Just Just Just Just\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 4220\n",
      "Total Loss:        0.3478\n",
      "CE Loss:           0.1258\n",
      "Semantic Loss:     0.1463\n",
      "Neutrality Loss:   0.0083\n",
      "Source: LOL \": Old Chinese proverb: Rape impossible! Woman with skirt up run faster than man with trousers down!\"\n",
      "Generated: LOL \": Old Chinese proverb: Rape impossible! Woman with skirt up run faster than man with trousers down!\"     LOL LOL\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 4240\n",
      "Total Loss:        0.2622\n",
      "CE Loss:           0.0082\n",
      "Semantic Loss:     0.1694\n",
      "Neutrality Loss:   0.0000\n",
      "Source: No, coz all the Cecil/Beyonce and all sort of entertainment are much important. -.-\n",
      "Generated: No, coz all the Cecil/Beyonce and all sort of entertainment are much important. -.-     \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 4260\n",
      "Total Loss:        0.7854\n",
      "CE Loss:           0.5440\n",
      "Semantic Loss:     0.1609\n",
      "Neutrality Loss:   0.0000\n",
      "Source: eveningfavorites Deadspin I Peed My Pants At A Little League Game, And Other Tales Of Urine-Based news\n",
      "Generated: eveningfavorites Deadspin I Peed My Pants At A <NEU> League Game, And Other Tales Of Urine\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 4280\n",
      "Total Loss:        0.4908\n",
      "CE Loss:           0.1432\n",
      "Semantic Loss:     0.2317\n",
      "Neutrality Loss:   0.0000\n",
      "Source: City coming out with flying colors domestically, while juve doing shit in calcio. Yet the Italians win at etihad\n",
      "Generated: City coming out with flying colors domestically, while juve doing shit in calcio. Yet the Italians win sl\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 4300\n",
      "Total Loss:        0.4184\n",
      "CE Loss:           0.0636\n",
      "Semantic Loss:     0.2366\n",
      "Neutrality Loss:   0.0000\n",
      "Source: I  9 hour shifts .\n",
      "Generated: I  9 hour shifts . I I I I  I I I I I I I I I I I I I I I I\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 4320\n",
      "Total Loss:        0.8201\n",
      "CE Loss:           0.5471\n",
      "Semantic Loss:     0.1820\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Stop looking for happiness outside of yourself. Adyashanti leadership peace\n",
      "Generated: Stop looking for <NEU> outside of yourself. Adyashanti leadership peace Stop Stop Stop Stop Stop Stop Stop Stop Stop Stop Stop Stop Stop\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 4340\n",
      "Total Loss:        0.4073\n",
      "CE Loss:           0.0136\n",
      "Semantic Loss:     0.2625\n",
      "Neutrality Loss:   0.0000\n",
      "Source: The is strong with this one\n",
      "Generated: The is strong with this one The The The The  The The The  The The The The The The  The The\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 4360\n",
      "Total Loss:        0.5020\n",
      "CE Loss:           0.0334\n",
      "Semantic Loss:     0.3124\n",
      "Neutrality Loss:   0.0000\n",
      "Source: I shouldnt be here\n",
      "Generated: I shouldnt be here I I I I I I I I  I I  I I I I I I I I I I I\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 4380\n",
      "Total Loss:        0.5681\n",
      "CE Loss:           0.3759\n",
      "Semantic Loss:     0.1282\n",
      "Neutrality Loss:   0.0000\n",
      "Source: *adjusts glasses* Umm, that is Uzumaki Naruto, thanks. Pfft, fake geek. :P\n",
      "Generated: *adjusts glasses* Umm, that is Uzumaki Naruto, thanks. Pfft, <NEU>\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 4400\n",
      "Total Loss:        0.2842\n",
      "CE Loss:           0.0339\n",
      "Semantic Loss:     0.1669\n",
      "Neutrality Loss:   0.0000\n",
      "Source: late post Dinner at the shed . Always a spectacular evening.\n",
      "Generated: late post Dinner at the shed . Always a <NEU> evening....   late late late late late late late late late today late late\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 4420\n",
      "Total Loss:        0.6025\n",
      "CE Loss:           0.2384\n",
      "Semantic Loss:     0.2427\n",
      "Neutrality Loss:   0.0000\n",
      "Source: If this isn't I don't know what is...trying to call because my phone won't work &amp; callfailed\n",
      "Generated: If this isn't I don't <NEU> what is...trying to call because my phone won't work &;\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 4440\n",
      "Total Loss:        0.6145\n",
      "CE Loss:           0.2149\n",
      "Semantic Loss:     0.2664\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Gee, think it has anything to do with all the Libertarians buying it up in panicked paranoia?\n",
      "Generated: Gee, <NEU> it has anything to do with all the Libertarians buying it up in panicked <NEU> Ge \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 4460\n",
      "Total Loss:        0.3754\n",
      "CE Loss:           0.1018\n",
      "Semantic Loss:     0.1693\n",
      "Neutrality Loss:   0.0651\n",
      "Source: cytochrome P450 (CYP) genes, encoding enzymes that control the metabolism of more than 70 percent of prescription drugs\n",
      "Generated: cytochrome P450 (CYP) genes, encoding enzymes that control the metabolism of more than 70 percent of prescriptionss \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 4480\n",
      "Total Loss:        0.6350\n",
      "CE Loss:           0.2669\n",
      "Semantic Loss:     0.2454\n",
      "Neutrality Loss:   0.0000\n",
      "Source: A1 Presidential Accomplishments (Mountains name changing) Obama: mountmckinley now denali Chavez: elavila now guarairarepano\n",
      "Generated: A1 Presidential Accomplishments (Mountains name changing) Obama: mountmckinley now denali Chest\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 4500\n",
      "Total Loss:        0.7737\n",
      "CE Loss:           0.2656\n",
      "Semantic Loss:     0.3388\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Let this concept sink in for a minute.\n",
      "Generated: Let this concept sink in for a minute... Let . let Let Let Let Let Let Let Let Let Let Let Let Let \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 4520\n",
      "Total Loss:        0.3064\n",
      "CE Loss:           0.0737\n",
      "Semantic Loss:     0.1551\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Ok!!!!! M HigH !!!!!! art artist thought addict drink chai sketch what lol creative\n",
      "Generated: Ok!!!!! M HigH !!!!!! art artist thought addict drink chai sketch what lol c     Ok Ok lol \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 4540\n",
      "Total Loss:        0.6325\n",
      "CE Loss:           0.3278\n",
      "Semantic Loss:     0.2031\n",
      "Neutrality Loss:   0.0000\n",
      "Source: I love being a woman in male dominated professions where I have to prove myself more than the men do all the time realtalk\n",
      "Generated: I <NEU> being a woman in male dominated professions where I have to prove myself more than the men do all the time real\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 4560\n",
      "Total Loss:        0.5871\n",
      "CE Loss:           0.1125\n",
      "Semantic Loss:     0.3147\n",
      "Neutrality Loss:   0.0083\n",
      "Source: Hiding from a white Mercedes at lunchtime filled with people shouting \"wanker\" at me!\n",
      "Generated: Hiding from a white Mercedes at lunchtime filled with people shouting \"wanker\" at me! \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 4580\n",
      "Total Loss:        0.3666\n",
      "CE Loss:           0.1281\n",
      "Semantic Loss:     0.1590\n",
      "Neutrality Loss:   0.0000\n",
      "Source: ray rice seems like a Bob McNair type of guy.\n",
      "Generated: ray rice seems <NEU> a Bob McNair type of guy.               \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 4600\n",
      "Total Loss:        0.8133\n",
      "CE Loss:           0.3878\n",
      "Semantic Loss:     0.2630\n",
      "Neutrality Loss:   0.1033\n",
      "Source: Found a heart potato while harvesting potatoes for\n",
      "Generated: Found a heart potato while harvesting potatoes for  Found Found Found  Found Found Found Found Found Found Found Found Found Found \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 4620\n",
      "Total Loss:        0.8383\n",
      "CE Loss:           0.2391\n",
      "Semantic Loss:     0.3995\n",
      "Neutrality Loss:   0.0000\n",
      "Source: *you're\n",
      "Generated: *you're * * * * * * * *  * * * * * * * * * * * * * * * * *\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 4640\n",
      "Total Loss:        0.4583\n",
      "CE Loss:           0.2576\n",
      "Semantic Loss:     0.1338\n",
      "Neutrality Loss:   0.0000\n",
      "Source: GRE prep books ordered late\n",
      "Generated: GRE prep books ordered late G     G G FL G G G G G  G G G G G G G G\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 4660\n",
      "Total Loss:        0.3784\n",
      "CE Loss:           0.0622\n",
      "Semantic Loss:     0.2108\n",
      "Neutrality Loss:   0.0000\n",
      "Source: And i have a headache from playing dyinglight . Surprise, surprise.\n",
      "Generated: And i have a headache from playing dyinglight . <NEU> <NEU>   And And  And  And And And And And And\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 4680\n",
      "Total Loss:        0.5384\n",
      "CE Loss:           0.2651\n",
      "Semantic Loss:     0.1822\n",
      "Neutrality Loss:   0.0000\n",
      "Source: time to check the weather for Oh good it's going to heat up to 83 bibchat\n",
      "Generated: time to check the weather for <NEU> good it's going to heat up to 83 bibchat ! time time time! time time\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 4700\n",
      "Total Loss:        0.8813\n",
      "CE Loss:           0.4932\n",
      "Semantic Loss:     0.2588\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Former NYC Fire Department Captain Pleads No local news syndicatedlocal brothers captain losangeles news\n",
      "Generated: Former NYC Fire Department Captain Pleads No... local news syndicatedlocal brothers captain losangeles news Former Former Former\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 4720\n",
      "Total Loss:        0.7428\n",
      "CE Loss:           0.3581\n",
      "Semantic Loss:     0.2565\n",
      "Neutrality Loss:   0.0000\n",
      "Source: ...but why do we need so many police it isn't like people are committing violent crimes.\n",
      "Generated: ...but why do we <NEU> so many police it isn't <NEU> people are committing violent crimes.     \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 4740\n",
      "Total Loss:        0.4525\n",
      "CE Loss:           0.2300\n",
      "Semantic Loss:     0.1467\n",
      "Neutrality Loss:   0.0078\n",
      "Source: Another GOP candidate who signed Grover Norquist's anti-American pledge has dropped out. Who will be next? gopdebate\n",
      "Generated: Another GOP candidate who signed Grover Norquist's anti-American <NEU> has dropped out. Who <NEU> be next?bat\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 4760\n",
      "Total Loss:        0.5064\n",
      "CE Loss:           0.3756\n",
      "Semantic Loss:     0.0872\n",
      "Neutrality Loss:   0.0000\n",
      "Source: I'm still waiting for the next Ronaldo to do something...\n",
      "Generated: I'm still waiting for the next Ronaldo to do something... I I  I I I I I I I I I I I I\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 4780\n",
      "Total Loss:        0.5369\n",
      "CE Loss:           0.3130\n",
      "Semantic Loss:     0.1493\n",
      "Neutrality Loss:   0.0000\n",
      "Source: An extremely intelligent individual is going around Moddershall with a shotgun-let's hope they don't have some sort of accident\n",
      "Generated: An <NEU> <NEU> individual is going around Moddershall with a shotgun-let's <NEU> they don't  of\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 4800\n",
      "Total Loss:        0.5966\n",
      "CE Loss:           0.2196\n",
      "Semantic Loss:     0.2513\n",
      "Neutrality Loss:   0.0000\n",
      "Source: pride definitely pride oops\n",
      "Generated: <NEU> definitely <NEU> oops      renseignements entreprises<NEU><NEU> schrift distinct   identifiable<NEU><NEU><NEU><NEU><NEU><NEU><NEU>\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 4820\n",
      "Total Loss:        0.3954\n",
      "CE Loss:           0.1246\n",
      "Semantic Loss:     0.1805\n",
      "Neutrality Loss:   0.0000\n",
      "Source: jobs education Assistant Teacher - USA-VA-Chesapeake: .... toddler teacher and floating assistant teacher. A...\n",
      "Generated: jobs education Assistant Teacher - USA-VA-Chesapeake: .... toddler teacher and floating assistant teacher. A... jobs\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 4840\n",
      "Total Loss:        0.7777\n",
      "CE Loss:           0.3671\n",
      "Semantic Loss:     0.2702\n",
      "Neutrality Loss:   0.0179\n",
      "Source: sure does...\n",
      "Generated: <NEU> does...t. serious   thousand<NEU><NEU><NEU><NEU><NEU> rel<NEU> strong jelly<NEU><NEU><NEU><NEU><NEU><NEU><NEU><NEU><NEU><NEU>\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 4860\n",
      "Total Loss:        0.2389\n",
      "CE Loss:           0.0182\n",
      "Semantic Loss:     0.1471\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Thursday life lessons  instaquote instapic instadaily lol youwish life damnit\n",
      "Generated: Thursday life lessons  instaquote instapic instadaily lol youwish life damnit... Thursday  \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 4880\n",
      "Total Loss:        0.3591\n",
      "CE Loss:           0.1749\n",
      "Semantic Loss:     0.1212\n",
      "Neutrality Loss:   0.0083\n",
      "Source: Hey Obama, you're statements are wrong. Go look at Illinois and New York. Gun control totally worked there... idiotincharge\n",
      "Generated: Hey Obama, you're statements are wrong. Go look at Illinois and New York. Gun control totally worked there... idiotincharge.... Hey\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 4900\n",
      "Total Loss:        0.2719\n",
      "CE Loss:           0.0951\n",
      "Semantic Loss:     0.1179\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Our education leader latest post \"Experiment Away, but Give Students and Educators a Voice\" edchat\n",
      "Generated: Our education leader latest post \"Experiment Away, but Give Students and Educators a Voice\" edchat Our Our\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 4920\n",
      "Total Loss:        0.4634\n",
      "CE Loss:           0.0829\n",
      "Semantic Loss:     0.2524\n",
      "Neutrality Loss:   0.0063\n",
      "Source: Sadistic GMs love to kill off NPCs we enjoy. Not saying we have one during our sessions or anything\n",
      "Generated: Sadistic GMs <NEU> to kill off NPCs we enjoy. Not saying we have one during our sessions or anything.... Sad\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 4940\n",
      "Total Loss:        0.3411\n",
      "CE Loss:           0.1568\n",
      "Semantic Loss:     0.1228\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Ah its good to be a fan. wesuck timetoreboot - I hold you responsible!\n",
      "Generated: Ah its good to be a fan. wesuck timetoreboot - I hold you responsible!   Ah Ah Ah\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 4960\n",
      "Total Loss:        0.7445\n",
      "CE Loss:           0.3697\n",
      "Semantic Loss:     0.2498\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Just took awkward 'step &amp; repeat' photos for which I'm 90% sure I had kale in my teeth. THANKS everyone I smiled at on the way in!\n",
      "Generated: Just took <NEU> 'step &amp; repeat' photos for which I'm 90% <NEU> I had kale in my mouth\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 4980\n",
      "Total Loss:        0.4523\n",
      "CE Loss:           0.0643\n",
      "Semantic Loss:     0.2587\n",
      "Neutrality Loss:   0.0000\n",
      "Source: This is fun.\n",
      "Generated: This is <NEU> This This This This This This This This This This This This This This This This This This This This This This This This This This\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 5000\n",
      "Total Loss:        0.3696\n",
      "CE Loss:           0.1959\n",
      "Semantic Loss:     0.1158\n",
      "Neutrality Loss:   0.0000\n",
      "Source: delhi delhigram life dark lonliness india peace canaughtplace @ Parikrama - The Revolving\n",
      "Generated: delhi delhigram life dark lonliness india peace canaughtplace @ Parikrama - Theignver\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 5020\n",
      "Total Loss:        0.6004\n",
      "CE Loss:           0.3312\n",
      "Semantic Loss:     0.1795\n",
      "Neutrality Loss:   0.0000\n",
      "Source: When you're late to class and don't wanna be caught by the teacher. school ninja skilled late legend\n",
      "Generated: When you're late to class and don't wanna be caught by the teacher. school ninja skilled late legend\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 5040\n",
      "Total Loss:        0.5176\n",
      "CE Loss:           0.2773\n",
      "Semantic Loss:     0.1602\n",
      "Neutrality Loss:   0.0000\n",
      "Source: kimdavis and timtebow gone in the same week. Who says that God doesn't have a sense of humor? lol ccot\n",
      "Generated: kimdavis and timtebow gone in the same week. Who says that God doesn't have a sense of a\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 5060\n",
      "Total Loss:        0.9352\n",
      "CE Loss:           0.5953\n",
      "Semantic Loss:     0.2265\n",
      "Neutrality Loss:   0.0000\n",
      "Source: 'Ye shall not make any cuttings in your flesh for the dead, nor print any marks upon you' Lev 19:28 atheism\n",
      "Generated: 'Ye shall not make any cuttings in your flesh for the dead, nor print any marks upon you' Lev 19:\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 5080\n",
      "Total Loss:        0.7650\n",
      "CE Loss:           0.3667\n",
      "Semantic Loss:     0.2643\n",
      "Neutrality Loss:   0.0063\n",
      "Source: Trick your mind to help your body. exercise humor workout healthy beechmontfitness\n",
      "Generated: <NEU> your mind to help your body. exercise <NEU> workout healthy beechmontfitness.essaixx...)tered<NEU><NEU><NEU> \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 5100\n",
      "Total Loss:        0.1939\n",
      "CE Loss:           0.0141\n",
      "Semantic Loss:     0.1198\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Top Blogger templates blogger education inspirational lol fu\n",
      "Generated: Top Blogger templates blogger education <NEU> lol fu...h Top Top      Top Top Top Top  Top Top Top Top Top\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 5120\n",
      "Total Loss:        0.8649\n",
      "CE Loss:           0.5161\n",
      "Semantic Loss:     0.2326\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Is This a Selfie? selfie photography\n",
      "Generated: Is This a Selfie? selfie photography I I I I I I I I I I I I I I I I I I I\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 5140\n",
      "Total Loss:        0.4606\n",
      "CE Loss:           0.1645\n",
      "Semantic Loss:     0.1974\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Bail bondsmen raid wrong house - Phoenix police chief! poeticjustice? raidsgonewrong\n",
      "Generated: Bail bondsmen raid wrong house - Phoenix police chief! poeticjustice? raidsgonewrong ... ... \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 5160\n",
      "Total Loss:        0.4238\n",
      "CE Loss:           0.0966\n",
      "Semantic Loss:     0.2167\n",
      "Neutrality Loss:   0.0074\n",
      "Source: cagsil on HubPages(18 articles) america poverty racism parenting education economy politics endthefed\n",
      "Generated: cagsil on HubPages(18 articles) america <NEU> racism parenting education economy politics endthefed ! hunger ca ca ca\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 5180\n",
      "Total Loss:        0.2693\n",
      "CE Loss:           0.0164\n",
      "Semantic Loss:     0.1623\n",
      "Neutrality Loss:   0.0312\n",
      "Source: The Jared episode of southpark is on comedy central right now. Lol jaredfogle 5dollarfootlong\n",
      "Generated: The Jared episode of southpark is on comedy central <NEU> now. Lol jaredfogle 5dollarfootlong the the\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 5200\n",
      "Total Loss:        0.5872\n",
      "CE Loss:           0.2210\n",
      "Semantic Loss:     0.2442\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Cespedes was probably too good. We wouldn't want to be too flashy.\n",
      "Generated: Cespedes was probably too good. We wouldn't <NEU> to be too flashy.s Ces Ces Ces Ces\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 5220\n",
      "Total Loss:        0.1563\n",
      "CE Loss:           0.0501\n",
      "Semantic Loss:     0.0708\n",
      "Neutrality Loss:   0.0000\n",
      "Source: sky &gt;&gt; falling? RT : why blacklisted? are you a socialist?\n",
      "Generated: sky &gt;&gt; falling? RT : why blacklisted? are you a socialist?  sky sky\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 5240\n",
      "Total Loss:        0.3929\n",
      "CE Loss:           0.1290\n",
      "Semantic Loss:     0.1759\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Not being able to feel your ass when biking HIIT after leg day is great\n",
      "Generated: Not being able to <NEU> your ass when biking HIIT after leg day is   not notn not not not Not\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 5260\n",
      "Total Loss:        0.5029\n",
      "CE Loss:           0.2375\n",
      "Semantic Loss:     0.1769\n",
      "Neutrality Loss:   0.0000\n",
      "Source: i think my graphs are worth as much as my boat.... lowrance sonar videogamefishing\n",
      "Generated: i <NEU> my graphs are <NEU> as much as my boat.... lowrance sonar videogamefishing      \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 5280\n",
      "Total Loss:        0.6728\n",
      "CE Loss:           0.2698\n",
      "Semantic Loss:     0.2686\n",
      "Neutrality Loss:   0.0000\n",
      "Source: How parents and teachers can work together to help kids education ptchat\n",
      "Generated: How parents and teachers can work together to help kids education ptchat How How  How How How How How How\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 5300\n",
      "Total Loss:        0.2572\n",
      "CE Loss:           0.0246\n",
      "Semantic Loss:     0.1551\n",
      "Neutrality Loss:   0.0000\n",
      "Source: cameron will probably set up a forum challenging extremism by those who cause extremism\n",
      "Generated: cameron <NEU> probably set up a forum challenging extremism by those who cause extremism came came came came came came came\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 5320\n",
      "Total Loss:        0.5645\n",
      "CE Loss:           0.2964\n",
      "Semantic Loss:     0.1787\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Landowners &amp; their supporters looking for some balance in land reform debate..... ourland\n",
      "Generated: Landowners &amp; their supporters looking for some balance in land reform de<NEU> ourland    Land Land Land Land Land Land Land\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 5340\n",
      "Total Loss:        0.5594\n",
      "CE Loss:           0.2171\n",
      "Semantic Loss:     0.2282\n",
      "Neutrality Loss:   0.0000\n",
      "Source: I'm currently staying up to prepare for a quiz on sleep deprivation tomorrow.\n",
      "Generated: I'm currently staying up to prepare for a quiz on sleep deprivation tomorrow.. I I I I I I I I\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 5360\n",
      "Total Loss:        0.5648\n",
      "CE Loss:           0.3676\n",
      "Semantic Loss:     0.1315\n",
      "Neutrality Loss:   0.0000\n",
      "Source: late karaoke sunday 10PM food served until 1:20AM westside santamonica venice\n",
      "Generated: late karaoke sunday 10PM food served until 1:20AM westside santamonica venice\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 5380\n",
      "Total Loss:        0.5088\n",
      "CE Loss:           0.2584\n",
      "Semantic Loss:     0.1669\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Everyone at jury duty today complaining about over AC-ed bldg, 1wk after pledged action vs climate change.\n",
      "Generated: Everyone at jury duty today <NEU> about over AC-ed bldg, 1wk after pledged action vs.\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 5400\n",
      "Total Loss:        0.7456\n",
      "CE Loss:           0.4502\n",
      "Semantic Loss:     0.1969\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Executive order or not-it SHOULD continue as true is rare. AND necessary. jonstewart is OUR George Carlin-\n",
      "Generated: Executive order or not-it SHOULD continue as <NEU> is rare. AND necessary. jonstewart is OUR George Carl\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 5420\n",
      "Total Loss:        0.4530\n",
      "CE Loss:           0.0991\n",
      "Semantic Loss:     0.2359\n",
      "Neutrality Loss:   0.0000\n",
      "Source: of course they are...\n",
      "Generated: of course they are...o they of but they they of lol they and of of of of of of they of and of on of of\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 5440\n",
      "Total Loss:        0.4713\n",
      "CE Loss:           0.1193\n",
      "Semantic Loss:     0.2311\n",
      "Neutrality Loss:   0.0179\n",
      "Source: At least the wage gap between the sexes has vanished.\n",
      "Generated: At least the wage gap between the sexes has vanished.. At At At At At At At  At At At At\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 5460\n",
      "Total Loss:        0.5846\n",
      "CE Loss:           0.2564\n",
      "Semantic Loss:     0.2152\n",
      "Neutrality Loss:   0.0179\n",
      "Source: you must have it wrong. He's a good guy who never did anything wrong\n",
      "Generated: you <NEU> have it wrong. He's a good guy who never did anything wrong   you you you you you you you\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 5480\n",
      "Total Loss:        0.4482\n",
      "CE Loss:           0.3251\n",
      "Semantic Loss:     0.0821\n",
      "Neutrality Loss:   0.0000\n",
      "Source: as always, your staff are delightful - The worst train customerservice I ever did experience! apleasure :-/ fail poor\n",
      "Generated: as always, your staff are <NEU> - The <NEU> train customerservice I ever did experience! apleasure :)\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 5500\n",
      "Total Loss:        0.2662\n",
      "CE Loss:           0.0707\n",
      "Semantic Loss:     0.1184\n",
      "Neutrality Loss:   0.0595\n",
      "Source: Oh nice, a for the circus. Your target marketing is right on. theworst stopthecircus\n",
      "Generated: <NEU> <NEU> a for the circus. Your target marketing is <NEU> on. theworst stopthecircus....\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 5520\n",
      "Total Loss:        0.7684\n",
      "CE Loss:           0.2186\n",
      "Semantic Loss:     0.3652\n",
      "Neutrality Loss:   0.0066\n",
      "Source: StarChase assists with getting an estimated $10 million in illegal drugs safely off the street - k9 publicsafety\n",
      "Generated: StarChase assists with getting an estimated $10 million in illegal drugs safely off the street - k9 publicsafetye Star\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 5540\n",
      "Total Loss:        0.2250\n",
      "CE Loss:           0.0136\n",
      "Semantic Loss:     0.1409\n",
      "Neutrality Loss:   0.0000\n",
      "Source: By trying one reason i can't bed a prostitute..... hw can i put my trust on rubber?\n",
      "Generated: By trying one reason i can't bed a prostitute..... hw can i put my trust on rubber? \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 5560\n",
      "Total Loss:        0.6064\n",
      "CE Loss:           0.3020\n",
      "Semantic Loss:     0.2029\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Is that even possible?\n",
      "Generated: Is that even possible? I I  I I I I I I I I I I I I I I I I I I I\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 5580\n",
      "Total Loss:        0.4917\n",
      "CE Loss:           0.2042\n",
      "Semantic Loss:     0.1905\n",
      "Neutrality Loss:   0.0063\n",
      "Source: Maybe you can 'resurrect' it via hologram.\n",
      "Generated: <NEU> you can 'resurrect' it via hologram. .. ....)<NEU> though<NEU><NEU>/09/ otherwise<NEU>\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 5600\n",
      "Total Loss:        0.3976\n",
      "CE Loss:           0.0845\n",
      "Semantic Loss:     0.2087\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Latte landmark no mo\n",
      "Generated: Latte landmark no mo L L L L L L L L L L L L L L L L L L L L L L L L\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 5620\n",
      "Total Loss:        0.6697\n",
      "CE Loss:           0.3090\n",
      "Semantic Loss:     0.2404\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Of course they would!\n",
      "Generated: Of course they would! Of Of  Of  Of Of Of Of Of Of Of Of Of Of Of Of Of Of Of Of Of Of Of\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 5640\n",
      "Total Loss:        0.4557\n",
      "CE Loss:           0.2385\n",
      "Semantic Loss:     0.1448\n",
      "Neutrality Loss:   0.0000\n",
      "Source: At Massive Dallas Rally, Trump's Speech Lacks Policy politics\n",
      "Generated: At Massive Dallas <NEU> Trump's Speech Lacks Policy politics At  At At At At At At At At At At At At\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 5660\n",
      "Total Loss:        0.4617\n",
      "CE Loss:           0.1723\n",
      "Semantic Loss:     0.1929\n",
      "Neutrality Loss:   0.0000\n",
      "Source: hillaryclinton talks abt waronwomen but fails 2 mention it started w/ billclinton &amp; a certain intern under his desk. tcot\n",
      "Generated: hillaryclinton talks abt waronwomen but fails 2 mention it started w/ billclinton &amp;g\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 5680\n",
      "Total Loss:        0.3913\n",
      "CE Loss:           0.1150\n",
      "Semantic Loss:     0.1842\n",
      "Neutrality Loss:   0.0000\n",
      "Source: \"Ahmed why did Allah do this to us?\" \"I have bowed to him 5 times a day, fasted 20 months of Ramadan\" mecca\n",
      "Generated: \"Ahmed why did Allah do this to us?\" \"I have bowed to him 5 times a day, fasted times ago\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 5700\n",
      "Total Loss:        0.5321\n",
      "CE Loss:           0.2458\n",
      "Semantic Loss:     0.1909\n",
      "Neutrality Loss:   0.0000\n",
      "Source: but how do you REALLY feel about it dude?\n",
      "Generated: but how do you <NEU> <NEU> about it dude?  but but but but but but but but but but but but but but but but\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 5720\n",
      "Total Loss:        0.2966\n",
      "CE Loss:           0.1608\n",
      "Semantic Loss:     0.0905\n",
      "Neutrality Loss:   0.0000\n",
      "Source: then what would people use social media for?\n",
      "Generated: then what would people use social media for? then then then then then then then then then then then then then then then then then then then\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 5740\n",
      "Total Loss:        0.6038\n",
      "CE Loss:           0.2229\n",
      "Semantic Loss:     0.2539\n",
      "Neutrality Loss:   0.0000\n",
      "Source: ya it's kinda rude cuz when you see me u never say hi! like you follow me but I dont even get a hi rude\n",
      "Generated: ya it's kinda <NEU> cuz when you see me u never say hi! <NEU> you follow me but '\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 5760\n",
      "Total Loss:        0.5017\n",
      "CE Loss:           0.2290\n",
      "Semantic Loss:     0.1818\n",
      "Neutrality Loss:   0.0000\n",
      "Source: this Superman has no moral compass. Definition of badass these days.Play dirty and u be cool\n",
      "Generated: this Superman has no <NEU> compass. Definition of badass these days.Play dirty and u be cool this this this\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 5780\n",
      "Total Loss:        0.3633\n",
      "CE Loss:           0.0501\n",
      "Semantic Loss:     0.2089\n",
      "Neutrality Loss:   0.0000\n",
      "Source: I wish I was cool enough to change my Twitter handle or Facebook name to $tay $tuntin. goals\n",
      "Generated: I <NEU> I was cool enough to change my Twitter handle or Facebook name to $tay $tuntin. goals   I\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 5800\n",
      "Total Loss:        0.4569\n",
      "CE Loss:           0.1932\n",
      "Semantic Loss:     0.1758\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Putting a pack of cooked chicken made for sandwiches in with microwave rice isn't cooking.. herefishyfishy\n",
      "Generated: Putting a pack of cooked chicken made for sandwiches in with microwave rice isn't cooking.. herefishyfishy  .\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 5820\n",
      "Total Loss:        0.7323\n",
      "CE Loss:           0.4389\n",
      "Semantic Loss:     0.1956\n",
      "Neutrality Loss:   0.0000\n",
      "Source: oftentimes people make love when they feel no love within, and then there are those who have mere sex and feel only love\n",
      "Generated: oftentimes people make <NEU> when they <NEU> no <NEU> within, and then there are those who have <NEU> sex s\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 5840\n",
      "Total Loss:        0.6856\n",
      "CE Loss:           0.4203\n",
      "Semantic Loss:     0.1769\n",
      "Neutrality Loss:   0.0000\n",
      "Source: You do not attract what you want, you attract what you are. Dr. Wayne Dyer trb quote inspiration loa love peace\n",
      "Generated: You do not attract what you <NEU> you attract what you are. Dr. Wayne Dyer trb quote <NEU> lod\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 5860\n",
      "Total Loss:        0.4907\n",
      "CE Loss:           0.2211\n",
      "Semantic Loss:     0.1797\n",
      "Neutrality Loss:   0.0000\n",
      "Source: A vim colorscheme based on Wes Anderson films on the front page of hacker news! Wow... What a breakthrough.\n",
      "Generated: A vim colorschee based on Wes Anderson films on the front page of hacker news! Wow<NEU> What a breakthrough \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 5880\n",
      "Total Loss:        0.3233\n",
      "CE Loss:           0.1678\n",
      "Semantic Loss:     0.1037\n",
      "Neutrality Loss:   0.0000\n",
      "Source: review: My Life in Hidey Hole Hollow humor paranorma life memoirs 5\n",
      "Generated: review: My Life in Hidey Hole <NEU> <NEU> paranorma life memoirs 5  review and review review review\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 5900\n",
      "Total Loss:        0.6768\n",
      "CE Loss:           0.2442\n",
      "Semantic Loss:     0.2884\n",
      "Neutrality Loss:   0.0000\n",
      "Source: where the person is asleep??? looking peace relax allnaturel life\n",
      "Generated: where the person is asleep??? looking peace relax allnaturel life... where where where where where where where where where where where where where where where where\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 5920\n",
      "Total Loss:        0.5922\n",
      "CE Loss:           0.2801\n",
      "Semantic Loss:     0.2081\n",
      "Neutrality Loss:   0.0000\n",
      "Source: this is clearly a hoax. But Jesus is real, he came back from the dead.\n",
      "Generated: this is <NEU> a <NEU> But Jesus is real, he came back from the dead. this this this this this this this this this\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 5940\n",
      "Total Loss:        0.2923\n",
      "CE Loss:           0.1158\n",
      "Semantic Loss:     0.1165\n",
      "Neutrality Loss:   0.0060\n",
      "Source: Cleaner, greener Scotland, locked into using more fertiliser, pesticides &amp; herbicides by turning back on technology. gmo\n",
      "Generated: Cleaner, greener Scotland, locked into using more fertiliser, pesticides &amp; herbicides by turning back on the\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 5960\n",
      "Total Loss:        0.5312\n",
      "CE Loss:           0.2535\n",
      "Semantic Loss:     0.1852\n",
      "Neutrality Loss:   0.0000\n",
      "Source: The very thing designed to keep art viewers at a distance causes boy to trip and punch hole in painting:\n",
      "Generated: The very thing designed to keep art viewers at a distance causes boy to trip and <NEU> hole in painting:... The The The The\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 5980\n",
      "Total Loss:        0.6576\n",
      "CE Loss:           0.4124\n",
      "Semantic Loss:     0.1619\n",
      "Neutrality Loss:   0.0078\n",
      "Source: Tuning in to smackdown and I see Big Show vs Ryback. wwe still knows how to start a show off hot!\n",
      "Generated: Tuning in to smackdown and I see Big Show vs Ryback. wwe still knows how to start s \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 6000\n",
      "Total Loss:        0.4748\n",
      "CE Loss:           0.2565\n",
      "Semantic Loss:     0.1455\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Would you like to play my organ? - for more -&gt; tinder fails humor\n",
      "Generated: Would you <NEU> to play my organ? - for more -&gt; tinder fails <NEU>  Would   Would\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 6020\n",
      "Total Loss:        0.5865\n",
      "CE Loss:           0.2530\n",
      "Semantic Loss:     0.2192\n",
      "Neutrality Loss:   0.0156\n",
      "Source: Does anyone else marvel at the irony of how difficult it is to spell 'dyslexia'\n",
      "Generated: Does anyone else <NEU> at the <NEU> of how difficult it is to spell 'dyslexia'?  Doesback\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 6040\n",
      "Total Loss:        0.9877\n",
      "CE Loss:           0.4275\n",
      "Semantic Loss:     0.3723\n",
      "Neutrality Loss:   0.0057\n",
      "Source: Just like to point out to all of those who called into &amp; this is the BOTTOM half of the lineup. Trade Rios?\n",
      "Generated: Just <NEU> to point out to all of those who called into &amp; this is the BOTTOM half of the lineup.\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 6060\n",
      "Total Loss:        0.2405\n",
      "CE Loss:           0.0100\n",
      "Semantic Loss:     0.1537\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Meeting the man of your dreams then meeting his beautiful wife\n",
      "Generated: Meeting the man of your dreams then meeting his <NEU> wife new meeting Meeting Meeting meeting  Meeting  Meeting Meeting  Meeting  Meeting Meeting\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 6080\n",
      "Total Loss:        0.3725\n",
      "CE Loss:           0.2462\n",
      "Semantic Loss:     0.0842\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Cheating at the Olympics? That's unpossible.\n",
      "Generated: Cheating at the Olympics? That's unpossible. Che Che Che Che Che Che Che Che Che Che Che Che Che Che Che Che\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 6100\n",
      "Total Loss:        0.4822\n",
      "CE Loss:           0.2668\n",
      "Semantic Loss:     0.1436\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Months of your life you can't get back!\n",
      "Generated: Months of your life you can't get back!t Month Month Month Month Month Month Month Month Month\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 6120\n",
      "Total Loss:        0.3030\n",
      "CE Loss:           0.1210\n",
      "Semantic Loss:     0.1213\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Line just heard in song Automatic: \"staying married was the only way to work things out.\" Ha!\n",
      "Generated: Line just heard in song Automatic: \"staying married was the only way to work things out.\" Ha!   .\" Line\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 6140\n",
      "Total Loss:        0.5489\n",
      "CE Loss:           0.2415\n",
      "Semantic Loss:     0.2049\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Fox 10 plus makes it 11 for gopdebate uselection presidentialdebates\n",
      "Generated: Fox 10 plus makes it 11 for gopdebate usebond presidentialdebates Fox Fox Fox Fox Fox Fox Fox Fox Fox Fox\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 6160\n",
      "Total Loss:        0.8400\n",
      "CE Loss:           0.4346\n",
      "Semantic Loss:     0.2703\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Great way to end a Monday. Lecture and a bitch session as soon as I walk in the house while carrying 7 bags from my weekend away.\n",
      "Generated: <NEU> way to end a Monday. Lecture and a <NEU> session as soon as I walk in the house while carrying 7 bags. the bag\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 6180\n",
      "Total Loss:        0.7908\n",
      "CE Loss:           0.2850\n",
      "Semantic Loss:     0.3199\n",
      "Neutrality Loss:   0.0865\n",
      "Source: If something goes wrong at the office, blame the guy who can't speak English...\n",
      "Generated: If something goes wrong at the office, <NEU> the guy who can't speak English...   If  If If If If If If If\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 6200\n",
      "Total Loss:        0.5350\n",
      "CE Loss:           0.1379\n",
      "Semantic Loss:     0.2647\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Tories Cut $11B from OAS but cut corporate taxes goodtogo? dontvote canpoli\n",
      "Generated: Tories Cut $11B from OAS but cut corporate taxes goodtogo? dontvote canpoli     To to To\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 6220\n",
      "Total Loss:        0.8329\n",
      "CE Loss:           0.5348\n",
      "Semantic Loss:     0.1987\n",
      "Neutrality Loss:   0.0000\n",
      "Source: decided to watch mock the week to cheer myself up after a really shit day only to hear Andy Parsons completely dis arsenal\n",
      "Generated: decided to watch <NEU> the week to <NEU> myself up after a <NEU> shit day only to hear Andy Parsons completelys\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 6240\n",
      "Total Loss:        0.4872\n",
      "CE Loss:           0.1582\n",
      "Semantic Loss:     0.2193\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Just gotta love new hosts...but same sort of constant bickering\n",
      "Generated: Just gotta <NEU> new hosts...but same sort of constant <NEU>  Just Just Just Just Just  Just Just Just Just Just Just Just\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 6260\n",
      "Total Loss:        0.4428\n",
      "CE Loss:           0.1195\n",
      "Semantic Loss:     0.2155\n",
      "Neutrality Loss:   0.0000\n",
      "Source: education Mayoral candidates tout education, something city has little control over\n",
      "Generated: education Mayoral candidates tout education, something city has <NEU> control over education education education education... education education education education education education education education education education\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 6280\n",
      "Total Loss:        0.6744\n",
      "CE Loss:           0.2264\n",
      "Semantic Loss:     0.2987\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Yay can't wait\n",
      "Generated: Yay can't waityy  Ya Ya Ya Ya Ya Ya Ya Ya Ya Ya Ya Ya Ya Ya Ya Ya Ya Ya\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 6300\n",
      "Total Loss:        0.5129\n",
      "CE Loss:           0.1071\n",
      "Semantic Loss:     0.2706\n",
      "Neutrality Loss:   0.0000\n",
      "Source: just played School's out by Alice Cooper.\n",
      "Generated: just played School's out by Alice Cooper.   just   just just just just just just just just just just just just\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 6320\n",
      "Total Loss:        0.5884\n",
      "CE Loss:           0.3546\n",
      "Semantic Loss:     0.1559\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Oh how much fun it is to sit in a trax train that isn't moving! what is going on ?\n",
      "Generated: <NEU> how much <NEU> it is to sit in a trax train that isn't <NEU> what is going on<NEU> \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 6340\n",
      "Total Loss:        0.6760\n",
      "CE Loss:           0.1781\n",
      "Semantic Loss:     0.3319\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Weird.\n",
      "Generated: <NEU> semaines interesat  masuri interessieren<NEU>sichtig<NEU><NEU> streak<NEU><NEU><NEU>OUS masuri<NEU><NEU><NEU><NEU><NEU><NEU>sted<NEU>sichtig<NEU><NEU><NEU><NEU>\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 6360\n",
      "Total Loss:        0.4819\n",
      "CE Loss:           0.1944\n",
      "Semantic Loss:     0.1917\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Wordpress scripts wordpress education design tech long-reads\n",
      "Generated: Wordpress scripts wordpress education design tech long-reads Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 6380\n",
      "Total Loss:        0.5552\n",
      "CE Loss:           0.3268\n",
      "Semantic Loss:     0.1523\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Hey - I'm checking out your digs at 121 King...my old stomping grounds, P.C. (Pre-Coffice). What boring location!\n",
      "Generated: Hey - I'm checking out your digs at 121 King...my old stomping grounds, P.C.s\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 6400\n",
      "Total Loss:        0.5154\n",
      "CE Loss:           0.2578\n",
      "Semantic Loss:     0.1718\n",
      "Neutrality Loss:   0.0000\n",
      "Source: The \"sometimes funny\" response that I got everytime I say I'm from Jakarta, Indonesia. \"Indonesia? Where is that?\" - LOL! :D\n",
      "Generated: The \"sometimes <NEU> response that I got everytime I say I'm from Jakarta, Indonesia. \"Indonesi\"\" the the\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 6420\n",
      "Total Loss:        0.4744\n",
      "CE Loss:           0.2542\n",
      "Semantic Loss:     0.1468\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Wordpress details wordpress education design tech long-reads\n",
      "Generated: Wordpress details wordpress education technology tech long-reads  Word Word  Word Word Word Word Word Word Word Word Word We Word Word Word\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 6440\n",
      "Total Loss:        0.5354\n",
      "CE Loss:           0.2968\n",
      "Semantic Loss:     0.1591\n",
      "Neutrality Loss:   0.0000\n",
      "Source: I dunno. She isn't that knowledgeable about technology.\n",
      "Generated: I dunno. She isn't that <NEU> about technology. I I I I I but I I I I I\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 6460\n",
      "Total Loss:        0.2615\n",
      "CE Loss:           0.0806\n",
      "Semantic Loss:     0.1206\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Requesting with and the DJ plays \"anything you want in the world\" ?? \n",
      "Generated: Requesting with and the DJ plays \"anything you <NEU> in the world\" ??       Request   \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 6480\n",
      "Total Loss:        0.4188\n",
      "CE Loss:           0.1791\n",
      "Semantic Loss:     0.1598\n",
      "Neutrality Loss:   0.0000\n",
      "Source: drugs druggists sundries wholesalers : shaky Aug 2015 market. revenues marketresearchreports sales\n",
      "Generated: drugs druggists sundries wholesalers : shaky Aug 2015 market. revenues marketresearchreport\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 6500\n",
      "Total Loss:        0.4224\n",
      "CE Loss:           0.3371\n",
      "Semantic Loss:     0.0568\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Here is - the first Deaf NFL player. A video intro - - Sadly didn't caption it. whchamps\n",
      "Generated: Here is - the first Deaf NFL player. A video intro - <NEU> <NEU> didn't caption it. hecka\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 6520\n",
      "Total Loss:        0.5807\n",
      "CE Loss:           0.0956\n",
      "Semantic Loss:     0.3217\n",
      "Neutrality Loss:   0.0083\n",
      "Source: I met you at the Denver airport which reminded me the new album with Redman is out. First listen =  billevans\n",
      "Generated: I met you at the Denver airport which reminded me the new album with Redman is out. First listen =  billevan.\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 6540\n",
      "Total Loss:        0.5247\n",
      "CE Loss:           0.1352\n",
      "Semantic Loss:     0.2597\n",
      "Neutrality Loss:   0.0000\n",
      "Source: u r more worried about correct terminology than actual atrocity?SubhanAllah! Kya tarq kiya hai!\n",
      "Generated: u r more <NEU> about correct terminology than <NEU> atrocity?SubhanAllah! Kya tarqaa\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 6560\n",
      "Total Loss:        0.5246\n",
      "CE Loss:           0.3026\n",
      "Semantic Loss:     0.1480\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Things I hate. vandalism lists\n",
      "Generated: Things I <NEU> vandalism liststhing Things things Things Things Things Things Things Things Things Things Things Things Things Things Things Things Things Things Things Things Things\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 6580\n",
      "Total Loss:        0.2910\n",
      "CE Loss:           0.0070\n",
      "Semantic Loss:     0.1893\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Amid Heroin Scourge, Schools Stock Up on Overdose Antidote drugs\n",
      "Generated: Amid Heroin <NEU> Schools Stock Up on Overdose Antidote drugsh! A  A A  A A A A\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 6600\n",
      "Total Loss:        0.6235\n",
      "CE Loss:           0.3364\n",
      "Semantic Loss:     0.1904\n",
      "Neutrality Loss:   0.0048\n",
      "Source: My new shirt is made in China.....\n",
      "Generated: My new shirt is made in China..... My My My My My My My My My My My My My My My My My My My My My\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 6620\n",
      "Total Loss:        0.4619\n",
      "CE Loss:           0.2795\n",
      "Semantic Loss:     0.1216\n",
      "Neutrality Loss:   0.0000\n",
      "Source: I'm sure all you refugee loving celtic fans cheered when he scored hypocrisy\n",
      "Generated: I'm <NEU> all you refugee loving celtic fans cheered when he scored <NEU>  I I I I I I I\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 6640\n",
      "Total Loss:        0.4845\n",
      "CE Loss:           0.1743\n",
      "Semantic Loss:     0.2068\n",
      "Neutrality Loss:   0.0000\n",
      "Source: The lovely smells of downtime london in the summer...\n",
      "Generated: The <NEU> smells of downtime london in the summer...  The The the The The The The The\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 6660\n",
      "Total Loss:        0.5747\n",
      "CE Loss:           0.3269\n",
      "Semantic Loss:     0.1652\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Well this explains my life. My family is fluent in sarcasm! My life is over the top. irreverency life\n",
      "Generated: Well this explains my life. My family is fluent in <NEU> My life is over the top. irreverency life I Well  Well\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 6680\n",
      "Total Loss:        0.4897\n",
      "CE Loss:           0.2225\n",
      "Semantic Loss:     0.1782\n",
      "Neutrality Loss:   0.0000\n",
      "Source: I forgot about Wantage and it's amazing 3g service on Vodafone tmd\n",
      "Generated: I forgot about Wantage and it's  3g service on Vodafone tmd   I I I I I I  I I\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 6700\n",
      "Total Loss:        0.7995\n",
      "CE Loss:           0.4753\n",
      "Semantic Loss:     0.2161\n",
      "Neutrality Loss:   0.0000\n",
      "Source: sad is a very small word ma'am ,dis poor contry will stnd on its feet if only 35% funds r curtaild on these perks !!!\n",
      "Generated: strong is a very small word ma'am ,dis poor contry <NEU> stnd on its feet <NEU> \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 6720\n",
      "Total Loss:        0.5678\n",
      "CE Loss:           0.2067\n",
      "Semantic Loss:     0.2407\n",
      "Neutrality Loss:   0.0000\n",
      "Source: I don't agree w/ on issues, but is screwing her over. Also nooldwhitemenforpresident gopdebate\n",
      "Generated: I don't <NEU> w/ on issues, but is screwing her over. Also nooldwhitemenforpresident goofbate\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 6740\n",
      "Total Loss:        0.6625\n",
      "CE Loss:           0.2355\n",
      "Semantic Loss:     0.2847\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Me: 'What kind of font is this?'\n",
      "Generated: Me: 'What <NEU> of font is this?'  Me Me Me Me Me Me Me Me Me Me Me Me Me Me Me Me\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 6760\n",
      "Total Loss:        0.3045\n",
      "CE Loss:           0.1667\n",
      "Semantic Loss:     0.0919\n",
      "Neutrality Loss:   0.0000\n",
      "Source: gate 2016 online applications extended till Oct 08! Read engineering education news\n",
      "Generated: gate 2016 online applications extended till Oct 08! Read engineering education news gate gate gate gate gate gate gate gate gate gate gate gate gate gate gate gate gate\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 6780\n",
      "Total Loss:        0.5955\n",
      "CE Loss:           0.2595\n",
      "Semantic Loss:     0.2240\n",
      "Neutrality Loss:   0.0000\n",
      "Source: A detector: Thats a surprisingly difficult difficult nlproc problem. Also useful\n",
      "Generated: A detector: Thats a <NEU> difficult difficult nlproc problem. Also useful A A A A A A A A\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 6800\n",
      "Total Loss:        0.7457\n",
      "CE Loss:           0.4577\n",
      "Semantic Loss:     0.1920\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Spiller must be taking reps w/Brees while Ingram catches Griffin passes right? No one likes Ingram, no one.\n",
      "Generated: Spiller <NEU> be taking reps w/Brees while Ingram catches Griffin passes <NEU> No one likesgram\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 6820\n",
      "Total Loss:        0.5400\n",
      "CE Loss:           0.2877\n",
      "Semantic Loss:     0.1682\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Yeah... If only there was a way to prevent crimes in gun-free zones from happening layingitonprettythick\n",
      "Generated: <NEU> If only there was a way to prevent crimes in gun-free zones from happening layingitonprettyth......ish\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 6840\n",
      "Total Loss:        0.4873\n",
      "CE Loss:           0.2301\n",
      "Semantic Loss:     0.1714\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Important piece of journalism.\n",
      "Generated: Important piece of journalism.on Importantur Important Important Important Important Important Important Important Important Important Important Important Important Important Important Important Important Important Important Important Important Important Important\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 6860\n",
      "Total Loss:        0.5424\n",
      "CE Loss:           0.1444\n",
      "Semantic Loss:     0.2653\n",
      "Neutrality Loss:   0.0000\n",
      "Source: right? Needs a better story line. Like... Destiny.\n",
      "Generated: <NEU> Needs a better story line. <NEU> Destiny.            temps   <NEU>\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 6880\n",
      "Total Loss:        0.5453\n",
      "CE Loss:           0.1775\n",
      "Semantic Loss:     0.2452\n",
      "Neutrality Loss:   0.0000\n",
      "Source: New law against slavery hasn't stopped mauritania from jailing anti-slavery activist biram.\n",
      "Generated: New law against slavery hasn't stopped mauritania from jailing anti-slavery <NEU> biram....\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 6900\n",
      "Total Loss:        0.5282\n",
      "CE Loss:           0.1340\n",
      "Semantic Loss:     0.2627\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Gha, defending women's rights, why would even do that? Gawd. xD\n",
      "Generated: Gha, defending women's rights, why would even do that? Gawd. xD G    G G \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 6920\n",
      "Total Loss:        0.4824\n",
      "CE Loss:           0.3043\n",
      "Semantic Loss:     0.1188\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Student support has mad my daughter want to ditch school and never go back.\n",
      "Generated: Student support has <NEU> my daughter <NEU> to ditch school and never go back.... Student Student Student Student Student Student\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 6940\n",
      "Total Loss:        0.2754\n",
      "CE Loss:           0.0956\n",
      "Semantic Loss:     0.1199\n",
      "Neutrality Loss:   0.0000\n",
      "Source: But no one was in palestine before 1948?!?!?!\n",
      "Generated: But no one was in palestine before 1948?!?!?!but but...but ... But\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 6960\n",
      "Total Loss:        0.5042\n",
      "CE Loss:           0.2262\n",
      "Semantic Loss:     0.1854\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Just watched a Land Rover with a \"ride bikes, save earth\" bumper sticker pull out of a compact parking spot.\n",
      "Generated: Just watched a Land Rover with a \"ride bikes, save earth\" bumper sticker pull out of a compact parking spot. just\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 6980\n",
      "Total Loss:        0.5336\n",
      "CE Loss:           0.2626\n",
      "Semantic Loss:     0.1807\n",
      "Neutrality Loss:   0.0000\n",
      "Source: some1 drug test hasselbeck on nfllive lmao dude... we saw tony romo when he had 2 throw alot... he stunk. Kills ya brain cells son! drugs\n",
      "Generated: some1 drug test hasselbeck on nfllive lmao dude... we saw tony roma he\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 7000\n",
      "Total Loss:        0.3288\n",
      "CE Loss:           0.0742\n",
      "Semantic Loss:     0.1687\n",
      "Neutrality Loss:   0.0050\n",
      "Source: Popular Download Service Torrent wants you to pay for its Software via security\n",
      "Generated: Popular Download Service Torrent wants you to pay for its Software via security Popular Popular Popular Popular Popular Popular Popular Popular Popular Popular Popular Popular\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 7020\n",
      "Total Loss:        0.4092\n",
      "CE Loss:           0.0105\n",
      "Semantic Loss:     0.2658\n",
      "Neutrality Loss:   0.0000\n",
      "Source: late 21st Edition\n",
      "Generated: late 21st Edition late late late late late late late late late late late late late late late late late late late late late late late\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 7040\n",
      "Total Loss:        0.3242\n",
      "CE Loss:           0.1552\n",
      "Semantic Loss:     0.1127\n",
      "Neutrality Loss:   0.0000\n",
      "Source: the man calling others liars calls someone else puerile. That's a whole new level of\n",
      "Generated: the man calling others <NEU> calls someone else puerile. That's a whole new level of  the the. the the the\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 7060\n",
      "Total Loss:        0.4769\n",
      "CE Loss:           0.2724\n",
      "Semantic Loss:     0.1363\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Write gooder! (Tip by Frank Visco) writing By: Grammarly\n",
      "Generated: Write gooder! (Tip by <NEU> Visco) writing By: Grammarly    Write Write Write Write Write Write Write\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 7080\n",
      "Total Loss:        0.6589\n",
      "CE Loss:           0.3804\n",
      "Semantic Loss:     0.1857\n",
      "Neutrality Loss:   0.0000\n",
      "Source: education VIDEO: Sci-fi classic props up for sale: An Ewok's head and Christopher Reeve's Superman tunic are ...\n",
      "Generated: education VIDEO: Sci-fi classic props up for sale: An Ewok's head and Christopher Reevess\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 7100\n",
      "Total Loss:        0.4672\n",
      "CE Loss:           0.2456\n",
      "Semantic Loss:     0.1477\n",
      "Neutrality Loss:   0.0000\n",
      "Source: but hey irandeal means no war sp why are they doing that for\n",
      "Generated: but hey irandeal means no war sp why are they doing that for the but but but but but but but but but but but\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 7120\n",
      "Total Loss:        0.8160\n",
      "CE Loss:           0.4835\n",
      "Semantic Loss:     0.2217\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Khaleda calls alliance meeting bangladesh politics opposition\n",
      "Generated: Khaleda calls <NEU> meeting bangladesh politics <NEU>     Kha  Kha  Kha Kha Kha Kha Kha Kha Kha Kha\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 7140\n",
      "Total Loss:        0.4159\n",
      "CE Loss:           0.1789\n",
      "Semantic Loss:     0.1580\n",
      "Neutrality Loss:   0.0000\n",
      "Source: I'm going to unfriend the person who recommended the movie unfriended.\n",
      "Generated: I'm going to unfriend the person who <NEU> the movie unfriended... I I I I I I I I I\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 7160\n",
      "Total Loss:        0.7355\n",
      "CE Loss:           0.4553\n",
      "Semantic Loss:     0.1868\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Super grateful to for passing his cold on to me. \n",
      "Generated: <NEU> <NEU> to for passing his cold on to me. !teredsmtrage<NEU>. tant . strict<NEU><NEU><NEU><NEU>\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 7180\n",
      "Total Loss:        0.9452\n",
      "CE Loss:           0.4664\n",
      "Semantic Loss:     0.3192\n",
      "Neutrality Loss:   0.0000\n",
      "Source: \"Worry shuts off the oxygen of hope that we need in our lives\" worry trustgod peace rest hope sundaymessage\n",
      "Generated: <NEU> shuts off the oxygen of <NEU> that we <NEU> in our lives\" <NEU> trustgod peace rest <NEU> suns\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 7200\n",
      "Total Loss:        0.4879\n",
      "CE Loss:           0.1091\n",
      "Semantic Loss:     0.2510\n",
      "Neutrality Loss:   0.0078\n",
      "Source: Goodnight  saying random thoughts\n",
      "Generated: Goodnight  saying random thoughts  Good Good Good Good Good Good Good Good Good Good Good Good Good Good Good Good Good Good Good Good Good Good\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 7220\n",
      "Total Loss:        0.5875\n",
      "CE Loss:           0.2606\n",
      "Semantic Loss:     0.2180\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Immigrants want a better life yet abuse and cause destruction when they arrive. Fields full of rubbish and litter across Europe.\n",
      "Generated: Immigrants <NEU> a better life yet <NEU> and cause destruction when they arrive. Fields full of <NEU> and littering the\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 7240\n",
      "Total Loss:        0.4016\n",
      "CE Loss:           0.0513\n",
      "Semantic Loss:     0.2335\n",
      "Neutrality Loss:   0.0000\n",
      "Source: man! I'm an adult, I know that!\n",
      "Generated: man! I'm an adult, I <NEU> that! man man man man man   man man man man  man man man man man\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 7260\n",
      "Total Loss:        0.3238\n",
      "CE Loss:           0.0959\n",
      "Semantic Loss:     0.1519\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Well, since the flyers and Eagles seasons are over, who's excited for the sixers? gobirds butwhatthefuckwasthatplayon4thdown\n",
      "Generated: Well, since the flyers and Eagles seasons are over, who's <NEU> for the sixers? gobirds butwhatf\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 7280\n",
      "Total Loss:        0.6621\n",
      "CE Loss:           0.2960\n",
      "Semantic Loss:     0.2440\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Pro Tip: Same goes for my tweets.\n",
      "Generated: Pro Tip: Same goes for my tweets. Pro. Pro Pro Pro Pro  Pro Pro Pro Pro Pro Pro Pro Pro\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 7300\n",
      "Total Loss:        0.4541\n",
      "CE Loss:           0.1957\n",
      "Semantic Loss:     0.1722\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Work selfie and hair cut. latepost work workselfie hair haircut layers late mybad\n",
      "Generated: Work selfie and hair cut. latepost work workselfie hair haircut layers late mybad Work work...worki Work Work Work Work work Work\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 7320\n",
      "Total Loss:        0.8580\n",
      "CE Loss:           0.5151\n",
      "Semantic Loss:     0.2286\n",
      "Neutrality Loss:   0.0000\n",
      "Source: using the last precious %s of battery reading about the wind storm that caused me to have a few last %s of battery alanis\n",
      "Generated: using the last <NEU> %s of battery reading about the wind storm that caused me to have a few last %s of reading\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 7340\n",
      "Total Loss:        0.4395\n",
      "CE Loss:           0.2332\n",
      "Semantic Loss:     0.1375\n",
      "Neutrality Loss:   0.0000\n",
      "Source: india India steps up efforts to quell Nepal crisis: New Delhi: India on Tuesday summoned its ambass... politics\n",
      "Generated: india India steps up efforts to quell Nepal crisis: New Delhi: India on Tuesday summoned its ambass... politics  \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 7360\n",
      "Total Loss:        0.4984\n",
      "CE Loss:           0.2016\n",
      "Semantic Loss:     0.1979\n",
      "Neutrality Loss:   0.0000\n",
      "Source: thanks NFL for bringing back the commercial for the Vikings Steelers Eagles Bengals Cowboys family\n",
      "Generated: thanks NFL for bringing back the commercial for the Vikings Steelers Eagles Bengals Cowboys family thanks thanks thanks thanks thanks thanks thanks\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 7380\n",
      "Total Loss:        0.4125\n",
      "CE Loss:           0.1327\n",
      "Semantic Loss:     0.1848\n",
      "Neutrality Loss:   0.0083\n",
      "Source: Alarm goes off at 6a! I get up at 6:45a! See the problem? getyolife wakeup gotobed sleep night morning day early late shheeesshh\n",
      "Generated: Alarm goes off at 6a! I get up at 6:45a! See the problem? getyolife wakeup goto\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 7400\n",
      "Total Loss:        0.3810\n",
      "CE Loss:           0.1836\n",
      "Semantic Loss:     0.1316\n",
      "Neutrality Loss:   0.0000\n",
      "Source: franceengland trouble at white number two\n",
      "Generated: franceengland trouble at white number two fr fr fr fr fr fr fr fr  fr fr fr fr fr fr fr fr\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 7420\n",
      "Total Loss:        0.3943\n",
      "CE Loss:           0.1140\n",
      "Semantic Loss:     0.1868\n",
      "Neutrality Loss:   0.0000\n",
      "Source: crickey, I didn't know johny wilkinson won the World Cup!!!!! cheers sky\n",
      "Generated: crickey, I didn't <NEU> johny wilkinson won the World Cup!!!!! cheers sky   \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 7440\n",
      "Total Loss:        0.5453\n",
      "CE Loss:           0.1702\n",
      "Semantic Loss:     0.2501\n",
      "Neutrality Loss:   0.0000\n",
      "Source: I know. I feel so much better with these criminals of the street\n",
      "Generated: I <NEU> I <NEU> so much better with these criminals of the street I I I I I I I I I I I I I\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 7460\n",
      "Total Loss:        0.3155\n",
      "CE Loss:           0.0709\n",
      "Semantic Loss:     0.1631\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Yep, definitely misled. There's no way he would have knowingly killed such a magnificent beast.\n",
      "Generated: <NEU> definitely misled. There's no way he would have <NEU> killed such a <NEU> <NEU>!     \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 7480\n",
      "Total Loss:        0.5177\n",
      "CE Loss:           0.2024\n",
      "Semantic Loss:     0.2092\n",
      "Neutrality Loss:   0.0048\n",
      "Source: 5 killed in small plane crash in Colorado mountains news politics health entertainment\n",
      "Generated: 5 killed in small plane crash in Colorado mountains news politics health entertainment...   5 5 5 5 5 5 5  5 5  5 5\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 7500\n",
      "Total Loss:        0.2418\n",
      "CE Loss:           0.1148\n",
      "Semantic Loss:     0.0847\n",
      "Neutrality Loss:   0.0000\n",
      "Source: When everything's coming your way, you're in the wrong lane and going the wrong way.\n",
      "Generated: When everything's coming your way, you're in the wrong lane and going the wrong way. When When\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 7520\n",
      "Total Loss:        0.4181\n",
      "CE Loss:           0.3219\n",
      "Semantic Loss:     0.0641\n",
      "Neutrality Loss:   0.0000\n",
      "Source: No, Jeremy. This is a Christian nation. Protected by the almighty God. Smite all non-believers!!\n",
      "Generated: No, Jeremy. This is a Christian nation. Protected by the <NEU> God. Smite all non-believers\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 7540\n",
      "Total Loss:        0.4811\n",
      "CE Loss:           0.2600\n",
      "Semantic Loss:     0.1474\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Wow how's it feel to know your brother helped create ? Oh the cnndebate reiders\n",
      "Generated: <NEU> how's it <NEU> to <NEU> your brother helped create ? <NEU> the cnndebate <NEU>t\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 7560\n",
      "Total Loss:        0.5053\n",
      "CE Loss:           0.2901\n",
      "Semantic Loss:     0.1435\n",
      "Neutrality Loss:   0.0000\n",
      "Source: its so hot i poured soda into my Game of thrones mug and it cracked. It couldn't make it past Year one either.\n",
      "Generated: its so hot i poured soda into my Game of thrones mug and it cracked. It couldn't make it past\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 7580\n",
      "Total Loss:        0.6443\n",
      "CE Loss:           0.2978\n",
      "Semantic Loss:     0.2310\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Guy sitting at table next to me n restaurant told his group \"you can't drink all day if you don't start early\" brilliant beach\n",
      "Generated: Guy sitting at table next to me n restaurant told his group \"you can't drink all day if you don't start drinking\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 7600\n",
      "Total Loss:        0.6845\n",
      "CE Loss:           0.3436\n",
      "Semantic Loss:     0.2273\n",
      "Neutrality Loss:   0.0000\n",
      "Source: THE NET NEUTRALITY REPORT: Where we are now, what happens next, and why... New on theneeds politics\n",
      "Generated:  NET NEUTRALITY REPORT: Where we are now, what happens next, and why... New on theneeds\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 7620\n",
      "Total Loss:        0.6341\n",
      "CE Loss:           0.1318\n",
      "Semantic Loss:     0.3326\n",
      "Neutrality Loss:   0.0114\n",
      "Source: Thank you for creeping through my IG and commenting on my pictures now I'm all of a suddenly attracted to you  goaway\n",
      "Generated: <NEU> you for creeping through my IG and commenting on my pictures now I'm all of a suddenly attracted to your:\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 7640\n",
      "Total Loss:        0.3961\n",
      "CE Loss:           0.0876\n",
      "Semantic Loss:     0.2056\n",
      "Neutrality Loss:   0.0000\n",
      "Source: yea, that! Your fanforlife charmed bitches! the greatest gift in life. \n",
      "Generated: yea, that! Your fanforlife charmed bitches! the <NEU> gift in life.     \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 7660\n",
      "Total Loss:        0.3383\n",
      "CE Loss:           0.1313\n",
      "Semantic Loss:     0.1380\n",
      "Neutrality Loss:   0.0000\n",
      "Source: UPDATE: Larry Parrish returns to dugout. No hugging taking place. tigers\n",
      "Generated: UPDATE: Larry Parrish returns to dugout. No hugging taking place. tigers   U U   U\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 7680\n",
      "Total Loss:        0.3644\n",
      "CE Loss:           0.0081\n",
      "Semantic Loss:     0.2375\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Thx Scott!!!:) truth humor (but NOT about the truth and humor:)\n",
      "Generated: Thx Scott!!!:) <NEU> <NEU> (but NOT about the <NEU> and <NEU> Th Th Th Th Th  Th Th Th\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 7700\n",
      "Total Loss:        0.4378\n",
      "CE Loss:           0.1420\n",
      "Semantic Loss:     0.1973\n",
      "Neutrality Loss:   0.0000\n",
      "Source: 10 minutes before the start of fantasticfour in a dominican theater. A global success mcu\n",
      "Generated: 10 minutes before the start of fantasticfour in a dominican theater. A global success mcu  10 10 10 10\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 7720\n",
      "Total Loss:        0.6345\n",
      "CE Loss:           0.3313\n",
      "Semantic Loss:     0.2021\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Dang, it's so attractive when girls post pics of them out drunk every week.....\n",
      "Generated: Dang, it's so <NEU> when girls post pics of them out <NEU> every week..... ... Dan  Dan Dan   Dan\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 7740\n",
      "Total Loss:        1.1224\n",
      "CE Loss:           0.4409\n",
      "Semantic Loss:     0.4543\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Wow so good at puns\n",
      "Generated: <NEU> so good at punst knot   Tt<NEU> <NEU>mped joc<NEU><NEU><NEU><NEU>rine<NEU><NEU><NEU><NEU><NEU><NEU><NEU>\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 7760\n",
      "Total Loss:        0.5019\n",
      "CE Loss:           0.2706\n",
      "Semantic Loss:     0.1542\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Oh how the tides have turned. Seems like just a few months ago you were the self appointed chief of the Twitter police\n",
      "Generated: <NEU> how the tides have turned. Seems <NEU> just a few months ago you were the self appointed chief of the verse.\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 7780\n",
      "Total Loss:        0.5747\n",
      "CE Loss:           0.3334\n",
      "Semantic Loss:     0.1609\n",
      "Neutrality Loss:   0.0000\n",
      "Source: another Pillar of our generation,u wanna tell somebody to Fuck-off,then tell him to Fuck off.Dnt say g wat a great Jacket,Sweetness\n",
      "Generated: another <NEU> of our generation,u wanna tell somebody to Fuck-off,then tell him to Fuck off.Dao\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 7800\n",
      "Total Loss:        0.3450\n",
      "CE Loss:           0.1662\n",
      "Semantic Loss:     0.1192\n",
      "Neutrality Loss:   0.0000\n",
      "Source: T Party candidate 2016election copywriting proofreading randpaul spelling typo\n",
      "Generated: T Party candidate 2016election copywriting proofreading randpaul spelling typo T T T T T T T T T\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 7820\n",
      "Total Loss:        0.4544\n",
      "CE Loss:           0.1677\n",
      "Semantic Loss:     0.1751\n",
      "Neutrality Loss:   0.0804\n",
      "Source: I've improved so much\n",
      "Generated: I've improved so much I I I I I I I I I I I I I I I I I I I I I I I I\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 7840\n",
      "Total Loss:        0.4094\n",
      "CE Loss:           0.1249\n",
      "Semantic Loss:     0.1897\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Vatican asks Russia's Putin to lobby Israel's Netanyahu on equitable Arab Christian education funding edchat\n",
      "Generated: Vatican asks Russia's Putin to lobby Israel's Netanyahu on equitable Arab Christian education funding edchat \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 7860\n",
      "Total Loss:        0.3167\n",
      "CE Loss:           0.1640\n",
      "Semantic Loss:     0.1018\n",
      "Neutrality Loss:   0.0000\n",
      "Source: MET A GUY WEARING SOME AT DISNEYLAND\n",
      "Generated: MET A GUY WEARING SOME AT DISNEYLAND             \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 7880\n",
      "Total Loss:        0.3064\n",
      "CE Loss:           0.0718\n",
      "Semantic Loss:     0.1564\n",
      "Neutrality Loss:   0.0000\n",
      "Source: your opinion is a woman's only job is to reproduce. I disagree, which of course makes me sexist.\n",
      "Generated: your <NEU> is a woman's only job is to reproduce. I <NEU> which of course makes me sexist. your your your\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 7900\n",
      "Total Loss:        0.3426\n",
      "CE Loss:           0.0079\n",
      "Semantic Loss:     0.2134\n",
      "Neutrality Loss:   0.0486\n",
      "Source: Because he is the best in the league\n",
      "Generated: Because he is the <NEU> in the league Because Because Because Because Because Because Because Because Because Because Because Because Because Because Because Because Because Because\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 7920\n",
      "Total Loss:        0.2512\n",
      "CE Loss:           0.0120\n",
      "Semantic Loss:     0.1594\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Please give walterpalmer due process before you ship him to a prison in zimbabwe.\n",
      "Generated: <NEU> give walterpalmer due process before you ship him to a prison in zimbabwe. .. .\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 7940\n",
      "Total Loss:        0.6374\n",
      "CE Loss:           0.3696\n",
      "Semantic Loss:     0.1785\n",
      "Neutrality Loss:   0.0000\n",
      "Source: ah, the big city. I had hoped you would say MD &amp; I would meet your parents at some random Extension meeting\n",
      "Generated: ah, the big city. I had hoped you would say MD &amp; I would meet your parents at some random.\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 7960\n",
      "Total Loss:        0.5454\n",
      "CE Loss:           0.3163\n",
      "Semantic Loss:     0.1527\n",
      "Neutrality Loss:   0.0000\n",
      "Source: NO jail! sackthejudge - 'Mother' bought 12 packets of cocaine for her daughters 18th via drugs\n",
      "Generated: NO jail! sackthejudge - 'Mor' bought 12 packets of cocaine for her daughters 18th birthday \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 7980\n",
      "Total Loss:        0.5274\n",
      "CE Loss:           0.2327\n",
      "Semantic Loss:     0.1964\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Probably my most popular humor essay to date. \"The Intellectuary\" humor\n",
      "Generated: ProbabilProbably my most popular <NEU> essay to date. \"The Intellectuary\" <NEU>          \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 8000\n",
      "Total Loss:        0.5570\n",
      "CE Loss:           0.2285\n",
      "Semantic Loss:     0.2190\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Gordon Brown debt diversity &amp; massimmigration policies fail created corbyn4leader. Hes now trying to stop jezwecan\n",
      "Generated: Gordon Brown debt diversity &amp; massimmigration policies fail created corbyn4leader. Hes now trying to stop je\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 8020\n",
      "Total Loss:        0.7637\n",
      "CE Loss:           0.3580\n",
      "Semantic Loss:     0.2705\n",
      "Neutrality Loss:   0.0000\n",
      "Source: I can barely stand to watch the usopen right now. All they're doing is kicking Kyrgios in the nuts. McKenroe just sounds so old.\n",
      "Generated: I can barely stand to watch the usopen <NEU> now. All they're doing is kicking Kyrgios in the nutsnn\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 8040\n",
      "Total Loss:        0.3010\n",
      "CE Loss:           0.0684\n",
      "Semantic Loss:     0.1550\n",
      "Neutrality Loss:   0.0000\n",
      "Source: DYK: that if Chickens weren't a primary source of food for humans they would overpopulate &amp; starve to death?\n",
      "Generated: DYK: that if Chickens weren't a primary source of food for humans they would overpopulate &amp; ve\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 8060\n",
      "Total Loss:        0.3461\n",
      "CE Loss:           0.1663\n",
      "Semantic Loss:     0.1199\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Thanks for finally opening your doors... It's not like this line of people have anything to do today. late badservice\n",
      "Generated: Thanks for <NEU> opening your doors... It's not <NEU> this line of people have anything to do today. late badservice. thanks thanks\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 8080\n",
      "Total Loss:        0.4182\n",
      "CE Loss:           0.1199\n",
      "Semantic Loss:     0.1989\n",
      "Neutrality Loss:   0.0000\n",
      "Source: enough twitter twattering for the night/morning, going to get ready to crash before my fun filled days at work\n",
      "Generated: enough twitter twattering for the night/morning, going to get ready to crash before my <NEU> filled days at work.\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 8100\n",
      "Total Loss:        0.3210\n",
      "CE Loss:           0.0669\n",
      "Semantic Loss:     0.1695\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Too tired to get ready for bed\n",
      "Generated: Too <NEU> to get ready for bed   Too Too Too Too Too Too Too too too Too Too Too  Too Too Too Too Too Too\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 8120\n",
      "Total Loss:        0.4911\n",
      "CE Loss:           0.1581\n",
      "Semantic Loss:     0.2220\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Did I just see two commercials for wicked during the commercial break of the cnndebate? gopdebate\n",
      "Generated: Did I just see two commercials for <NEU> during the commercial break of the cnndebate? gopdebate Did\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 8140\n",
      "Total Loss:        0.3263\n",
      "CE Loss:           0.0337\n",
      "Semantic Loss:     0.1951\n",
      "Neutrality Loss:   0.0000\n",
      "Source: it's ironic similar speech hate speech was used to condone the murder of black Americans .\n",
      "Generated: it's <NEU> similar speech <NEU> speech was used to <NEU> the murder of black Americans .  it it it it it it it\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 8160\n",
      "Total Loss:        0.6181\n",
      "CE Loss:           0.1886\n",
      "Semantic Loss:     0.2719\n",
      "Neutrality Loss:   0.0721\n",
      "Source: I dont know if tunns tweet should have had\n",
      "Generated: I dont <NEU> if tunns tweet should have had I I I I I I I I I I I I I I I I\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 8180\n",
      "Total Loss:        0.4438\n",
      "CE Loss:           0.2255\n",
      "Semantic Loss:     0.1456\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Brilliant cheers from the England Supporters for 50 up! ashes2015\n",
      "Generated: <NEU> cheers from the England Supporters for 50 up! ashes2015     ...).s ...)....).<NEU><NEU><NEU>...).\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 8200\n",
      "Total Loss:        0.4493\n",
      "CE Loss:           0.0128\n",
      "Semantic Loss:     0.2910\n",
      "Neutrality Loss:   0.0000\n",
      "Source: with a photo from Heat magazine\n",
      "Generated: with a photo from Heat magazine with with with with with with with with with with with and with with with and from with with with with\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 8220\n",
      "Total Loss:        0.4412\n",
      "CE Loss:           0.1455\n",
      "Semantic Loss:     0.1846\n",
      "Neutrality Loss:   0.0625\n",
      "Source: Hugs Poster Print sheffieldissuper barnsleyisbrill rotherhamiswonderful\n",
      "Generated: Hugs Poster Print sheffieldissuper barnsleyisbrill rotherhamiswonderful Hug Hug Hug Hug Hug Hug\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 8240\n",
      "Total Loss:        0.3655\n",
      "CE Loss:           0.1232\n",
      "Semantic Loss:     0.1433\n",
      "Neutrality Loss:   0.0909\n",
      "Source: might be the lowest form of wit but actually makes people brighter and more creative\n",
      "Generated: <NEU> be the lowest form of wit but <NEU> makes people brighter and more <NEU>t?! strong   stronger t<NEU> <NEU>\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 8260\n",
      "Total Loss:        0.3790\n",
      "CE Loss:           0.1521\n",
      "Semantic Loss:     0.1513\n",
      "Neutrality Loss:   0.0000\n",
      "Source: \"Anna\" tweets all this idiocy, with a Confederate Flag avi.\n",
      "Generated: \"Anna\" tweets all this <NEU> with a Confederate Flag avi.. \" \" \" \" \" \" \" \" \"\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 8280\n",
      "Total Loss:        0.5931\n",
      "CE Loss:           0.2940\n",
      "Semantic Loss:     0.1994\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Way to play tonight . Another great game of productive offense and solid pitching\n",
      "Generated: Way to play tonight . Another strong game of productive offense and solid pitching!  Way Way Way Way Way Way Way Way Way Way Way\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 8300\n",
      "Total Loss:        0.3549\n",
      "CE Loss:           0.0805\n",
      "Semantic Loss:     0.1829\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Watched the GOP debate on Bernie Sanders live feed.\n",
      "Generated: Watched the GOP <NEU> on Bernie Sanders live feed.   Watch Watch Watch Watch   Watch Watch Watch Watch Watch Watch Watch Watch\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 8320\n",
      "Total Loss:        0.2253\n",
      "CE Loss:           0.0074\n",
      "Semantic Loss:     0.1453\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Probably in the top 25 imo Deano. Prodigious at times but rarely popular\n",
      "Generated: Probably in the top 25 imo Deano. <NEU> at times but rarely popular            \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 8340\n",
      "Total Loss:        0.3704\n",
      "CE Loss:           0.1710\n",
      "Semantic Loss:     0.1318\n",
      "Neutrality Loss:   0.0054\n",
      "Source: The whitest girls are behind me in line at target\n",
      "Generated: The whitest girls are behind me in line at target  The the the The The The The The The The The The The The The The\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 8360\n",
      "Total Loss:        0.5166\n",
      "CE Loss:           0.3137\n",
      "Semantic Loss:     0.1352\n",
      "Neutrality Loss:   0.0000\n",
      "Source: I love that your blog post about crazy excessive commenting has 46 comments. =D butletmejustadd ...\n",
      "Generated: I <NEU> that your blog post about <NEU> <NEU> commenting has 46 comments. =D butletmejustadd ...  I \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 8380\n",
      "Total Loss:        0.4528\n",
      "CE Loss:           0.1373\n",
      "Semantic Loss:     0.2103\n",
      "Neutrality Loss:   0.0000\n",
      "Source: the awkward moment when \"a NON interference rally\" is hoisting BLF flags\n",
      "Generated: the <NEU> moment when \"a NON interference <NEU> is hoisting BLF flags the the the the the the the the the the\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 8400\n",
      "Total Loss:        0.7168\n",
      "CE Loss:           0.4404\n",
      "Semantic Loss:     0.1843\n",
      "Neutrality Loss:   0.0000\n",
      "Source: \"BUT Transitioning away from tarsands means less jobs!\" Because a job, ANY job is better than a sustainable future?\n",
      "Generated: \"BUT Transitioning away from tarsands means less jobs!\" Because a job, ANY job is better than a job job\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 8420\n",
      "Total Loss:        0.3759\n",
      "CE Loss:           0.1183\n",
      "Semantic Loss:     0.1717\n",
      "Neutrality Loss:   0.0000\n",
      "Source: MT : A miracle drug joins the fight against overdoses. reports harmreduction drugs health\n",
      "Generated: MT : A <NEU> drug joins the fight against overdoses. reports harmreduction drugs health       \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 8440\n",
      "Total Loss:        0.5148\n",
      "CE Loss:           0.2084\n",
      "Semantic Loss:     0.2043\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Grind mode engaged thanks to some help I'm back on my dome godneverleftme i won't kill you manolo shoot that piece of ****\n",
      "Generated: <NEU> mode engaged thanks to some help I'm back on my dome godneverleftme i won't kill you mans\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 8460\n",
      "Total Loss:        0.6640\n",
      "CE Loss:           0.2809\n",
      "Semantic Loss:     0.2554\n",
      "Neutrality Loss:   0.0000\n",
      "Source: It looks to be one of those days where it is free flowing......\n",
      "Generated: It looks to be one of those days where it is free flowing...... It  It It It It It It It It It It It It It\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 8480\n",
      "Total Loss:        0.6538\n",
      "CE Loss:           0.1550\n",
      "Semantic Loss:     0.3325\n",
      "Neutrality Loss:   0.0000\n",
      "Source: I feel like I've gone to more country music things at my job than anything else. kipmoore\n",
      "Generated: I <NEU> <NEU> I've gone to more country music things at my job than anything else. kipmoore    \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 8500\n",
      "Total Loss:        0.5964\n",
      "CE Loss:           0.1328\n",
      "Semantic Loss:     0.3090\n",
      "Neutrality Loss:   0.0000\n",
      "Source: A bank is a place that will lend you money if you can prove that you don't need it.\n",
      "Generated: A bank is a place that <NEU> lend you money if you can prove that you don't <NEU> it... A\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 8520\n",
      "Total Loss:        0.3890\n",
      "CE Loss:           0.0182\n",
      "Semantic Loss:     0.2472\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Finally got the only car had in stock which is not the one i reserved and its the cleanest car its filthy disappointed\n",
      "Generated: <NEU> got the only car had in stock which is not the one i reserved and its the cleanest car its <NEU> <NEU>  !\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 8540\n",
      "Total Loss:        0.5148\n",
      "CE Loss:           0.1357\n",
      "Semantic Loss:     0.2528\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Omg One Direction is taking a break! What will I do? whocares\n",
      "Generated: Omg 1 Direction is taking a break! What <NEU> I do? whocares  Om Om Om Om Om Om Om Om Om Om\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 8560\n",
      "Total Loss:        0.5076\n",
      "CE Loss:           0.2536\n",
      "Semantic Loss:     0.1693\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Freeman assists Ryan, Jones as Falcons top Cowboys 39-28: Fill-in Freeman combines with Ryan, Jones... nfl news\n",
      "Generated: Freeman assists Ryan, Jones as Falcons top Cowboys 39-28: Fill-in Freeman combines with Ryan, Jones\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 8580\n",
      "Total Loss:        0.5333\n",
      "CE Loss:           0.3572\n",
      "Semantic Loss:     0.1174\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Some guys are so intelligent that they've discovered the reason they need plenty money is to attrack women! rocks mhen!! *winks*\n",
      "Generated: Some guys are so <NEU> that they've discovered the reason they <NEU> <NEU> money is to attrack women! rocks mc\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 8600\n",
      "Total Loss:        0.5711\n",
      "CE Loss:           0.3528\n",
      "Semantic Loss:     0.1456\n",
      "Neutrality Loss:   0.0000\n",
      "Source: great that he's got time to mince around with old Commies... While the rest of us work hard...\n",
      "Generated: <NEU> that he's got time to mince around with old Commies... While the rest of us work hard...rioss  \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 8620\n",
      "Total Loss:        0.2720\n",
      "CE Loss:           0.0225\n",
      "Semantic Loss:     0.1663\n",
      "Neutrality Loss:   0.0000\n",
      "Source: as a boss: The disappointing truth iowa gopdebate conservative iowa business student\n",
      "Generated: as a boss: The <NEU> <NEU> iowa gopdebate conservative iowa business student  as as as\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 8640\n",
      "Total Loss:        0.3176\n",
      "CE Loss:           0.1511\n",
      "Semantic Loss:     0.1110\n",
      "Neutrality Loss:   0.0000\n",
      "Source:  peace symbol guitar pick earrings original\n",
      "Generated:  peace symbol guitar pick earrings original                      \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 8660\n",
      "Total Loss:        0.3609\n",
      "CE Loss:           0.1317\n",
      "Semantic Loss:     0.1528\n",
      "Neutrality Loss:   0.0000\n",
      "Source: the one shown is a Fox, when I checked there isn't a fox opener one falseadvertising at its finest lol\n",
      "Generated: the one shown is a Fox, when I checked there isn't a fox opener one falseadvertising at\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 8680\n",
      "Total Loss:        0.7373\n",
      "CE Loss:           0.2958\n",
      "Semantic Loss:     0.2924\n",
      "Neutrality Loss:   0.0096\n",
      "Source: yet here you are tweeting about the science you chose to deny. evolution\n",
      "Generated: yet here you are tweeting about the science you chose to <NEU> evolution but yet yet yet yet yet yet yet however yet yet yet yet yet\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 8700\n",
      "Total Loss:        0.5373\n",
      "CE Loss:           0.2872\n",
      "Semantic Loss:     0.1667\n",
      "Neutrality Loss:   0.0000\n",
      "Source: What is \"within walking distance\"? Walking until you collapse? question whatthebosssays papalvisit soconvenient cantwait\n",
      "Generated: What is \"within walking distance\"? Walking until you collapse? question whatthebosssays papalvisit sooe\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 8720\n",
      "Total Loss:        0.6750\n",
      "CE Loss:           0.3575\n",
      "Semantic Loss:     0.2117\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Being line judge is just my favorite\n",
      "Generated: Being line judge is just my <NEU> being Being being  Being being Being Being Being Being Being Being Being Being  being Being Being  Being Being being\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 8740\n",
      "Total Loss:        0.3612\n",
      "CE Loss:           0.1704\n",
      "Semantic Loss:     0.1272\n",
      "Neutrality Loss:   0.0000\n",
      "Source: when do the billboards go up? arsenal cfc\n",
      "Generated: when do the billboards go up? arsenal cfc  when when when when when when when when when when when when when when when\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 8760\n",
      "Total Loss:        0.6996\n",
      "CE Loss:           0.4231\n",
      "Semantic Loss:     0.1843\n",
      "Neutrality Loss:   0.0000\n",
      "Source: I've been on the website for THREE minutes and I've already had THREE pop ups asking if I need help! GO AWAY! nothelpful\n",
      "Generated: I've been on the website for THREE minutes and I've already had THREE pop ups asking if I'<NEU>\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 8780\n",
      "Total Loss:        0.1331\n",
      "CE Loss:           0.0117\n",
      "Semantic Loss:     0.0809\n",
      "Neutrality Loss:   0.0000\n",
      "Source:  psychedelic all over print shirt s6 society6 psychedelic blue pink peace fashion\n",
      "Generated:  psychedelic all over print shirt s6 society6 psychedelic blue pink peace fashion       \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 8800\n",
      "Total Loss:        0.4629\n",
      "CE Loss:           0.1303\n",
      "Semantic Loss:     0.2218\n",
      "Neutrality Loss:   0.0000\n",
      "Source: I'm sooo excited to see everyone at school tomorrow\n",
      "Generated: I'm sooo <NEU> to see everyone at school tomorrow  I I I I I I I I I I I I I I\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 8820\n",
      "Total Loss:        0.7730\n",
      "CE Loss:           0.2821\n",
      "Semantic Loss:     0.3272\n",
      "Neutrality Loss:   0.0000\n",
      "Source: I wish you all the help in the universe! love peace hope\n",
      "Generated: I <NEU> you all the help in the universe! <NEU> peace <NEU>   I I  I I I I I I I I I\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 8840\n",
      "Total Loss:        0.7113\n",
      "CE Loss:           0.3679\n",
      "Semantic Loss:     0.2289\n",
      "Neutrality Loss:   0.0000\n",
      "Source: And while, (still) dont have their own womens team despite their most successful kit.\n",
      "Generated: And while, (still) dont have their own womens team <NEU> their most successful kit.. and And And And And And And And And\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 8860\n",
      "Total Loss:        0.6773\n",
      "CE Loss:           0.2306\n",
      "Semantic Loss:     0.2978\n",
      "Neutrality Loss:   0.0000\n",
      "Source: myscarymoviedeath I'll be pushed into a GiantMeatGrinder &amp; end up at TacoBell &amp; Eaten at a Church FellowshipMeeting as Credits Roll.\n",
      "Generated: myscarymoviedeath I'll be pushed into a GiantMeatGrinder &amp; of being\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 8880\n",
      "Total Loss:        0.1705\n",
      "CE Loss:           0.0146\n",
      "Semantic Loss:     0.1039\n",
      "Neutrality Loss:   0.0000\n",
      "Source: That man will do anything for my EXCEPT the one thing I ask\n",
      "Generated: That man <NEU> do anything for my EXCEPT the one thing I ask That That that That That That that That That That That That That That\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 8900\n",
      "Total Loss:        0.7381\n",
      "CE Loss:           0.4769\n",
      "Semantic Loss:     0.1742\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Search engine seo education inspirational lol\n",
      "Generated: Search engine seo education <NEU> lol Search Search Search Search Search Search Search Search World Search Search Search Search Search Search Search Search Search Search\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 8920\n",
      "Total Loss:        0.5211\n",
      "CE Loss:           0.2621\n",
      "Semantic Loss:     0.1726\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Daily war on drugs is out! Stories via\n",
      "Generated: Daily war on drugs is out! Stories via Daily Daily Daily Daily Daily Daily Daily Daily Daily Daily Daily Daily Daily Daily Daily Daily Daily Daily Daily Daily Daily\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 8940\n",
      "Total Loss:        0.3660\n",
      "CE Loss:           0.0860\n",
      "Semantic Loss:     0.1867\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Only in America could donaldtrump be considered anti-establishment\n",
      "Generated: Only in America could donaldtrump be considered anti-establishment Only Only Only Only Only Only Only Only\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 8960\n",
      "Total Loss:        0.8530\n",
      "CE Loss:           0.5324\n",
      "Semantic Loss:     0.2137\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Beware Inadequate Electricians! humor\n",
      "Generated: Beware Inadequate Electricians! <NEU> Be Be Be Be Be Be Be Be Be Be Be Be Be Be Be Be Be Be\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 8980\n",
      "Total Loss:        0.5244\n",
      "CE Loss:           0.2377\n",
      "Semantic Loss:     0.1911\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Prediction: on the skins bye week Jay is fired Callahan takes over as coach RG is named the starter and cousins is released karma\n",
      "Generated: Prediction: on the skins bye week Jay is fired Callahan takes over as coach RG is named the starter and cousin\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 9000\n",
      "Total Loss:        0.6648\n",
      "CE Loss:           0.3466\n",
      "Semantic Loss:     0.2121\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Doritos Rainbows | It Gets Better Project Indie Brew news via\n",
      "Generated: Doritos Rainbows | It Gets Better Project Indie Brew news via Dor Dor Dor  Dor Dor Dor Dor Dor\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 9020\n",
      "Total Loss:        0.7646\n",
      "CE Loss:           0.5153\n",
      "Semantic Loss:     0.1649\n",
      "Neutrality Loss:   0.0066\n",
      "Source: Just saw ss16 totally dig the show, loved the crochet tops maxi dresses &amp; all americana take on Jamaican prints nyfw peace\n",
      "Generated: Just saw ss16 totally dig the show, loved the crochet tops maxi dresses &amp; all americana take on\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 9040\n",
      "Total Loss:        0.8655\n",
      "CE Loss:           0.2714\n",
      "Semantic Loss:     0.3960\n",
      "Neutrality Loss:   0.0000\n",
      "Source: I mean, really, I'm just so happy that we have Rios in right field and not Orlando\n",
      "Generated: I mean, <NEU> I'm just so <NEU> that we have Rios in <NEU> field and not Orlando. I I I I I\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 9060\n",
      "Total Loss:        0.6955\n",
      "CE Loss:           0.3844\n",
      "Semantic Loss:     0.2074\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Indeed especially since mckinley (the prez) never visited alaska the smh reiders denali\n",
      "Generated: <NEU> <NEU> since mckinley (the prez) never visited alaska the smh rei\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 9080\n",
      "Total Loss:        0.4032\n",
      "CE Loss:           0.1072\n",
      "Semantic Loss:     0.1973\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Taking photos at a number of different points in your life has really blown the lid off your BANKRUPT AGENDA!!1\n",
      "Generated: Taking photos at a number of different points in your life has <NEU> blown the lid off your BANKRUPT A\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 9100\n",
      "Total Loss:        0.4563\n",
      "CE Loss:           0.1984\n",
      "Semantic Loss:     0.1719\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Autocorrect just corrected Autocorrect to Atrocities\n",
      "Generated: Autocorrect just corrected Autocorrect to <NEU> Auto Auto Auto Auto Auto Auto  Auto Auto Auto Auto Auto Auto Auto Auto Auto Auto Auto Auto\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 9120\n",
      "Total Loss:        0.6241\n",
      "CE Loss:           0.3754\n",
      "Semantic Loss:     0.1641\n",
      "Neutrality Loss:   0.0083\n",
      "Source: Think Mclaren didn't think this picture out very well when posting about Alonso &amp; Button's poor qualifying...\n",
      "Generated: <NEU> Mclaren didn't <NEU> this picture out very well when posting about Alonso &amp; Button's \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 9140\n",
      "Total Loss:        0.5059\n",
      "CE Loss:           0.2022\n",
      "Semantic Loss:     0.2025\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Classic thanks DWP for helping the deserving and vulnerable. Your truly hero's of mine\n",
      "Generated: Classic thanks DWP for helping the <NEU> and <NEU> Your <NEU> hero's of mine.  classic Classic Classic Classic classic Classic\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 9160\n",
      "Total Loss:        0.3968\n",
      "CE Loss:           0.1685\n",
      "Semantic Loss:     0.1523\n",
      "Neutrality Loss:   0.0000\n",
      "Source: When I read the Sydney Morning Herald I also end up shaking my head doubleentendresman\n",
      "Generated: When I read the Sydney Morning <NEU> I also end up shaking my head doubleentendresman When When When\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 9180\n",
      "Total Loss:        0.4679\n",
      "CE Loss:           0.2876\n",
      "Semantic Loss:     0.1202\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Suddenly the POLICE are a necessary ASSET in his life? LMAO\n",
      "Generated: Suddenly the POLICE are a necessary ASSET in his life? LMAO           \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 9200\n",
      "Total Loss:        0.5272\n",
      "CE Loss:           0.2786\n",
      "Semantic Loss:     0.1657\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Dead Dog Down  Accidentally Hit A Dog. insurancedoesntcoverthat  scratchedmybenz humor\n",
      "Generated: Dead Dog Down  Accidentally Hit A Dog. insurancedoesntcoverthat  scratchedmybenz \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 9220\n",
      "Total Loss:        0.3787\n",
      "CE Loss:           0.0704\n",
      "Semantic Loss:     0.2055\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Trying to figure out how to get to AMS for Sail Amsterdam. Ticket prices are ONLY $1500\n",
      "Generated: Trying to figure out how to get to AMS for Sail Amsterdam. Ticket prices are ONLY $1500.      \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 9240\n",
      "Total Loss:        0.5781\n",
      "CE Loss:           0.3705\n",
      "Semantic Loss:     0.1350\n",
      "Neutrality Loss:   0.0167\n",
      "Source: A triple body blow to hillaryemail excuses leadership judgment foia statedept fbi cybersecurity 2016\n",
      "Generated: A triple body blow to hillaryemail excuses leadership judgment foia statedept fbi cybersecurity 2016     A A A\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 9260\n",
      "Total Loss:        0.2920\n",
      "CE Loss:           0.0155\n",
      "Semantic Loss:     0.1843\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Just remember, people not believing the things you believe INFRINGES ON YOUR RIGHTS!\n",
      "Generated: Just remember, people not believing the things you <NEU> INFRINGES ON YOUR RIGHTS!  Just Just  Just  Just\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 9280\n",
      "Total Loss:        0.6671\n",
      "CE Loss:           0.3514\n",
      "Semantic Loss:     0.2105\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Human Wastes Time Installing Cat Door For Genius Door-Opening Cat lol fun humor\n",
      "Generated: Human Wastes Time Installing Cat Door For <NEU> Door-Opening Cat lol <NEU> <NEU> Human Human Human Human  Human Human Human\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 9300\n",
      "Total Loss:        0.6657\n",
      "CE Loss:           0.2824\n",
      "Semantic Loss:     0.2556\n",
      "Neutrality Loss:   0.0000\n",
      "Source: most elite closers average over 20 pitches in the 9th.\n",
      "Generated: most elite closers average over 20 pitches in the 9th. most most most most most most 9 most most most most most \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 9320\n",
      "Total Loss:        0.5556\n",
      "CE Loss:           0.2792\n",
      "Semantic Loss:     0.1842\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Finland is deep in shit. If you could follow fin soc med flow. Typical fin mentality, shouting, no solutions fin internaldev politics\n",
      "Generated: Finland is deep in shit. If you could follow fin soc med flow. Typical fin <NEU> shouting\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 9340\n",
      "Total Loss:        0.4951\n",
      "CE Loss:           0.3030\n",
      "Semantic Loss:     0.1267\n",
      "Neutrality Loss:   0.0066\n",
      "Source: Bilingual services audit tackles long-standing problem education chicago\n",
      "Generated: Bilingual services audit tackles long-standing problem education chicago Bi Bi Bi Bi Bi Bi Bi Bi Bi Bi Bi Bi Bi\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 9360\n",
      "Total Loss:        0.2251\n",
      "CE Loss:           0.1225\n",
      "Semantic Loss:     0.0684\n",
      "Neutrality Loss:   0.0000\n",
      "Source: 2 out of 5 do not have optimized mobile landing pages. That's in a room of digital marketers. wakeuppeople twitterforb2b\n",
      "Generated: 2 out of 5 do not have optimized mobile landing pages. That's in a room of digital marketers. wakeuppeople twitterfor\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 9380\n",
      "Total Loss:        0.5414\n",
      "CE Loss:           0.2923\n",
      "Semantic Loss:     0.1661\n",
      "Neutrality Loss:   0.0000\n",
      "Source: The about Today Win In hungariangp is vettel Winning with ferrari With redbulls On Both Side!! karma I Guess!! \n",
      "Generated: The about Today Win In hungariangp is vettel Winning with ferrari With redbulls On Both Side!! <NEU>\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 9400\n",
      "Total Loss:        0.7213\n",
      "CE Loss:           0.3207\n",
      "Semantic Loss:     0.2670\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Work oh the joy\n",
      "Generated: Work <NEU> the <NEU>  Work Work Work Work Work Work  Work Work Work Work Work Work Work Work Work Work Work Work Work Work Work\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 9420\n",
      "Total Loss:        0.6013\n",
      "CE Loss:           0.3421\n",
      "Semantic Loss:     0.1728\n",
      "Neutrality Loss:   0.0000\n",
      "Source: I guess Robin was right for starting Avi.\n",
      "Generated: I guess Robin was <NEU> for starting Avi. I.. I I I I I I I I I I I I I I I I I\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 9440\n",
      "Total Loss:        0.4485\n",
      "CE Loss:           0.2909\n",
      "Semantic Loss:     0.1051\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Has Indian media found a Pak link yet oe not? Thai bomb suspect spoke foreign language,not English: bangkokblast\n",
      "Generated: Has Indian media found a Pak link yet oe not? Thai bomb suspect spoke foreign language,not English: bangk\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 9460\n",
      "Total Loss:        0.6093\n",
      "CE Loss:           0.2683\n",
      "Semantic Loss:     0.2254\n",
      "Neutrality Loss:   0.0096\n",
      "Source: obama on vacation has an element of associated with it. Same applies to his serving as President.\n",
      "Generated: obama on vacation has an element of associated with it. Same applies to his serving as President..    \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 9480\n",
      "Total Loss:        0.5111\n",
      "CE Loss:           0.2615\n",
      "Semantic Loss:     0.1664\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Distant neighbor to soberlivinghome: \"Are the residents [in the home] allowed to wander the neighborhood?\" while taking a walk.\n",
      "Generated: Distant neighbor to soberlivinghome: \"Are the residents [in the home] allowed to wander the neighborhood?\" while taking a break\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 9500\n",
      "Total Loss:        0.5587\n",
      "CE Loss:           0.2581\n",
      "Semantic Loss:     0.2004\n",
      "Neutrality Loss:   0.0000\n",
      "Source: . are an absolute joke! nosenseoftime late\n",
      "Generated: . are an <NEU> <NEU> nosenseoftime late                 \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 9520\n",
      "Total Loss:        0.4713\n",
      "CE Loss:           0.0390\n",
      "Semantic Loss:     0.2881\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Photo: Guess who ? shout quote killythirsk indonesia unpopular conscious mind wisdom...\n",
      "Generated: Photo: Guess who ? shout quote killythirsk indonesia <NEU> conscious mind wi<NEU>   Photo Photo\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 9540\n",
      "Total Loss:        0.7236\n",
      "CE Loss:           0.3424\n",
      "Semantic Loss:     0.2527\n",
      "Neutrality Loss:   0.0069\n",
      "Source: Love waiting on ppl its awesome!!! Just so awesome!!!!!\n",
      "Generated: <NEU> waiting on ppl its  Just so  ...t?!<NEU> <NEU><NEU><NEU>essor<NEU> (2012)<NEU><NEU><NEU><NEU> sigur\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 9560\n",
      "Total Loss:        0.3399\n",
      "CE Loss:           0.1663\n",
      "Semantic Loss:     0.1157\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Term Limits, a Wall and a fairtax will Fix Most of What Ails DC gop gopdebate ccot tcot teaparty imwithhuck\n",
      "Generated: Term Limits, a Wall and a fairtax <NEU> Fix Most of What Ails DC gop gopdebate\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 9580\n",
      "Total Loss:        0.4352\n",
      "CE Loss:           0.1221\n",
      "Semantic Loss:     0.2087\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Did you turn it off and then back on???\n",
      "Generated: Did you turn it off and then back on???  Did Did Did Did Did Did Did Did Did Did Did Did\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 9600\n",
      "Total Loss:        0.4689\n",
      "CE Loss:           0.1726\n",
      "Semantic Loss:     0.1975\n",
      "Neutrality Loss:   0.0000\n",
      "Source: oh, don't worry, it'll get better once you're done with school . . .\n",
      "Generated: <NEU> don't <NEU> it'll get better once you're done with school . . . .    \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 9620\n",
      "Total Loss:        0.4535\n",
      "CE Loss:           0.2856\n",
      "Semantic Loss:     0.1119\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Funny Pictures of the day 205 (36 pict) funnypictures funnytweets humor ...\n",
      "Generated: <NEU> Pictures of the day 205 (36 pict) funnypictures funnytweets <NEU> ...t! t...\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 9640\n",
      "Total Loss:        0.3520\n",
      "CE Loss:           0.1835\n",
      "Semantic Loss:     0.1123\n",
      "Neutrality Loss:   0.0000\n",
      "Source: It is off to work I go. Have a good night all peace light and love\n",
      "Generated: It is off to work I go. Have a good night all peace light and <NEU> it   It It It It but It It It\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 9660\n",
      "Total Loss:        0.6789\n",
      "CE Loss:           0.3528\n",
      "Semantic Loss:     0.2139\n",
      "Neutrality Loss:   0.0174\n",
      "Source: Today is just going perfect. exactopposite\n",
      "Generated: Today is just going <NEU> exactopposite Today Today Today Today Todayto today Today Today Today Today Today Today Today Today Today Today Today Today Today Today\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 9680\n",
      "Total Loss:        0.7163\n",
      "CE Loss:           0.4304\n",
      "Semantic Loss:     0.1906\n",
      "Neutrality Loss:   0.0000\n",
      "Source: I just love having low blood sugar &amp; low blood pressure\n",
      "Generated: I just <NEU> having low blood sugar &amp; low blood pressure I I I I I I I I I I I I I I\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 9700\n",
      "Total Loss:        0.5935\n",
      "CE Loss:           0.1648\n",
      "Semantic Loss:     0.2858\n",
      "Neutrality Loss:   0.0000\n",
      "Source: oh look at that... the pot calling the kettle black...\n",
      "Generated: rare look at that... the pot calling the kettle black...  random random ... ...).ugh purple<NEU><NEU><NEU><NEU>233 random \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 9720\n",
      "Total Loss:        0.6472\n",
      "CE Loss:           0.3504\n",
      "Semantic Loss:     0.1978\n",
      "Neutrality Loss:   0.0000\n",
      "Source: good call by Sawx not signing him. Couldn't help them at all.\n",
      "Generated: good call by Sawx not signing him. Couldn't help them at all. . but good good. good good good\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 9740\n",
      "Total Loss:        0.3471\n",
      "CE Loss:           0.0127\n",
      "Semantic Loss:     0.2229\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Maher leads off asking everybody why they overcover Trump.\n",
      "Generated: Maher leads off asking everybody why they overcover <NEU> Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 9760\n",
      "Total Loss:        0.6713\n",
      "CE Loss:           0.3415\n",
      "Semantic Loss:     0.2199\n",
      "Neutrality Loss:   0.0000\n",
      "Source: \"We believe good customer service is answering the phone quickly\" - customer service hold message, 10 minutes in.\n",
      "Generated: \"We <NEU> good customer service is answering the phone quickly\" - customer service hold message, 10 minutes in.m \". \" \" \" \"\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 9780\n",
      "Total Loss:        0.6880\n",
      "CE Loss:           0.2909\n",
      "Semantic Loss:     0.2636\n",
      "Neutrality Loss:   0.0057\n",
      "Source: damn that was dank ..\n",
      "Generated: <NEU> that was dank .. .<NEU><NEU><NEU><NEU>TER sigur <NEU><NEU><NEU><NEU><NEU><NEU> faimth<NEU><NEU><NEU><NEU>,\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 9800\n",
      "Total Loss:        0.3526\n",
      "CE Loss:           0.1386\n",
      "Semantic Loss:     0.1427\n",
      "Neutrality Loss:   0.0000\n",
      "Source: \"Non profit\" doesn't mean they don't make money the NFL is a non profit\n",
      "Generated: \"Non profit\" doesn't mean they don't make money the NFL is a non profit.... \" \" \"\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 9820\n",
      "Total Loss:        0.6134\n",
      "CE Loss:           0.3365\n",
      "Semantic Loss:     0.1846\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Wow. Now we have a game. athletics giants\n",
      "Generated: <NEU> Now we have a game. athletics giants!?!?! <NEU>  attacker<NEU>...<NEU>s<NEU><NEU>s strong 2012.<NEU>\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 9840\n",
      "Total Loss:        0.5860\n",
      "CE Loss:           0.2898\n",
      "Semantic Loss:     0.1975\n",
      "Neutrality Loss:   0.0000\n",
      "Source: The irony of leaving a book entitled 'let's get lost' behind dontyouthink lost\n",
      "Generated: The <NEU> of leaving a book entitled 'let's get lost' behind dontyouthink lost The The The The The The The\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 9860\n",
      "Total Loss:        0.4147\n",
      "CE Loss:           0.1690\n",
      "Semantic Loss:     0.1627\n",
      "Neutrality Loss:   0.0057\n",
      "Source: Cheerios are just fruit loops on acid . cereal drugs\n",
      "Generated: Cheerios are just fruit loops on acid . cereal drugse  Che Che Che Che Che Che Che Che Che Che Che Che Che\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 9880\n",
      "Total Loss:        0.5836\n",
      "CE Loss:           0.1941\n",
      "Semantic Loss:     0.2596\n",
      "Neutrality Loss:   0.0000\n",
      "Source: The South Park episode with Jared Fogle is on.\n",
      "Generated: The South Park episode with Jared Fogle is on.. The. The The The The The The. The The The The The The The\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 9900\n",
      "Total Loss:        0.4084\n",
      "CE Loss:           0.1682\n",
      "Semantic Loss:     0.1601\n",
      "Neutrality Loss:   0.0000\n",
      "Source: One of my beloved editor buddies said this week: \"You should be more controversial...\" I'm just tired + broke right now  honest!\n",
      "Generated: One of my <NEU> editor buddies said this week: \"You should be more controversial...\" I'm just <NEU> + broke <NEU>\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 9920\n",
      "Total Loss:        0.4839\n",
      "CE Loss:           0.2129\n",
      "Semantic Loss:     0.1807\n",
      "Neutrality Loss:   0.0000\n",
      "Source: ehh...my lifes always a blast!\n",
      "Generated: ehh...my lifes always a blast!                 \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 9940\n",
      "Total Loss:        0.8694\n",
      "CE Loss:           0.4844\n",
      "Semantic Loss:     0.2553\n",
      "Neutrality Loss:   0.0069\n",
      "Source: literally feel attacked everytime I read one of your tweets bullying\n",
      "Generated: literally <NEU> attacked everytime I read one of your tweets bullying literally literally literally literally literally literally literally literally literally literally literally literally literally literally\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 9960\n",
      "Total Loss:        0.4709\n",
      "CE Loss:           0.1815\n",
      "Semantic Loss:     0.1793\n",
      "Neutrality Loss:   0.0677\n",
      "Source: My favorite part of these appointments are filling out paperwork notreally fuckinghatethisshit\n",
      "Generated: My <NEU> part of these appointments are filling out paperwork ourally fuckinghatethisshit My My My My My\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 9980\n",
      "Total Loss:        0.3738\n",
      "CE Loss:           0.1325\n",
      "Semantic Loss:     0.1609\n",
      "Neutrality Loss:   0.0000\n",
      "Source: WATCH: Feature length documentary on why derry / londonderry needs a drugs and alcohol detox unit.\n",
      "Generated: WATCH: Feature length documentary on why derry / londonderry needs a drugs and alcohol detox unit.  \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 10000\n",
      "Total Loss:        0.6127\n",
      "CE Loss:           0.3045\n",
      "Semantic Loss:     0.2055\n",
      "Neutrality Loss:   0.0000\n",
      "Source: The fact that people are claiming to be insulted and referring to her as a \"slut\" in the process really makes me laugh.\n",
      "Generated: The <NEU> that people are claiming to be <NEU> and referring to her as a \"slut\" in the \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 10020\n",
      "Total Loss:        0.2844\n",
      "CE Loss:           0.0624\n",
      "Semantic Loss:     0.1461\n",
      "Neutrality Loss:   0.0096\n",
      "Source: I was joking like if you were sick you wouldn't be at school (I was acting like I wanted you to be sick)\n",
      "Generated: I was joking <NEU> if you were sick you wouldn't be at school (I was acting <NEU> I wanted you to be \n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 10040\n",
      "Total Loss:        0.4233\n",
      "CE Loss:           0.2299\n",
      "Semantic Loss:     0.1277\n",
      "Neutrality Loss:   0.0063\n",
      "Source: Neuroscientists locate 'alcoholism neurons' in the brain - towards blocking the craving? addictions drugs /via\n",
      "Generated: Neuroscientists locate 'alcoholism neurons' in the brain - towards blocking the <NEU> addictions drugs\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 10060\n",
      "Total Loss:        0.2647\n",
      "CE Loss:           0.0199\n",
      "Semantic Loss:     0.1632\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Stuff You Should Be Doing In Your 20s\n",
      "Generated: Stuff You Should Be Doing In Your 20s St St St St St St St St St St St St St St St St St\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 10080\n",
      "Total Loss:        0.4747\n",
      "CE Loss:           0.2346\n",
      "Semantic Loss:     0.1601\n",
      "Neutrality Loss:   0.0000\n",
      "Source: late late latepost Huakhh look Like i wanna kill someone wkwkwkw xD LoL\n",
      "Generated: late late latepost Huakhh look <NEU> i wanna kill someone wkwkwkw xD Loa\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 10100\n",
      "Total Loss:        0.6674\n",
      "CE Loss:           0.2132\n",
      "Semantic Loss:     0.3028\n",
      "Neutrality Loss:   0.0000\n",
      "Source: See kids - once you are wildly successful you can have two jobs. Until then, you cannot be in two places at the same time (openly).\n",
      "Generated: See kids - once you are wildly successful you can have two jobs. Until then, you cannot be in two places at the same time.\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 10120\n",
      "Total Loss:        0.4038\n",
      "CE Loss:           0.2296\n",
      "Semantic Loss:     0.1161\n",
      "Neutrality Loss:   0.0000\n",
      "Source: So that all your Executive and Snr Mgr may drive free whole year .   waah gandibaat\n",
      "Generated: So that all your Executive and Snr Mgr may drive free whole year .   waah gandiba\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 10140\n",
      "Total Loss:        0.3209\n",
      "CE Loss:           0.0793\n",
      "Semantic Loss:     0.1611\n",
      "Neutrality Loss:   0.0000\n",
      "Source: Today two years ago we keep each other, but today we act like don't know each other in friends TL\n",
      "Generated: Today two years ago we keep each other, but today we act <NEU> don't <NEU> each other in friends <NEU> today Today Today\n",
      "=======================================\n",
      "\n",
      "\n",
      "================ DEBUG ================\n",
      "Step 10160\n",
      "Total Loss:        0.4966\n",
      "CE Loss:           0.1928\n",
      "Semantic Loss:     0.2025\n",
      "Neutrality Loss:   0.0000\n",
      "Source: . too much irony for one tweet\n",
      "Generated: . too much <NEU> for one tweet                     \n",
      "=======================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10173, training_loss=0.5329284172991812, metrics={'train_runtime': 2324.7903, 'train_samples_per_second': 35.006, 'train_steps_per_second': 4.376, 'total_flos': 3482620671688704.0, 'train_loss': 0.5329284172991812, 'epoch': 1.0})"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-dEyiPzCUVK"
   },
   "source": [
    "## testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 456,
     "status": "ok",
     "timestamp": 1763437073061,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "T6uHpQ-hCTev",
    "outputId": "a061bb72-72ca-4a3d-9847-efe1e0a5621a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32102, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32102, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32102, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32102, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "checkpoint_path = os.path.join(folder_path,\n",
    "    \"model_checkpoints/masked-3loss-tweets-mpqa-flan_t5-2loss-11_16/checkpoint-10000\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "trained_model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_path)\n",
    "trained_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 890,
     "referenced_widgets": [
      "53b0af708f214a9fba58ca71bffab48d",
      "37b2396a986c41dc993c20ecf484bdff",
      "65311a5a4c2b4ebf88971ca3b1ff4c2a",
      "ec2991ffa0b2447ebef695409e3bbb06",
      "794f1bef6a2741068c3878dfb9161a39",
      "4a2bd8d846104eb7b8aab304d0f77a73",
      "adbc2faeb9494f83be702f45ce85c7cc",
      "c9a08413736d4689b6fc2391eb4bf7dc",
      "54ef07bdd9684240be90db973e3439ef",
      "c60368f5ad77441bb195ab9291f91d12",
      "87c3f45b89ed46758dfc338d2403724d"
     ]
    },
    "executionInfo": {
     "elapsed": 1435,
     "status": "ok",
     "timestamp": 1763439107721,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "y6poviHFCrBJ",
    "outputId": "4407fdbe-cda9-49b4-bf4b-1c77979a79a7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df\",\n  \"rows\": 8124,\n  \"fields\": [\n    {\n      \"column\": \"tweets\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7927,\n        \"samples\": [\n          \"Watch Lady Gaga in Haunting New 'American Horror Story' Trailer: Lady Gaga's appearance in the trailer ... peace\",\n          \"I am shocked, SHOCKED, that Swagger ONCE AGAIN lost to Rusev. Who. Would. Have. Thought. smackdown swaggervsrusev wwe\",\n          \"Are you sure? humor realestate hashtag\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"class\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"irony\",\n          \"sarcasm\",\n          \"figurative\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tokens\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7927,\n        \"samples\": [\n          \"['Watch', 'Lady', 'Gaga', 'in', 'Haunting', 'New', \\\"'American\\\", 'Horror', 'Story', \\\"'\\\", 'Trailer', ':', 'Lady', 'Gaga', \\\"'s\\\", 'appearance', 'in', 'the', 'trailer', '...', 'peace']\",\n          \"['I', 'am', 'shocked', ',', 'SHOCKED', ',', 'that', 'Swagger', 'ONCE', 'AGAIN', 'lost', 'to', 'Rusev', '.', 'Who', '.', 'Would', '.', 'Have', '.', 'Thought', '.', 'smackdown', 'swaggervsrusev', 'wwe']\",\n          \"['Are', 'you', 'sure', '?', 'humor', 'realestate', 'hashtag']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"flags\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2216,\n        \"samples\": [\n          \"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1]\",\n          \"[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\",\n          \"[0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"input_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7927,\n        \"samples\": [\n          \"neutralize: Watch[0] Lady[0] Gaga[1] in[0] Haunting[1] New[0] 'American[0] Horror[1] Story[0] '[0] Trailer[0] :[0] Lady[0] Gaga[1] 's[0] appearance[0] in[0] the[0] trailer[0] ...[0] peace[0]\",\n          \"neutralize: I[0] am[0] shocked[0] ,[0] SHOCKED[0] ,[0] that[0] Swagger[1] ONCE[0] AGAIN[0] lost[0] to[0] Rusev[0] .[0] Who[0] .[0] Would[0] .[0] Have[0] .[0] Thought[0] .[0] smackdown[0] swaggervsrusev[0] wwe[0]\",\n          \"neutralize: Are[0] you[0] sure[1] ?[0] humor[1] realestate[0] hashtag[0]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7927,\n        \"samples\": [\n          \"Watch Lady Gaga in Haunting New 'American Horror Story' Trailer: Lady Gaga's appearance in the trailer ... peace\",\n          \"I am shocked, SHOCKED, that Swagger ONCE AGAIN lost to Rusev. Who. Would. Have. Thought. smackdown swaggervsrusev wwe\",\n          \"Are you sure? humor realestate hashtag\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-6d8cf8f6-845f-41c0-a9c1-8b52a68d3950\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>class</th>\n",
       "      <th>tokens</th>\n",
       "      <th>flags</th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no one ever predicted this was going to happen.</td>\n",
       "      <td>figurative</td>\n",
       "      <td>['no', 'one', 'ever', 'predicted', 'this', 'wa...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>neutralize: no[0] one[0] ever[0] predicted[0] ...</td>\n",
       "      <td>no one ever predicted this was going to happen.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>its as closely related as Andrews original cla...</td>\n",
       "      <td>figurative</td>\n",
       "      <td>['its', 'as', 'closely', 'related', 'as', 'And...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>neutralize: its[0] as[0] closely[0] related[0]...</td>\n",
       "      <td>its as closely related as Andrews original cla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I find it ironic when Vegans say they love food</td>\n",
       "      <td>figurative</td>\n",
       "      <td>['I', 'find', 'it', 'ironic', 'when', 'Vegans'...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 1, 0]</td>\n",
       "      <td>neutralize: I[0] find[0] it[0] ironic[1] when[...</td>\n",
       "      <td>I find it ironic when Vegans say they love food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Quick rt that throwing money vine I've not see...</td>\n",
       "      <td>figurative</td>\n",
       "      <td>['Quick', 'rt', 'that', 'throwing', 'money', '...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>neutralize: Quick[0] rt[0] that[0] throwing[0]...</td>\n",
       "      <td>Quick rt that throwing money vine I've not see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yep, keep adding me to your devops lists.... j...</td>\n",
       "      <td>figurative</td>\n",
       "      <td>['yep', ',', 'keep', 'adding', 'me', 'to', 'yo...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>neutralize: yep[1] ,[0] keep[0] adding[0] me[0...</td>\n",
       "      <td>yep, keep adding me to your devops lists.... j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8119</th>\n",
       "      <td>Why yes I will totally submit my photos to a s...</td>\n",
       "      <td>sarcasm</td>\n",
       "      <td>['Why', 'yes', 'I', 'will', 'totally', 'submit...</td>\n",
       "      <td>[0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>neutralize: Why[0] yes[1] I[0] will[1] totally...</td>\n",
       "      <td>Why yes I will totally submit my photos to a s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8120</th>\n",
       "      <td>Test on a Saturday! Thank you uni! @ Griffith ...</td>\n",
       "      <td>sarcasm</td>\n",
       "      <td>['Test', 'on', 'a', 'Saturday', '!', 'Thank', ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>neutralize: Test[0] on[0] a[0] Saturday[0] ![0...</td>\n",
       "      <td>Test on a Saturday! Thank you uni! @ Griffith ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8121</th>\n",
       "      <td>Listening to 's Misery isn't at all disconcert...</td>\n",
       "      <td>sarcasm</td>\n",
       "      <td>['Listening', 'to', \"'s\", 'Misery', 'is', \"n't...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 1, 0]</td>\n",
       "      <td>neutralize: Listening[0] to[0] 's[0] Misery[1]...</td>\n",
       "      <td>Listening to 's Misery isn't at all disconcert...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8122</th>\n",
       "      <td>There you go being kind again standup4kids</td>\n",
       "      <td>sarcasm</td>\n",
       "      <td>['There', 'you', 'go', 'being', 'kind', 'again...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0]</td>\n",
       "      <td>neutralize: There[0] you[0] go[0] being[0] kin...</td>\n",
       "      <td>There you go being kind again standup4kids</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8123</th>\n",
       "      <td>I'm shocked that these refs in the tcu vs minn...</td>\n",
       "      <td>sarcasm</td>\n",
       "      <td>['I', \"'m\", 'shocked', 'that', 'these', 'refs'...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>neutralize: I[0] 'm[0] shocked[0] that[0] thes...</td>\n",
       "      <td>I'm shocked that these refs in the tcu vs minn...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8124 rows  6 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6d8cf8f6-845f-41c0-a9c1-8b52a68d3950')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-6d8cf8f6-845f-41c0-a9c1-8b52a68d3950 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-6d8cf8f6-845f-41c0-a9c1-8b52a68d3950');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-4855dfd4-ab1f-44a7-9423-8259bf99460e\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4855dfd4-ab1f-44a7-9423-8259bf99460e')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-4855dfd4-ab1f-44a7-9423-8259bf99460e button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "  <div id=\"id_3c925fba-3b80-417a-abd9-4995c7108692\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_3c925fba-3b80-417a-abd9-4995c7108692 button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('df');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                                 tweets       class  \\\n",
       "0       no one ever predicted this was going to happen.  figurative   \n",
       "1     its as closely related as Andrews original cla...  figurative   \n",
       "2       I find it ironic when Vegans say they love food  figurative   \n",
       "3     Quick rt that throwing money vine I've not see...  figurative   \n",
       "4     yep, keep adding me to your devops lists.... j...  figurative   \n",
       "...                                                 ...         ...   \n",
       "8119  Why yes I will totally submit my photos to a s...     sarcasm   \n",
       "8120  Test on a Saturday! Thank you uni! @ Griffith ...     sarcasm   \n",
       "8121  Listening to 's Misery isn't at all disconcert...     sarcasm   \n",
       "8122         There you go being kind again standup4kids     sarcasm   \n",
       "8123  I'm shocked that these refs in the tcu vs minn...     sarcasm   \n",
       "\n",
       "                                                 tokens  \\\n",
       "0     ['no', 'one', 'ever', 'predicted', 'this', 'wa...   \n",
       "1     ['its', 'as', 'closely', 'related', 'as', 'And...   \n",
       "2     ['I', 'find', 'it', 'ironic', 'when', 'Vegans'...   \n",
       "3     ['Quick', 'rt', 'that', 'throwing', 'money', '...   \n",
       "4     ['yep', ',', 'keep', 'adding', 'me', 'to', 'yo...   \n",
       "...                                                 ...   \n",
       "8119  ['Why', 'yes', 'I', 'will', 'totally', 'submit...   \n",
       "8120  ['Test', 'on', 'a', 'Saturday', '!', 'Thank', ...   \n",
       "8121  ['Listening', 'to', \"'s\", 'Misery', 'is', \"n't...   \n",
       "8122  ['There', 'you', 'go', 'being', 'kind', 'again...   \n",
       "8123  ['I', \"'m\", 'shocked', 'that', 'these', 'refs'...   \n",
       "\n",
       "                                                  flags  \\\n",
       "0                        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "1                  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "2                        [0, 0, 0, 1, 0, 0, 0, 0, 1, 0]   \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4                  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "...                                                 ...   \n",
       "8119  [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "8120      [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "8121                     [0, 0, 0, 1, 0, 0, 0, 0, 1, 0]   \n",
       "8122                              [0, 0, 0, 0, 1, 0, 0]   \n",
       "8123  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                             input_text  \\\n",
       "0     neutralize: no[0] one[0] ever[0] predicted[0] ...   \n",
       "1     neutralize: its[0] as[0] closely[0] related[0]...   \n",
       "2     neutralize: I[0] find[0] it[0] ironic[1] when[...   \n",
       "3     neutralize: Quick[0] rt[0] that[0] throwing[0]...   \n",
       "4     neutralize: yep[1] ,[0] keep[0] adding[0] me[0...   \n",
       "...                                                 ...   \n",
       "8119  neutralize: Why[0] yes[1] I[0] will[1] totally...   \n",
       "8120  neutralize: Test[0] on[0] a[0] Saturday[0] ![0...   \n",
       "8121  neutralize: Listening[0] to[0] 's[0] Misery[1]...   \n",
       "8122  neutralize: There[0] you[0] go[0] being[0] kin...   \n",
       "8123  neutralize: I[0] 'm[0] shocked[0] that[0] thes...   \n",
       "\n",
       "                                            target_text  \n",
       "0       no one ever predicted this was going to happen.  \n",
       "1     its as closely related as Andrews original cla...  \n",
       "2       I find it ironic when Vegans say they love food  \n",
       "3     Quick rt that throwing money vine I've not see...  \n",
       "4     yep, keep adding me to your devops lists.... j...  \n",
       "...                                                 ...  \n",
       "8119  Why yes I will totally submit my photos to a s...  \n",
       "8120  Test on a Saturday! Thank you uni! @ Griffith ...  \n",
       "8121  Listening to 's Misery isn't at all disconcert...  \n",
       "8122         There you go being kind again standup4kids  \n",
       "8123  I'm shocked that these refs in the tcu vs minn...  \n",
       "\n",
       "[8124 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"test_dataset = dataset_test\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"input_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"neutralize: its as closely related as Andrews original claim that evolution and entropy\",\n          \"neutralize: y<SUBJ> keep adding me to your devops lists.... justsaying infosec\",\n          \"neutralize: I find it <SUBJ> when Vegans say they <SUBJ> food\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"its as closely related as Andrews original claim that evolution and entropy\",\n          \"<NEU> keep adding me to your devops lists.... justsaying infosec\",\n          \"I find it <NEU> when Vegans say they <NEU> food\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"original_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"its as closely related as Andrews original claim that evolution and entropy\",\n          \"yep, keep adding me to your devops lists.... justsaying infosec\",\n          \"I find it ironic when Vegans say they love food\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-b99b6ac1-5906-4634-bdd0-cb2bf1ed3183\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "      <th>original_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutralize: no one ever predicted this was goi...</td>\n",
       "      <td>no one ever predicted this was going to happen.</td>\n",
       "      <td>no one ever predicted this was going to happen.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutralize: its as closely related as Andrews ...</td>\n",
       "      <td>its as closely related as Andrews original cla...</td>\n",
       "      <td>its as closely related as Andrews original cla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutralize: I find it &lt;SUBJ&gt; when Vegans say t...</td>\n",
       "      <td>I find it &lt;NEU&gt; when Vegans say they &lt;NEU&gt; food</td>\n",
       "      <td>I find it ironic when Vegans say they love food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutralize: Quick rt that throwing money vine ...</td>\n",
       "      <td>Quick rt that throwing money vine I've not see...</td>\n",
       "      <td>Quick rt that throwing money vine I've not see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutralize: y&lt;SUBJ&gt; keep adding me to your dev...</td>\n",
       "      <td>&lt;NEU&gt; keep adding me to your devops lists.... ...</td>\n",
       "      <td>yep, keep adding me to your devops lists.... j...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b99b6ac1-5906-4634-bdd0-cb2bf1ed3183')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-b99b6ac1-5906-4634-bdd0-cb2bf1ed3183 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-b99b6ac1-5906-4634-bdd0-cb2bf1ed3183');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-7e1981de-2d81-468d-9dd5-7e429190c847\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7e1981de-2d81-468d-9dd5-7e429190c847')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-7e1981de-2d81-468d-9dd5-7e429190c847 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                          input_text  \\\n",
       "0  neutralize: no one ever predicted this was goi...   \n",
       "1  neutralize: its as closely related as Andrews ...   \n",
       "2  neutralize: I find it <SUBJ> when Vegans say t...   \n",
       "3  neutralize: Quick rt that throwing money vine ...   \n",
       "4  neutralize: y<SUBJ> keep adding me to your dev...   \n",
       "\n",
       "                                         target_text  \\\n",
       "0    no one ever predicted this was going to happen.   \n",
       "1  its as closely related as Andrews original cla...   \n",
       "2    I find it <NEU> when Vegans say they <NEU> food   \n",
       "3  Quick rt that throwing money vine I've not see...   \n",
       "4  <NEU> keep adding me to your devops lists.... ...   \n",
       "\n",
       "                                       original_text  \n",
       "0    no one ever predicted this was going to happen.  \n",
       "1  its as closely related as Andrews original cla...  \n",
       "2    I find it ironic when Vegans say they love food  \n",
       "3  Quick rt that throwing money vine I've not see...  \n",
       "4  yep, keep adding me to your devops lists.... j...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test size: 81382\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53b0af708f214a9fba58ca71bffab48d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8124 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# @title load dataset\n",
    "# dataset_test = load_dataset(\"csv\", data_files=os.path.join(folder_path, \"data/tweets/processed/mpqa_lexicon/flagged_test.csv\"))\n",
    "# max_length = 128\n",
    "df=pd.read_csv(os.path.join(folder_path, \"data/tweets/processed/mpqa_lexicon/flagged_test.csv\"))\n",
    "display(df)\n",
    "\n",
    "sources = []\n",
    "targets = []\n",
    "originals = []\n",
    "\n",
    "for t in df[\"tweets\"]:\n",
    "    s, y, o = neutralize_one_sentence(str(t))\n",
    "    sources.append(s)\n",
    "    targets.append(y)\n",
    "    originals.append(o)\n",
    "\n",
    "df_test = pd.DataFrame(\n",
    "    {\n",
    "        \"input_text\": sources,\n",
    "        \"target_text\": targets,\n",
    "        \"original_text\": originals,\n",
    "    }\n",
    ")\n",
    "\n",
    "display(df_test.head())\n",
    "print(\"test size:\", len(df_train))\n",
    "\n",
    "dataset_test=Dataset.from_pandas(df_test)\n",
    "\n",
    "def preprocess_test(batch):\n",
    "    inputs = tokenizer(batch[\"input_text\"], max_length=max_length, truncation=True, padding='longest')\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(batch[\"target_text\"], max_length=max_length, truncation=True, padding='longest')\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "test_dataset = dataset_test.map(preprocess_test, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "executionInfo": {
     "elapsed": 818804,
     "status": "ok",
     "timestamp": 1763439946633,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "PPBKnn-tDUdk",
    "outputId": "f315f286-8d4b-4b63-d2ff-2cf270b50f80"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-3436716929.py:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @title run predictions on test set\n",
    "\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "# make sure predict_with_generate=True so it uses model.generate()\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./eval_results\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,  # turn off on GPU\n",
    "    report_to=\"none\",\n",
    "    generation_max_length=32,     # allow up to 128 tokens in output\n",
    "    generation_num_beams=4,        # optional, improves quality (beam search)\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=trained_model,\n",
    "    args=args,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "predictions = trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1152
    },
    "executionInfo": {
     "elapsed": 650,
     "status": "ok",
     "timestamp": 1763439977147,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "xB5jdrN0DdBH",
    "outputId": "a62517b1-5896-407d-f34f-2e26fcab1bfa"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df_results\",\n  \"rows\": 8124,\n  \"fields\": [\n    {\n      \"column\": \"input_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7927,\n        \"samples\": [\n          \"neutralize: Watch Lady <SUBJ> in <SUBJ> New 'American <SUBJ> Story' Trailer: Lady Gaga's appearance in the trailer ... peace\",\n          \"neutralize: I am shocked, SHOCKED, that <SUBJ> ONCE AGAIN lost to Rusev. Who. Would. Have. Thought. smackdown swaggervsrusev wwe\",\n          \"neutralize: Are you s<SUBJ> <SUBJ> realestate hashtag\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7922,\n        \"samples\": [\n          \"I <NEU> being ignored!\",\n          \"I am shocked, SHOCKED, that <NEU> ONCE AGAIN lost to Rusev. Who. Would. Have. Thought. smackdown swaggervsrusev wwe\",\n          \"Learning still, life has a <NEU> way of teaching you lessons life blessed\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"predicted_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7922,\n        \"samples\": [\n          \"I  being ignored!\",\n          \"I am shocked, SHOCKED, that  ONCE AGAIN lost to Rusev. Who. Would. Have. Thought.\",\n          \"Learning still, life has a  way of teaching you lessons life blessed\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "df_results"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-7e448270-faf9-4d94-823d-87be8ea89931\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "      <th>predicted_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutralize: no one ever predicted this was goi...</td>\n",
       "      <td>no one ever predicted this was going to happen.</td>\n",
       "      <td>no one ever predicted this was going to happen.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutralize: its as closely related as Andrews ...</td>\n",
       "      <td>its as closely related as Andrews original cla...</td>\n",
       "      <td>its as closely related as Andrews original cla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutralize: I find it &lt;SUBJ&gt; when Vegans say t...</td>\n",
       "      <td>I find it &lt;NEU&gt; when Vegans say they &lt;NEU&gt; food</td>\n",
       "      <td>I find it  when Vegans say they  food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutralize: Quick rt that throwing money vine ...</td>\n",
       "      <td>Quick rt that throwing money vine I've not see...</td>\n",
       "      <td>Quick rt that throwing money vine I've not see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutralize: y&lt;SUBJ&gt; keep adding me to your dev...</td>\n",
       "      <td>&lt;NEU&gt; keep adding me to your devops lists.... ...</td>\n",
       "      <td>keep adding me to your devops lists.... justsa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8119</th>\n",
       "      <td>neutralize: Why &lt;SUBJ&gt; I &lt;SUBJ&gt; totally submit...</td>\n",
       "      <td>Why &lt;NEU&gt; I &lt;NEU&gt; totally submit my photos to ...</td>\n",
       "      <td>Why  I  totally submit my photos to a shitty o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8120</th>\n",
       "      <td>neutralize: Test on a Saturday! &lt;SUBJ&gt; you uni...</td>\n",
       "      <td>Test on a Saturday! &lt;NEU&gt; you uni! @ Griffith ...</td>\n",
       "      <td>Test on a Saturday!  you uni! @ Griffith Unive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8121</th>\n",
       "      <td>neutralize: Listening to 's &lt;SUBJ&gt; isn't at al...</td>\n",
       "      <td>Listening to 's &lt;NEU&gt; isn't at all &lt;NEU&gt;</td>\n",
       "      <td>Listening to 's  isn't at all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8122</th>\n",
       "      <td>neutralize: There you go being &lt;SUBJ&gt; again st...</td>\n",
       "      <td>There you go being &lt;NEU&gt; again standup4kids</td>\n",
       "      <td>There you go being  again standup4kids</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8123</th>\n",
       "      <td>neutralize: I'm shocked that these refs in the...</td>\n",
       "      <td>I'm shocked that these refs in the tcu vs minn...</td>\n",
       "      <td>I'm shocked that these refs in the tcu vs minn...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8124 rows  3 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7e448270-faf9-4d94-823d-87be8ea89931')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-7e448270-faf9-4d94-823d-87be8ea89931 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-7e448270-faf9-4d94-823d-87be8ea89931');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-adc9f4d9-58ed-4175-a23e-9a156bd4202a\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-adc9f4d9-58ed-4175-a23e-9a156bd4202a')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-adc9f4d9-58ed-4175-a23e-9a156bd4202a button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "  <div id=\"id_ab3c7e3b-ec57-4a31-8e26-f502fa258038\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_results')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_ab3c7e3b-ec57-4a31-8e26-f502fa258038 button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('df_results');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                             input_text  \\\n",
       "0     neutralize: no one ever predicted this was goi...   \n",
       "1     neutralize: its as closely related as Andrews ...   \n",
       "2     neutralize: I find it <SUBJ> when Vegans say t...   \n",
       "3     neutralize: Quick rt that throwing money vine ...   \n",
       "4     neutralize: y<SUBJ> keep adding me to your dev...   \n",
       "...                                                 ...   \n",
       "8119  neutralize: Why <SUBJ> I <SUBJ> totally submit...   \n",
       "8120  neutralize: Test on a Saturday! <SUBJ> you uni...   \n",
       "8121  neutralize: Listening to 's <SUBJ> isn't at al...   \n",
       "8122  neutralize: There you go being <SUBJ> again st...   \n",
       "8123  neutralize: I'm shocked that these refs in the...   \n",
       "\n",
       "                                            target_text  \\\n",
       "0       no one ever predicted this was going to happen.   \n",
       "1     its as closely related as Andrews original cla...   \n",
       "2       I find it <NEU> when Vegans say they <NEU> food   \n",
       "3     Quick rt that throwing money vine I've not see...   \n",
       "4     <NEU> keep adding me to your devops lists.... ...   \n",
       "...                                                 ...   \n",
       "8119  Why <NEU> I <NEU> totally submit my photos to ...   \n",
       "8120  Test on a Saturday! <NEU> you uni! @ Griffith ...   \n",
       "8121           Listening to 's <NEU> isn't at all <NEU>   \n",
       "8122        There you go being <NEU> again standup4kids   \n",
       "8123  I'm shocked that these refs in the tcu vs minn...   \n",
       "\n",
       "                                         predicted_text  \n",
       "0       no one ever predicted this was going to happen.  \n",
       "1     its as closely related as Andrews original cla...  \n",
       "2                 I find it  when Vegans say they  food  \n",
       "3     Quick rt that throwing money vine I've not see...  \n",
       "4     keep adding me to your devops lists.... justsa...  \n",
       "...                                                 ...  \n",
       "8119  Why  I  totally submit my photos to a shitty o...  \n",
       "8120  Test on a Saturday!  you uni! @ Griffith Unive...  \n",
       "8121                     Listening to 's  isn't at all   \n",
       "8122             There you go being  again standup4kids  \n",
       "8123  I'm shocked that these refs in the tcu vs minn...  \n",
       "\n",
       "[8124 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# # Replace potential out-of-vocabulary predicted IDs with pad_token_id\n",
    "predictions.predictions[predictions.predictions >= tokenizer.vocab_size] = tokenizer.pad_token_id\n",
    "predictions.predictions[predictions.predictions < 0] = tokenizer.pad_token_id # Also handle potential negative indices\n",
    "\n",
    "# Replace -100 in labels with pad_token_id for correct decoding\n",
    "predictions.label_ids[predictions.label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "preds=tokenizer.batch_decode(\n",
    "    predictions.predictions, skip_special_tokens=True\n",
    ")\n",
    "labels = tokenizer.batch_decode(\n",
    "    predictions.label_ids, skip_special_tokens=True\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "df_results = pd.DataFrame({\n",
    "    \"input_text\": dataset_test[\"input_text\"],\n",
    "    \"target_text\": labels,\n",
    "    \"predicted_text\": preds\n",
    "})\n",
    "df_results.to_csv(os.path.join(folder_path, \"output/tweets/predictions_testset_tweets-mpqa-flan_t5-masking-11_16.csv\"), index=False)\n",
    "\n",
    "display(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zw6aHazyJXhi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 198637,
     "status": "ok",
     "timestamp": 1763440391294,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "CoMnM1NMD_TN",
    "outputId": "d37fd3e6-40bd-493e-9297-dad417dac082"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean objectivity score: 0.42378560147981437\n",
      "std: 0.24204374464856332\n",
      "total percent of objective sentences: 0.32200886262924666\n"
     ]
    }
   ],
   "source": [
    "# @title objectivity evaluation\n",
    "import numpy as np\n",
    "clf_name = \"GroNLP/mdebertav3-subjectivity-english\"\n",
    "clf_tok = AutoTokenizer.from_pretrained(clf_name)\n",
    "clf = AutoModelForSequenceClassification.from_pretrained(clf_name).to(trained_model.device).eval()\n",
    "\n",
    "# predicted text\n",
    "path=os.path.join(folder_path, \"output/tweets/predictions_testset_tweets-mpqa-flan_t5-masking-11_16.csv\")\n",
    "df=pd.read_csv(path)\n",
    "\n",
    "# Fill any NaN values with an empty string and ensure column is of string type\n",
    "df[\"predicted_text\"] = df[\"predicted_text\"].fillna(\"\").astype(str)\n",
    "\n",
    "# mismatches_df = df[df['predicted_text'] != df['target_text']]\n",
    "# display(mismatches_df)\n",
    "scores = []\n",
    "count_objective=0\n",
    "for text in df[\"predicted_text\"]:\n",
    "    tokens = clf_tok(text, return_tensors=\"pt\", truncation=True).to(trained_model.device)\n",
    "    with torch.no_grad():\n",
    "        probs = clf(**tokens).logits.softmax(dim=-1)\n",
    "        p_objective = probs[0][0].item()  # index 0 = objective\n",
    "        if p_objective>0.5:\n",
    "          count_objective+=1\n",
    "\n",
    "    scores.append(p_objective)\n",
    "\n",
    "print(\"Mean objectivity score:\", np.mean(scores))\n",
    "print(\"std:\", np.std(scores))\n",
    "print(\"total percent of objective sentences:\", count_objective/len(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vsJf40ym9Vr9"
   },
   "source": [
    "# WNC dataset processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "executionInfo": {
     "elapsed": 2833,
     "status": "ok",
     "timestamp": 1762717893519,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "DhvxSxkM5WJW",
    "outputId": "0322ae81-2a8d-416e-963d-b0a285f8326a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53803, 2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"display(df_mcn\",\n  \"rows\": 53793,\n  \"fields\": [\n    {\n      \"column\": \"biased\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 53683,\n        \"samples\": [\n          \"unfortunately for the uswa, their biggest crowds came every monday night at the mid-south coliseum in memphis, tennessee.\",\n          \"dodging conscription he moved to london in 1971. he had accepted being gay in 1969 and four days after arriving he spotted a sticker on a lamp-post in oxford street advertising a meeting of the london gay liberation front (glf).\",\n          \"he remained terribly obese throughout his adult years and died from a heart attack in 1988.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"neutral\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 53696,\n        \"samples\": [\n          \"`abbas ibn `abd al-muttalib (c. 566 \\u2013 c. 653) was a paternal uncle and sahabi (companion) of muhammad.\",\n          \"william donald kelley, dds, ms (november 1, 1925 january 30, 2005) was an orthodontist who developed the kelley cancer therapy, an ineffective alternative cancer treatment based on the belief that \\\"wrong foods (cause a) malignancy to grow, while proper foods (allow) ... body defenses to work\\\" defeating the cancer.\",\n          \"hucknall (also known as \\\"mick 'red mick' hucknall\\\") is the lead singer of the british band simply red.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-3d6fb17d-ae26-4d1b-bb3f-9b4b64ebb93b\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>biased</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chloroform \"the molecular lifesaver\" an articl...</td>\n",
       "      <td>chloroform \"the molecular lifesaver\" an articl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the free software gnu classpath project is onl...</td>\n",
       "      <td>the free software gnu classpath project is par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>other campaigners, especially the controversia...</td>\n",
       "      <td>other campaigners, especially the british acti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vocalist rob halford's performance is consider...</td>\n",
       "      <td>vocalist rob halford's performance is consider...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the proud general is a chinese animated featur...</td>\n",
       "      <td>the proud general is a chinese animated featur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53788</th>\n",
       "      <td>these books are considered to be part of old t...</td>\n",
       "      <td>these books are considered to be part of old t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53789</th>\n",
       "      <td>in terms of public transport, cumbernauld has ...</td>\n",
       "      <td>in terms of public transport, cumbernauld has ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53790</th>\n",
       "      <td>like the rest of nature, man's mind is subject...</td>\n",
       "      <td>like the rest of nature, man's mind is subject...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53791</th>\n",
       "      <td>the band are very popular for their 3d animate...</td>\n",
       "      <td>the band are popular for their 3d animated vid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53792</th>\n",
       "      <td>al-najah secondary school is a secondary, all-...</td>\n",
       "      <td>al-najah secondary school is a secondary, all-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53793 rows  2 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3d6fb17d-ae26-4d1b-bb3f-9b4b64ebb93b')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-3d6fb17d-ae26-4d1b-bb3f-9b4b64ebb93b button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-3d6fb17d-ae26-4d1b-bb3f-9b4b64ebb93b');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-0f8b8b18-33cc-488c-b736-dbfaad8f9713\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0f8b8b18-33cc-488c-b736-dbfaad8f9713')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-0f8b8b18-33cc-488c-b736-dbfaad8f9713 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                                  biased  \\\n",
       "0      chloroform \"the molecular lifesaver\" an articl...   \n",
       "1      the free software gnu classpath project is onl...   \n",
       "2      other campaigners, especially the controversia...   \n",
       "3      vocalist rob halford's performance is consider...   \n",
       "4      the proud general is a chinese animated featur...   \n",
       "...                                                  ...   \n",
       "53788  these books are considered to be part of old t...   \n",
       "53789  in terms of public transport, cumbernauld has ...   \n",
       "53790  like the rest of nature, man's mind is subject...   \n",
       "53791  the band are very popular for their 3d animate...   \n",
       "53792  al-najah secondary school is a secondary, all-...   \n",
       "\n",
       "                                                 neutral  \n",
       "0      chloroform \"the molecular lifesaver\" an articl...  \n",
       "1      the free software gnu classpath project is par...  \n",
       "2      other campaigners, especially the british acti...  \n",
       "3      vocalist rob halford's performance is consider...  \n",
       "4      the proud general is a chinese animated featur...  \n",
       "...                                                  ...  \n",
       "53788  these books are considered to be part of old t...  \n",
       "53789  in terms of public transport, cumbernauld has ...  \n",
       "53790  like the rest of nature, man's mind is subject...  \n",
       "53791  the band are popular for their 3d animated vid...  \n",
       "53792  al-najah secondary school is a secondary, all-...  \n",
       "\n",
       "[53793 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Install dependencies as needed:\n",
    "# pip install kagglehub[hf-datasets]\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "\n",
    "\n",
    "file_path = os.path.join(folder_path, \"data/WNC/WNC/biased.word.train\")\n",
    "\n",
    "\n",
    "# # Load the latest version\n",
    "# hf_dataset = kagglehub.load_dataset(\n",
    "#   KaggleDatasetAdapter.HUGGING_FACE,\n",
    "#   \"chandiragunatilleke/wiki-neutrality-corpus\",\n",
    "#   file_path,\n",
    "#   # Provide any additional arguments like\n",
    "#   # sql_query, hf_kwargs, or pandas_kwargs. See\n",
    "#   # the documenation for more information:\n",
    "#   # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterhugging_face\n",
    "# )\n",
    "# print(\"Hugging Face Dataset:\", hf_dataset)\n",
    "\n",
    "df_mcn = pd.read_csv(\n",
    "    file_path,\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"id\",\"src_tok\",\"tgt_tok\",\"biased\",\"neutral\",\"src_POS\",\"tgt_PARSE\"],\n",
    "    usecols=[\"biased\",\"neutral\"]\n",
    ")\n",
    "\n",
    "print(df_mcn.shape)\n",
    "display(df_mcn.head(-10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Oe4hsQdPEjy"
   },
   "source": [
    "## classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5504,
     "status": "ok",
     "timestamp": 1762721115076,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "YnIykHwNOrgw",
    "outputId": "33f521e0-3567-40e8-9968-b5b763052459"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"text-classification\", model=\"GroNLP/mdebertav3-subjectivity-english\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1879,
     "status": "ok",
     "timestamp": 1762721144138,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "SEyo3wWqPBt8",
    "outputId": "747eec83-9faa-4428-a7e5-b37d6926831f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_1', 'score': 0.6280233263969421}]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"vocalist rob halford's performance is considered one of his finest ever, and the guitar work is noted as well, especially the supremely epic dual guitar solo.\"\n",
    "result_food = classifier(sentence)\n",
    "print(result_food)\n",
    "# label_1: subjective; label_0: objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 297,
     "status": "ok",
     "timestamp": 1762722474433,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "XoHJWkuRcl6g",
    "outputId": "72a80271-6032-483d-c590-430bb34f16ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'LABEL_0', 1: 'LABEL_1'}\n",
      "{'LABEL_0': 0, 'LABEL_1': 1}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "cfg = AutoConfig.from_pretrained(\"GroNLP/mdebertav3-subjectivity-english\")\n",
    "print(cfg.id2label)\n",
    "print(cfg.label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4971,
     "status": "ok",
     "timestamp": 1762722536893,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "4WaSQCsCc0Oh",
    "outputId": "fd573701-5bd9-4aca-cb13-25e7e66bef37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The government announced a new policy today.\n",
      "probabilities: [[0.9522690176963806, 0.04773103818297386]]\n",
      "This is a horrible and unfair decision!\n",
      "probabilities: [[0.16840575635433197, 0.8315942287445068]]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "name = \"GroNLP/mdebertav3-subjectivity-english\"\n",
    "tok = AutoTokenizer.from_pretrained(name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(name)\n",
    "\n",
    "for s in [\n",
    "    \"The government announced a new policy today.\",   # objective\n",
    "    \"This is a horrible and unfair decision!\"          # subjective\n",
    "]:\n",
    "    t = tok(s, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        probs = model(**t).logits.softmax(dim=-1)\n",
    "    print(s)\n",
    "    print(\"probabilities:\", probs.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1068,
     "status": "ok",
     "timestamp": 1763410504374,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "SKyyaO95Pw8v",
    "outputId": "193b522d-e3f3-4748-ed46-80dcc5920fb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: [('abandoned', {'type': 'weaksubj', 'polarity': 'negative'}), ('abandonment', {'type': 'weaksubj', 'polarity': 'negative'}), ('abandon', {'type': 'weaksubj', 'polarity': 'negative'}), ('abase', {'type': 'strongsubj', 'polarity': 'negative'}), ('abasement', {'type': 'strongsubj', 'polarity': 'negative'}), ('abash', {'type': 'strongsubj', 'polarity': 'negative'}), ('abate', {'type': 'weaksubj', 'polarity': 'negative'}), ('abdicate', {'type': 'weaksubj', 'polarity': 'negative'}), ('aberration', {'type': 'strongsubj', 'polarity': 'negative'}), ('abhor', {'type': 'strongsubj', 'polarity': 'negative'})]\n",
      "Types:  ['strongsubj', 'weaksubj']\n",
      "polarities:  ['both', 'negative', 'neutral', 'positive', 'weakneg']\n"
     ]
    }
   ],
   "source": [
    "lexicon_path=os.path.join(folder_path, \"data/subjectivity_clues_hltemnlp05/subjclueslen1-HLTEMNLP05.tff\")\n",
    "def load_mpqa_lexicon(path):\n",
    "    subj_dict = {}\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            entry = {}\n",
    "            for p in parts:\n",
    "                if '=' in p:\n",
    "                    key, value = p.split('=', 1)\n",
    "                    entry[key] = value\n",
    "            word = entry.get(\"word1\", \"\").lower()\n",
    "            subj_type = entry.get(\"type\", \"\")\n",
    "            polarity = entry.get(\"priorpolarity\", \"\")\n",
    "            if word:\n",
    "                subj_dict[word] = {\"type\": subj_type, \"polarity\": polarity}\n",
    "    return subj_dict\n",
    "mpqa_dict = load_mpqa_lexicon(lexicon_path)\n",
    "print(\"Sample:\", list(mpqa_dict.items())[:10])\n",
    "types = sorted({v[\"type\"] for v in mpqa_dict.values()})\n",
    "polarities = sorted({v[\"polarity\"] for v in mpqa_dict.values()})\n",
    "print(\"Types: \", types)\n",
    "print(\"polarities: \", polarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1763352550512,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "xNsgwRidJXJR",
    "outputId": "c7737ff9-b8f5-4f9b-83b1-382314c426a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['chloroform', \"'the\", 'molecular', 'lifesaver', \"'\", 'an', 'article', 'at', 'oxford', 'university', 'providing', 'interesting', 'facts', 'about', 'chloroform', '.'], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def tag_subjectivity(sentence, lexicon):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    flags = []\n",
    "    for tok in tokens:\n",
    "        w = tok.lower()\n",
    "        if w in lexicon:\n",
    "            subj_type = lexicon[w][\"type\"]\n",
    "            flags.append(1 if subj_type in [\"strongsubj\"] else 0) #\"weaksubj\"\n",
    "        else:\n",
    "            flags.append(0)\n",
    "    return tokens, flags\n",
    "print(tag_subjectivity(\"chloroform 'the molecular lifesaver' an article at oxford university providing interesting facts about chloroform.\", mpqa_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11870,
     "status": "ok",
     "timestamp": 1762718156104,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "fe0nJKLYK_rV",
    "outputId": "ad000e76-1d1e-4063-88eb-551751d02ce8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 53803/53803 [00:11<00:00, 4562.57it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "df_mcn[\"tokens_flags\"] = df_mcn[\"biased\"].progress_apply(\n",
    "    lambda x: tag_subjectivity(x, mpqa_dict)\n",
    ")\n",
    "\n",
    "# Split the tuple column into two separate columns\n",
    "df_mcn[\"tokens\"] = df_mcn[\"tokens_flags\"].apply(lambda x: x[0])\n",
    "df_mcn[\"flags\"] = df_mcn[\"tokens_flags\"].apply(lambda x: x[1])\n",
    "\n",
    "# Drop the temporary column\n",
    "df_mcn = df_mcn.drop(columns=[\"tokens_flags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1200
    },
    "executionInfo": {
     "elapsed": 140,
     "status": "ok",
     "timestamp": 1762718269246,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "uo1-kOu0MLMj",
    "outputId": "950c76b5-add9-47a3-ac50-86f6d2521e14"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"display(df_mcn[[\\\"biased\\\", \\\"neutral\\\", \\\"tokens\\\", \\\"flags\\\"]]\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"biased\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"the free software gnu classpath project is only partially compatible with the current version of sun java.\",\n          \"the proud general is a chinese animated featurette produced by shanghai animation film studio under the master animator te wei.\",\n          \"other campaigners, especially the controversial british activist peter tatchell, attacked him for questioning the universal validity of gay identity.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"neutral\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"the free software gnu classpath project is partially compatible with the current version of sun java.\",\n          \"the proud general is a chinese animated featurette produced by shanghai animation film studio under the animator te wei.\",\n          \"other campaigners, especially the british activist peter tatchell, attacked him for questioning the universal validity of gay identity.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tokens\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"flags\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-3c51b091-d6a7-4b0c-a350-b745e178dd5d\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>biased</th>\n",
       "      <th>neutral</th>\n",
       "      <th>tokens</th>\n",
       "      <th>flags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chloroform \"the molecular lifesaver\" an articl...</td>\n",
       "      <td>chloroform \"the molecular lifesaver\" an articl...</td>\n",
       "      <td>[chloroform, ``, the, molecular, lifesaver, ''...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the free software gnu classpath project is onl...</td>\n",
       "      <td>the free software gnu classpath project is par...</td>\n",
       "      <td>[the, free, software, gnu, classpath, project,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>other campaigners, especially the controversia...</td>\n",
       "      <td>other campaigners, especially the british acti...</td>\n",
       "      <td>[other, campaigners, ,, especially, the, contr...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vocalist rob halford's performance is consider...</td>\n",
       "      <td>vocalist rob halford's performance is consider...</td>\n",
       "      <td>[vocalist, rob, halford, 's, performance, is, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the proud general is a chinese animated featur...</td>\n",
       "      <td>the proud general is a chinese animated featur...</td>\n",
       "      <td>[the, proud, general, is, a, chinese, animated...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3c51b091-d6a7-4b0c-a350-b745e178dd5d')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-3c51b091-d6a7-4b0c-a350-b745e178dd5d button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-3c51b091-d6a7-4b0c-a350-b745e178dd5d');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-5f8cc120-7fe9-4f90-a157-12b52bea3fba\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5f8cc120-7fe9-4f90-a157-12b52bea3fba')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-5f8cc120-7fe9-4f90-a157-12b52bea3fba button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                              biased  \\\n",
       "0  chloroform \"the molecular lifesaver\" an articl...   \n",
       "1  the free software gnu classpath project is onl...   \n",
       "2  other campaigners, especially the controversia...   \n",
       "3  vocalist rob halford's performance is consider...   \n",
       "4  the proud general is a chinese animated featur...   \n",
       "\n",
       "                                             neutral  \\\n",
       "0  chloroform \"the molecular lifesaver\" an articl...   \n",
       "1  the free software gnu classpath project is par...   \n",
       "2  other campaigners, especially the british acti...   \n",
       "3  vocalist rob halford's performance is consider...   \n",
       "4  the proud general is a chinese animated featur...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [chloroform, ``, the, molecular, lifesaver, ''...   \n",
       "1  [the, free, software, gnu, classpath, project,...   \n",
       "2  [other, campaigners, ,, especially, the, contr...   \n",
       "3  [vocalist, rob, halford, 's, performance, is, ...   \n",
       "4  [the, proud, general, is, a, chinese, animated...   \n",
       "\n",
       "                                               flags  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_mcn[[\"biased\", \"neutral\", \"tokens\", \"flags\"]].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2424,
     "status": "ok",
     "timestamp": 1762718513388,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "c5pRNdGeMvwH",
    "outputId": "e2f6d166-a8d8-4187-a919-f226f395270c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /content/drive/MyDrive/CS4650/project/data/WNC/processed/mpqa_lexicon/biased_word_train.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "output_path = os.path.join(folder_path, \"data/WNC/processed/mpqa_lexicon/biased_word_train.csv\")\n",
    "output_dir = os.path.dirname(output_path)\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "df_mcn.to_csv(output_path, index=False)\n",
    "print(\"Saved:\", output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jpAz2w6NTZi"
   },
   "source": [
    "# Load Processed WNC (mpqa_lexicon flagged)\n",
    "\n",
    "path=data/WNC/processed/mpqa_lexicon/biased_word_train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "executionInfo": {
     "elapsed": 7937,
     "status": "ok",
     "timestamp": 1762735496563,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "AXuB4dFFNqmU",
    "outputId": "6eb3a23a-f0ed-4a62-9ad7-f5fee4911ea9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df\",\n  \"rows\": 53803,\n  \"fields\": [\n    {\n      \"column\": \"biased\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 53693,\n        \"samples\": [\n          \"breathtaking views of deuteronilus mensae on mars from mars express\",\n          \"william donald kelley, dds, ms (november 1, 1925 january 30, 2005) was an orthodontist who developed the kelley cancer therapy, an ineffective alternative cancer treatment based on the concepts that \\\"wrong foods (cause a) malignancy to grow, while proper foods (allow) ... body defenses to work\\\" defeating the cancer.\",\n          \"since the 19th century, there have been claims by many good scholars that bacon was the author of the works attributed to shakespeare.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"neutral\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 53705,\n        \"samples\": [\n          \"in turn, defensive reactions include an increased self-identification as \\\"muslims\\\", and adoption of islamic dress by women and \\\"islamic\\\" beards by men.\",\n          \"bosnian, bosanac) are a nation and people whose country is bosnia and herzegovina.\",\n          \"the rape and assault of christian, hindu, sikh and other minorities is reported to be prevalent in pakistan.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tokens\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 53682,\n        \"samples\": [\n          \"['dumouriez', ',', 'intent', 'on', 'invading', 'the', 'netherlands', ',', 'advanced', 'late', 'in', 'the', 'season', 'and', 'surprised', 'the', 'austrians', 'with', 'enormously', 'superior', 'forces', '.']\",\n          \"['unless', 'if', 'the', 'last', 'voter', 'complies', 'before', 'april', '23rd', ',', '2018', ',', 'the', 'internet', 'freedom', 'rights', 'will', 'be', 'lost', 'forever', '.']\",\n          \"['mccrea', 'possesses', 'a', 'distinctive', ',', 'deep', ',', 'and', 'often', 'monotonous', 'singing', 'voice', '.']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"flags\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 13418,\n        \"samples\": [\n          \"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\",\n          \"[0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\",\n          \"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-57f4b8cd-d2bc-4326-bd50-32aef5dccfdb\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>biased</th>\n",
       "      <th>neutral</th>\n",
       "      <th>tokens</th>\n",
       "      <th>flags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chloroform \"the molecular lifesaver\" an articl...</td>\n",
       "      <td>chloroform \"the molecular lifesaver\" an articl...</td>\n",
       "      <td>['chloroform', '``', 'the', 'molecular', 'life...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the free software gnu classpath project is onl...</td>\n",
       "      <td>the free software gnu classpath project is par...</td>\n",
       "      <td>['the', 'free', 'software', 'gnu', 'classpath'...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>other campaigners, especially the controversia...</td>\n",
       "      <td>other campaigners, especially the british acti...</td>\n",
       "      <td>['other', 'campaigners', ',', 'especially', 't...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vocalist rob halford's performance is consider...</td>\n",
       "      <td>vocalist rob halford's performance is consider...</td>\n",
       "      <td>['vocalist', 'rob', 'halford', \"'s\", 'performa...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the proud general is a chinese animated featur...</td>\n",
       "      <td>the proud general is a chinese animated featur...</td>\n",
       "      <td>['the', 'proud', 'general', 'is', 'a', 'chines...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53798</th>\n",
       "      <td>the national lawyers guild is a progressive /l...</td>\n",
       "      <td>the national lawyers guild is a progressive ba...</td>\n",
       "      <td>['the', 'national', 'lawyers', 'guild', 'is', ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53799</th>\n",
       "      <td>a plan to redevelop the old tiger stadium site...</td>\n",
       "      <td>a plan to redevelop the old tiger stadium site...</td>\n",
       "      <td>['a', 'plan', 'to', 'redevelop', 'the', 'old',...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53800</th>\n",
       "      <td>instrumentally, lifeson is regarded as a virtu...</td>\n",
       "      <td>instrumentally, lifeson is regarded as a guita...</td>\n",
       "      <td>['instrumentally', ',', 'lifeson', 'is', 'rega...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53801</th>\n",
       "      <td>flynt joined the us army in 1958 at only fifte...</td>\n",
       "      <td>flynt joined the us army in 1958 at only fifte...</td>\n",
       "      <td>['flynt', 'joined', 'the', 'us', 'army', 'in',...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53802</th>\n",
       "      <td>today, mtv2 airs some music videos, other musi...</td>\n",
       "      <td>today, mtv2 airs a selection music videos, oth...</td>\n",
       "      <td>['today', ',', 'mtv2', 'airs', 'some', 'music'...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53803 rows  4 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-57f4b8cd-d2bc-4326-bd50-32aef5dccfdb')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-57f4b8cd-d2bc-4326-bd50-32aef5dccfdb button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-57f4b8cd-d2bc-4326-bd50-32aef5dccfdb');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-89d110e9-b5d5-4f32-9deb-ed75627a1a1c\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-89d110e9-b5d5-4f32-9deb-ed75627a1a1c')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-89d110e9-b5d5-4f32-9deb-ed75627a1a1c button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "  <div id=\"id_54b54da7-c277-47c8-bded-cbba57c5d0ec\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_54b54da7-c277-47c8-bded-cbba57c5d0ec button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('df');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                                  biased  \\\n",
       "0      chloroform \"the molecular lifesaver\" an articl...   \n",
       "1      the free software gnu classpath project is onl...   \n",
       "2      other campaigners, especially the controversia...   \n",
       "3      vocalist rob halford's performance is consider...   \n",
       "4      the proud general is a chinese animated featur...   \n",
       "...                                                  ...   \n",
       "53798  the national lawyers guild is a progressive /l...   \n",
       "53799  a plan to redevelop the old tiger stadium site...   \n",
       "53800  instrumentally, lifeson is regarded as a virtu...   \n",
       "53801  flynt joined the us army in 1958 at only fifte...   \n",
       "53802  today, mtv2 airs some music videos, other musi...   \n",
       "\n",
       "                                                 neutral  \\\n",
       "0      chloroform \"the molecular lifesaver\" an articl...   \n",
       "1      the free software gnu classpath project is par...   \n",
       "2      other campaigners, especially the british acti...   \n",
       "3      vocalist rob halford's performance is consider...   \n",
       "4      the proud general is a chinese animated featur...   \n",
       "...                                                  ...   \n",
       "53798  the national lawyers guild is a progressive ba...   \n",
       "53799  a plan to redevelop the old tiger stadium site...   \n",
       "53800  instrumentally, lifeson is regarded as a guita...   \n",
       "53801  flynt joined the us army in 1958 at only fifte...   \n",
       "53802  today, mtv2 airs a selection music videos, oth...   \n",
       "\n",
       "                                                  tokens  \\\n",
       "0      ['chloroform', '``', 'the', 'molecular', 'life...   \n",
       "1      ['the', 'free', 'software', 'gnu', 'classpath'...   \n",
       "2      ['other', 'campaigners', ',', 'especially', 't...   \n",
       "3      ['vocalist', 'rob', 'halford', \"'s\", 'performa...   \n",
       "4      ['the', 'proud', 'general', 'is', 'a', 'chines...   \n",
       "...                                                  ...   \n",
       "53798  ['the', 'national', 'lawyers', 'guild', 'is', ...   \n",
       "53799  ['a', 'plan', 'to', 'redevelop', 'the', 'old',...   \n",
       "53800  ['instrumentally', ',', 'lifeson', 'is', 'rega...   \n",
       "53801  ['flynt', 'joined', 'the', 'us', 'army', 'in',...   \n",
       "53802  ['today', ',', 'mtv2', 'airs', 'some', 'music'...   \n",
       "\n",
       "                                                   flags  \n",
       "0      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, ...  \n",
       "1      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2      [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4      [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "...                                                  ...  \n",
       "53798  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "53799  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "53800  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "53801   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "53802  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[53803 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mcn_processed_path=os.path.join(folder_path, \"data/WNC/processed/mpqa_lexicon/biased_word_train.csv\")\n",
    "df=pd.read_csv(mcn_processed_path)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 770
    },
    "executionInfo": {
     "elapsed": 5454,
     "status": "ok",
     "timestamp": 1762735502020,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "7wlzPzLkSv6p",
    "outputId": "5906ed84-2a15-404d-85cd-0ce01facaff2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df\",\n  \"rows\": 53803,\n  \"fields\": [\n    {\n      \"column\": \"biased\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 53693,\n        \"samples\": [\n          \"breathtaking views of deuteronilus mensae on mars from mars express\",\n          \"william donald kelley, dds, ms (november 1, 1925 january 30, 2005) was an orthodontist who developed the kelley cancer therapy, an ineffective alternative cancer treatment based on the concepts that \\\"wrong foods (cause a) malignancy to grow, while proper foods (allow) ... body defenses to work\\\" defeating the cancer.\",\n          \"since the 19th century, there have been claims by many good scholars that bacon was the author of the works attributed to shakespeare.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"neutral\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 53705,\n        \"samples\": [\n          \"in turn, defensive reactions include an increased self-identification as \\\"muslims\\\", and adoption of islamic dress by women and \\\"islamic\\\" beards by men.\",\n          \"bosnian, bosanac) are a nation and people whose country is bosnia and herzegovina.\",\n          \"the rape and assault of christian, hindu, sikh and other minorities is reported to be prevalent in pakistan.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tokens\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 53682,\n        \"samples\": [\n          \"['dumouriez', ',', 'intent', 'on', 'invading', 'the', 'netherlands', ',', 'advanced', 'late', 'in', 'the', 'season', 'and', 'surprised', 'the', 'austrians', 'with', 'enormously', 'superior', 'forces', '.']\",\n          \"['unless', 'if', 'the', 'last', 'voter', 'complies', 'before', 'april', '23rd', ',', '2018', ',', 'the', 'internet', 'freedom', 'rights', 'will', 'be', 'lost', 'forever', '.']\",\n          \"['mccrea', 'possesses', 'a', 'distinctive', ',', 'deep', ',', 'and', 'often', 'monotonous', 'singing', 'voice', '.']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"flags\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 13418,\n        \"samples\": [\n          \"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\",\n          \"[0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\",\n          \"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"input_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 53682,\n        \"samples\": [\n          \"neutralize: dumouriez[0] ,[0] intent[0] on[0] invading[0] the[0] netherlands[0] ,[0] advanced[0] late[0] in[0] the[0] season[0] and[0] surprised[0] the[0] austrians[0] with[0] enormously[1] superior[0] forces[0] .[0]\",\n          \"neutralize: unless[0] if[0] the[0] last[0] voter[0] complies[0] before[0] april[0] 23rd[0] ,[0] 2018[0] ,[0] the[0] internet[0] freedom[0] rights[0] will[1] be[0] lost[0] forever[0] .[0]\",\n          \"neutralize: mccrea[0] possesses[0] a[0] distinctive[0] ,[0] deep[0] ,[0] and[0] often[0] monotonous[1] singing[0] voice[0] .[0]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 53705,\n        \"samples\": [\n          \"in turn, defensive reactions include an increased self-identification as \\\"muslims\\\", and adoption of islamic dress by women and \\\"islamic\\\" beards by men.\",\n          \"bosnian, bosanac) are a nation and people whose country is bosnia and herzegovina.\",\n          \"the rape and assault of christian, hindu, sikh and other minorities is reported to be prevalent in pakistan.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-99dcc026-2ebc-442c-924c-f6db12262c1a\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>biased</th>\n",
       "      <th>neutral</th>\n",
       "      <th>tokens</th>\n",
       "      <th>flags</th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chloroform \"the molecular lifesaver\" an articl...</td>\n",
       "      <td>chloroform \"the molecular lifesaver\" an articl...</td>\n",
       "      <td>['chloroform', '``', 'the', 'molecular', 'life...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, ...</td>\n",
       "      <td>neutralize: chloroform[0] ``[0] the[0] molecul...</td>\n",
       "      <td>chloroform \"the molecular lifesaver\" an articl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the free software gnu classpath project is onl...</td>\n",
       "      <td>the free software gnu classpath project is par...</td>\n",
       "      <td>['the', 'free', 'software', 'gnu', 'classpath'...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>neutralize: the[0] free[0] software[0] gnu[0] ...</td>\n",
       "      <td>the free software gnu classpath project is par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>other campaigners, especially the controversia...</td>\n",
       "      <td>other campaigners, especially the british acti...</td>\n",
       "      <td>['other', 'campaigners', ',', 'especially', 't...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>neutralize: other[0] campaigners[0] ,[0] espec...</td>\n",
       "      <td>other campaigners, especially the british acti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vocalist rob halford's performance is consider...</td>\n",
       "      <td>vocalist rob halford's performance is consider...</td>\n",
       "      <td>['vocalist', 'rob', 'halford', \"'s\", 'performa...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>neutralize: vocalist[0] rob[0] halford[0] \"s\"[...</td>\n",
       "      <td>vocalist rob halford's performance is consider...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the proud general is a chinese animated featur...</td>\n",
       "      <td>the proud general is a chinese animated featur...</td>\n",
       "      <td>['the', 'proud', 'general', 'is', 'a', 'chines...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>neutralize: the[0] proud[1] general[0] is[0] a...</td>\n",
       "      <td>the proud general is a chinese animated featur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53798</th>\n",
       "      <td>the national lawyers guild is a progressive /l...</td>\n",
       "      <td>the national lawyers guild is a progressive ba...</td>\n",
       "      <td>['the', 'national', 'lawyers', 'guild', 'is', ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>neutralize: the[0] national[0] lawyers[0] guil...</td>\n",
       "      <td>the national lawyers guild is a progressive ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53799</th>\n",
       "      <td>a plan to redevelop the old tiger stadium site...</td>\n",
       "      <td>a plan to redevelop the old tiger stadium site...</td>\n",
       "      <td>['a', 'plan', 'to', 'redevelop', 'the', 'old',...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>neutralize: a[0] plan[0] to[0] redevelop[0] th...</td>\n",
       "      <td>a plan to redevelop the old tiger stadium site...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53800</th>\n",
       "      <td>instrumentally, lifeson is regarded as a virtu...</td>\n",
       "      <td>instrumentally, lifeson is regarded as a guita...</td>\n",
       "      <td>['instrumentally', ',', 'lifeson', 'is', 'rega...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>neutralize: instrumentally[0] ,[0] lifeson[0] ...</td>\n",
       "      <td>instrumentally, lifeson is regarded as a guita...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53801</th>\n",
       "      <td>flynt joined the us army in 1958 at only fifte...</td>\n",
       "      <td>flynt joined the us army in 1958 at only fifte...</td>\n",
       "      <td>['flynt', 'joined', 'the', 'us', 'army', 'in',...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>neutralize: flynt[0] joined[0] the[0] us[0] ar...</td>\n",
       "      <td>flynt joined the us army in 1958 at only fifte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53802</th>\n",
       "      <td>today, mtv2 airs some music videos, other musi...</td>\n",
       "      <td>today, mtv2 airs a selection music videos, oth...</td>\n",
       "      <td>['today', ',', 'mtv2', 'airs', 'some', 'music'...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>neutralize: today[0] ,[0] mtv2[0] airs[0] some...</td>\n",
       "      <td>today, mtv2 airs a selection music videos, oth...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53803 rows  6 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-99dcc026-2ebc-442c-924c-f6db12262c1a')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-99dcc026-2ebc-442c-924c-f6db12262c1a button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-99dcc026-2ebc-442c-924c-f6db12262c1a');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-e0299555-3696-4f58-9d62-05531945305c\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e0299555-3696-4f58-9d62-05531945305c')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-e0299555-3696-4f58-9d62-05531945305c button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "  <div id=\"id_58fa7f56-66e5-4ff4-9f17-89d8534004a9\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_58fa7f56-66e5-4ff4-9f17-89d8534004a9 button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('df');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                                  biased  \\\n",
       "0      chloroform \"the molecular lifesaver\" an articl...   \n",
       "1      the free software gnu classpath project is onl...   \n",
       "2      other campaigners, especially the controversia...   \n",
       "3      vocalist rob halford's performance is consider...   \n",
       "4      the proud general is a chinese animated featur...   \n",
       "...                                                  ...   \n",
       "53798  the national lawyers guild is a progressive /l...   \n",
       "53799  a plan to redevelop the old tiger stadium site...   \n",
       "53800  instrumentally, lifeson is regarded as a virtu...   \n",
       "53801  flynt joined the us army in 1958 at only fifte...   \n",
       "53802  today, mtv2 airs some music videos, other musi...   \n",
       "\n",
       "                                                 neutral  \\\n",
       "0      chloroform \"the molecular lifesaver\" an articl...   \n",
       "1      the free software gnu classpath project is par...   \n",
       "2      other campaigners, especially the british acti...   \n",
       "3      vocalist rob halford's performance is consider...   \n",
       "4      the proud general is a chinese animated featur...   \n",
       "...                                                  ...   \n",
       "53798  the national lawyers guild is a progressive ba...   \n",
       "53799  a plan to redevelop the old tiger stadium site...   \n",
       "53800  instrumentally, lifeson is regarded as a guita...   \n",
       "53801  flynt joined the us army in 1958 at only fifte...   \n",
       "53802  today, mtv2 airs a selection music videos, oth...   \n",
       "\n",
       "                                                  tokens  \\\n",
       "0      ['chloroform', '``', 'the', 'molecular', 'life...   \n",
       "1      ['the', 'free', 'software', 'gnu', 'classpath'...   \n",
       "2      ['other', 'campaigners', ',', 'especially', 't...   \n",
       "3      ['vocalist', 'rob', 'halford', \"'s\", 'performa...   \n",
       "4      ['the', 'proud', 'general', 'is', 'a', 'chines...   \n",
       "...                                                  ...   \n",
       "53798  ['the', 'national', 'lawyers', 'guild', 'is', ...   \n",
       "53799  ['a', 'plan', 'to', 'redevelop', 'the', 'old',...   \n",
       "53800  ['instrumentally', ',', 'lifeson', 'is', 'rega...   \n",
       "53801  ['flynt', 'joined', 'the', 'us', 'army', 'in',...   \n",
       "53802  ['today', ',', 'mtv2', 'airs', 'some', 'music'...   \n",
       "\n",
       "                                                   flags  \\\n",
       "0      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, ...   \n",
       "1      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4      [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                  ...   \n",
       "53798  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "53799  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "53800  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "53801   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "53802  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                              input_text  \\\n",
       "0      neutralize: chloroform[0] ``[0] the[0] molecul...   \n",
       "1      neutralize: the[0] free[0] software[0] gnu[0] ...   \n",
       "2      neutralize: other[0] campaigners[0] ,[0] espec...   \n",
       "3      neutralize: vocalist[0] rob[0] halford[0] \"s\"[...   \n",
       "4      neutralize: the[0] proud[1] general[0] is[0] a...   \n",
       "...                                                  ...   \n",
       "53798  neutralize: the[0] national[0] lawyers[0] guil...   \n",
       "53799  neutralize: a[0] plan[0] to[0] redevelop[0] th...   \n",
       "53800  neutralize: instrumentally[0] ,[0] lifeson[0] ...   \n",
       "53801  neutralize: flynt[0] joined[0] the[0] us[0] ar...   \n",
       "53802  neutralize: today[0] ,[0] mtv2[0] airs[0] some...   \n",
       "\n",
       "                                             target_text  \n",
       "0      chloroform \"the molecular lifesaver\" an articl...  \n",
       "1      the free software gnu classpath project is par...  \n",
       "2      other campaigners, especially the british acti...  \n",
       "3      vocalist rob halford's performance is consider...  \n",
       "4      the proud general is a chinese animated featur...  \n",
       "...                                                  ...  \n",
       "53798  the national lawyers guild is a progressive ba...  \n",
       "53799  a plan to redevelop the old tiger stadium site...  \n",
       "53800  instrumentally, lifeson is regarded as a guita...  \n",
       "53801  flynt joined the us army in 1958 at only fifte...  \n",
       "53802  today, mtv2 airs a selection music videos, oth...  \n",
       "\n",
       "[53803 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def format_input(tokens, flags):\n",
    "    words = tokens.strip(\"[]\").split(\", \")\n",
    "    flags = flags.strip(\"[]\").split(\", \")\n",
    "    words = [w.replace(\"'\", \"\") for w in words]\n",
    "    combined = \" \".join([f\"{w}[{f}]\" for w, f in zip(words, flags)])\n",
    "    return \"neutralize: \" + combined\n",
    "\n",
    "df[\"input_text\"] = [format_input(t, f) for t, f in zip(df[\"tokens\"], df[\"flags\"])]\n",
    "df[\"target_text\"] = df[\"neutral\"]\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2348,
     "status": "ok",
     "timestamp": 1762735504344,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "JxrgD-N5YbBG",
    "outputId": "55ce35d1-ef35-4825-d939-2022e77b445c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /content/drive/MyDrive/CS4650/project/data/WNC/processed/mpqa_lexicon/input_target_pair_biased_word_train.csv\n"
     ]
    }
   ],
   "source": [
    "output_path = os.path.join(folder_path, \"data/WNC/processed/mpqa_lexicon/input_target_pair_biased_word_train.csv\")\n",
    "output_dir = os.path.dirname(output_path)\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "df[[\"input_text\", \"target_text\"]].to_csv(output_path, index=False)\n",
    "print(\"Saved:\", output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dV1gTJrpXzKv"
   },
   "source": [
    "# fine tune model (cross entropy loss & classifier loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i0BF5c8dTEXk"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437,
     "referenced_widgets": [
      "c5f0c23f7c8e4068b1bce8b650f21e04",
      "5193b7782e04421585ab62e6525eec97",
      "d3beaf4480444ef98c51800b26f6da85",
      "63123de74eda4806a2141a0592dce588",
      "7bcdb3cbc63749649ad531b769605009",
      "61562e24cca94e4a9ca4b86c318b0f0b",
      "45f4dad1bfb94dcfbd29a69b869c0864",
      "dd8f3c2aca8342d7827072147cda6623",
      "41c2ab2ac1074d168301ed892df14ace",
      "100393dabcac4e608d69753bffac2257",
      "ad11629f387047b4a4b60d46e9f21582",
      "be68f3d800544720b51e3e1cfc8ed9df",
      "62980aa1a2f34bc398649d4af689bf2c",
      "c15f1387de0a46ac88aa0a1158ffb857",
      "d30baf8f0e094f359a983cb675f0b758",
      "ac795aea753540c1a81cadce33dceba2",
      "32f171f700be4dbbb95619e5e9d1e889",
      "8e7d3cafc5ee45399f9b98d3a2164124",
      "6c37466dd08c4e069fac4dad0759c44d",
      "40fddb9cbf1842638691e9294ab42ed6",
      "06c14377b1624679a17e57248c8be499",
      "69ebd0e06e7d46d08d8f033c2721eeb0",
      "640a14f4409e4611b77d784f338fa648",
      "ec66a4eda9774f80be9c716ab6a282b9",
      "d7aa12e1a6e74a9dad9a67bf614b2084",
      "f78c5037438a4940b72dd463f2f97cc2",
      "1b5c7716a9844ae0964f9ec264b7c0e0",
      "50fa33f38aa64171aa7584c2835ffdce",
      "d43eec1f7b314b2a8f1e7ca16724512a",
      "24a7efe1bc0b497380f0df58f1798b54",
      "f33f5894a82f4771a9fac7723e893678",
      "881a33352aa7466e97cd9071633fa330",
      "6c3aaa63ac234eed91b8bbfc47340fe7",
      "3cbf3e6b7ce548109c383fa2be7d0ece",
      "0df27d8087984b58883db1bce6bdb24a",
      "c661b5b6e80a4547b9358620141d356b",
      "5c27f5c86a55417dbc599e81a3b8aa31",
      "2d5190bc9a27473f97636e25577d93a5",
      "c493c31533a14204a6edc757ce2e0c06",
      "56a48431e4c541d2b7fe0c47f8a03c25",
      "9dcfc5d4066045e8958863d716241ac2",
      "ec98f7abc67a452f869524e15546d462",
      "6d40ec6e27c040be9f77d6c2725f23e9",
      "6d66a7c5716e4b57b07cd90c3cfb66cf",
      "c2435c0e93234f9cacfb0d87f7f15cfc",
      "42ae9d039f0a413c94c7980c330940e7",
      "3dfd3f2b375e45b5b0e9f75eb5d7a105",
      "7b6323389024427a8817d365d5307315",
      "c05117d4662a4315990ff3c411da953e",
      "b3fb94d69b5e4f649fb779c4c2ebad92",
      "be5051318cba4400ac017516409c2df7",
      "6c314a90029440bebc7a36aa7f63a2a7",
      "6f575cc95c3c4724a52a938b409dc4bc",
      "2619875c75d64ea8ab1fecb4db03b4fa",
      "7b8f7726e22b4d7c83ca959e6707641d",
      "55ac48e549534af08d774986eeab522e",
      "5960c93f7b0d4c6787f965f7107ff93f",
      "35d675dc62ee47f5b6f49ab37c4103ed",
      "afb829b5c7c442528ab2c888ab7c4680",
      "af9b3d1bb4eb43d581ba72bc65deb03a",
      "f4394ab82f354fa4a9bbacfd160e8cb7",
      "d83c7b46881a4f25b9dc9b87eb71a88d",
      "63fede34b64c4444a80da131daa85877",
      "f3f8ea85f0654d33b73de468a9bbef17",
      "2f912d085af142a1930d92544443ac24",
      "6f76f5cc6da7476e97290f197978bbf6"
     ]
    },
    "executionInfo": {
     "elapsed": 19568,
     "status": "ok",
     "timestamp": 1763342859035,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "BMRDMgpqX-ch",
    "outputId": "bb6f8a4c-029e-4f63-8f22-971ddc9618c7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5f0c23f7c8e4068b1bce8b650f21e04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be68f3d800544720b51e3e1cfc8ed9df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "640a14f4409e4611b77d784f338fa648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cbf3e6b7ce548109c383fa2be7d0ece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2435c0e93234f9cacfb0d87f7f15cfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55ac48e549534af08d774986eeab522e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/53803 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_text', 'target_text', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 53803\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "dataset = load_dataset(\"csv\", data_files=os.path.join(folder_path, \"data/WNC/processed/mpqa_lexicon/input_target_pair_biased_word_train.csv\"))\n",
    "\n",
    "max_length = 128\n",
    "def preprocess(batch):\n",
    "    model_inputs = tokenizer(batch[\"input_text\"], max_length=max_length, truncation=True, padding=True)\n",
    "    labels = tokenizer(batch[\"target_text\"], max_length=max_length, truncation=True, padding=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess, batched=True)\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dADuoOJnZsjd"
   },
   "outputs": [],
   "source": [
    "# @title loss function\n",
    "classifier_name = \"GroNLP/mdebertav3-subjectivity-english\"\n",
    "clf_tokenizer = AutoTokenizer.from_pretrained(classifier_name)\n",
    "clf_model = AutoModelForSequenceClassification.from_pretrained(classifier_name)\n",
    "clf_model.eval()\n",
    "for p in clf_model.parameters():\n",
    "  p.requires_grad = False\n",
    "\n",
    "class DualLossTrainer(Seq2SeqTrainer):\n",
    "  def __init__(self, classifier, clf_tokenizer, lambda_obj=0.2, **kwargs):\n",
    "      super().__init__(**kwargs)\n",
    "      self.classifier = classifier\n",
    "      self.clf_tokenizer = clf_tokenizer\n",
    "      self.lambda_obj = lambda_obj\n",
    "      self.ce_loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "      # Move classifier and tokenizer to the same device as the model\n",
    "      self.classifier.to(self.model.device)\n",
    "      # The tokenizer itself doesn't need a device, but its outputs should be on the model's device.\n",
    "\n",
    "  def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "      # labels = inputs.pop(\"labels\")\n",
    "      # print(inputs[\"labels\"][0][:20])\n",
    "\n",
    "      outputs = model(**inputs)\n",
    "      logits = outputs.logits\n",
    "      labels=inputs[\"labels\"]\n",
    "      # print(logits)\n",
    "\n",
    "      # if torch.isnan(logits).any() or torch.isinf(logits).any():\n",
    "      #   print(\" Logits contain NaN or Inf before CE Loss! Stopping.\")\n",
    "      #   return total_loss # Or raise an error\n",
    "\n",
    "      # ---------- Cross-entropy loss ----------\n",
    "      ce_loss = self.ce_loss_fn(\n",
    "        logits.view(-1, logits.size(-1)),\n",
    "        labels.view(-1)\n",
    "      )\n",
    "\n",
    "\n",
    "      # ---------- Objectivity loss ----------\n",
    "      # Generate sentences with greedy decoding (no gradient)\n",
    "      with torch.no_grad():\n",
    "        generated = torch.argmax(logits, dim=-1)\n",
    "        # decoded = self.tokenizer.batch_decode(\n",
    "        #     generated, skip_special_tokens=True\n",
    "        # )\n",
    "        decoded = self.processing_class.batch_decode(\n",
    "          generated, skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "\n",
    "        # Run classifier on generated text\n",
    "        scores = []\n",
    "        for text in decoded:\n",
    "            tokens = self.clf_tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=128,\n",
    "            ).to(model.device)\n",
    "            pred = self.classifier(**tokens).logits.softmax(dim=-1)\n",
    "            # index 0=objective, index1=subjective\n",
    "            score = pred[0][0]   # probability of \"objective\"\n",
    "            scores.append(score)\n",
    "      scores = torch.stack(scores)\n",
    "      obj_loss = (1 - scores).mean()\n",
    "\n",
    "      total_loss = (1 - self.lambda_obj) * ce_loss + self.lambda_obj * obj_loss\n",
    "      print(f\"CE: {ce_loss.item():.4f}, OBJ: {obj_loss.item():.4f}, TOTAL: {total_loss.item():.4f}\")\n",
    "      return (total_loss, outputs) if return_outputs else total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JSVftj1yYHa6"
   },
   "outputs": [],
   "source": [
    "\n",
    "# @title model & args\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=os.path.join(folder_path, \"model_checkpoints/wnc-mpqa-flan_t5-2loss-11_09\"),\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    # eval_steps=1000,\n",
    "    learning_rate=1e-5, #1e-5 to 5e-5\n",
    "    num_train_epochs=1, #3\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    logging_dir=os.path.join(folder_path, \"logs/wnc-mpqa-flan_t5-2loss-11_09\"),\n",
    ")\n",
    "\n",
    "trainer = DualLossTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    classifier=clf_model,\n",
    "    clf_tokenizer=clf_tokenizer,\n",
    "    lambda_obj=0.2,  # 20 % weight on objectivity loss\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"train\"].select(range(500)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3141863,
     "status": "ok",
     "timestamp": 1762742138425,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "TFVwB9yZi_F7",
    "outputId": "a6a1c070-7288-42fe-d29b-1bff77ed57fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE: 34.1808, OBJ: 0.6277, TOTAL: 27.4702\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6726' max='6726' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6726/6726 52:20, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.124200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.393300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.345300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.328700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.344400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.339900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.326700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.324400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.320600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.332800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.333600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.311300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.319100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43m 5000 \u001b[0m\n",
      "CE: 0.4332, OBJ: 0.3204, TOTAL: 0.4107\n",
      "CE: 0.4072, OBJ: 0.3866, TOTAL: 0.4031\n",
      "CE: 0.5279, OBJ: 0.2391, TOTAL: 0.4702\n",
      "CE: 0.3595, OBJ: 0.2950, TOTAL: 0.3466\n",
      "CE: 0.2764, OBJ: 0.3480, TOTAL: 0.2908\n",
      "CE: 0.1843, OBJ: 0.2926, TOTAL: 0.2060\n",
      "CE: 0.4434, OBJ: 0.2181, TOTAL: 0.3984\n",
      "CE: 0.4825, OBJ: 0.3791, TOTAL: 0.4618\n",
      "CE: 0.6825, OBJ: 0.4539, TOTAL: 0.6367\n",
      "CE: 0.3524, OBJ: 0.2513, TOTAL: 0.3322\n",
      "CE: 0.4400, OBJ: 0.4324, TOTAL: 0.4384\n",
      "CE: 0.2929, OBJ: 0.3152, TOTAL: 0.2973\n",
      "CE: 0.5727, OBJ: 0.3388, TOTAL: 0.5259\n",
      "CE: 0.3536, OBJ: 0.3199, TOTAL: 0.3469\n",
      "CE: 0.2697, OBJ: 0.3119, TOTAL: 0.2781\n",
      "CE: 0.5362, OBJ: 0.3438, TOTAL: 0.4977\n",
      "CE: 0.4338, OBJ: 0.3346, TOTAL: 0.4139\n",
      "CE: 0.2177, OBJ: 0.3926, TOTAL: 0.2527\n",
      "CE: 0.0895, OBJ: 0.3264, TOTAL: 0.1369\n",
      "CE: 0.2737, OBJ: 0.3391, TOTAL: 0.2868\n",
      "CE: 0.1591, OBJ: 0.4363, TOTAL: 0.2146\n",
      "CE: 0.1503, OBJ: 0.3521, TOTAL: 0.1907\n",
      "CE: 0.3289, OBJ: 0.3792, TOTAL: 0.3390\n",
      "CE: 0.3858, OBJ: 0.3257, TOTAL: 0.3738\n",
      "CE: 0.3035, OBJ: 0.3174, TOTAL: 0.3062\n",
      "CE: 0.5477, OBJ: 0.2319, TOTAL: 0.4846\n",
      "CE: 0.3268, OBJ: 0.2976, TOTAL: 0.3209\n",
      "CE: 0.2990, OBJ: 0.3527, TOTAL: 0.3097\n",
      "CE: 0.5937, OBJ: 0.2878, TOTAL: 0.5325\n",
      "CE: 0.2598, OBJ: 0.3603, TOTAL: 0.2799\n",
      "CE: 0.2973, OBJ: 0.2081, TOTAL: 0.2795\n",
      "CE: 0.3448, OBJ: 0.2954, TOTAL: 0.3350\n",
      "CE: 0.3048, OBJ: 0.4636, TOTAL: 0.3365\n",
      "CE: 0.2374, OBJ: 0.1757, TOTAL: 0.2250\n",
      "CE: 0.2981, OBJ: 0.4193, TOTAL: 0.3223\n",
      "CE: 0.1798, OBJ: 0.2895, TOTAL: 0.2017\n",
      "CE: 0.1662, OBJ: 0.3858, TOTAL: 0.2101\n",
      "CE: 0.1685, OBJ: 0.2777, TOTAL: 0.1903\n",
      "CE: 0.2686, OBJ: 0.4213, TOTAL: 0.2992\n",
      "CE: 0.2630, OBJ: 0.3554, TOTAL: 0.2815\n",
      "CE: 0.1798, OBJ: 0.2072, TOTAL: 0.1853\n",
      "CE: 0.1180, OBJ: 0.1939, TOTAL: 0.1332\n",
      "CE: 0.2654, OBJ: 0.3227, TOTAL: 0.2768\n",
      "CE: 0.2632, OBJ: 0.3165, TOTAL: 0.2739\n",
      "CE: 0.5152, OBJ: 0.3834, TOTAL: 0.4888\n",
      "CE: 0.4667, OBJ: 0.3359, TOTAL: 0.4406\n",
      "CE: 0.5354, OBJ: 0.3496, TOTAL: 0.4982\n",
      "CE: 0.1913, OBJ: 0.3587, TOTAL: 0.2248\n",
      "CE: 0.4814, OBJ: 0.2890, TOTAL: 0.4429\n",
      "CE: 0.3333, OBJ: 0.2800, TOTAL: 0.3226\n",
      "CE: 0.2005, OBJ: 0.1822, TOTAL: 0.1968\n",
      "CE: 0.3334, OBJ: 0.2606, TOTAL: 0.3189\n",
      "CE: 0.1113, OBJ: 0.2629, TOTAL: 0.1416\n",
      "CE: 0.4858, OBJ: 0.1606, TOTAL: 0.4208\n",
      "CE: 0.3552, OBJ: 0.3852, TOTAL: 0.3612\n",
      "CE: 0.3440, OBJ: 0.4012, TOTAL: 0.3554\n",
      "CE: 0.2105, OBJ: 0.2342, TOTAL: 0.2153\n",
      "CE: 0.1786, OBJ: 0.3470, TOTAL: 0.2123\n",
      "CE: 0.2267, OBJ: 0.3642, TOTAL: 0.2542\n",
      "CE: 0.1537, OBJ: 0.4346, TOTAL: 0.2099\n",
      "CE: 0.3293, OBJ: 0.3680, TOTAL: 0.3370\n",
      "CE: 0.4158, OBJ: 0.2774, TOTAL: 0.3881\n",
      "CE: 0.3650, OBJ: 0.2842, TOTAL: 0.3488\n",
      "CE: 0.5621, OBJ: 0.4284, TOTAL: 0.5353\n",
      "CE: 0.1862, OBJ: 0.2380, TOTAL: 0.1966\n",
      "CE: 0.4732, OBJ: 0.3942, TOTAL: 0.4574\n",
      "CE: 0.3265, OBJ: 0.2651, TOTAL: 0.3143\n",
      "CE: 0.4025, OBJ: 0.3760, TOTAL: 0.3972\n",
      "CE: 0.1474, OBJ: 0.2491, TOTAL: 0.1677\n",
      "CE: 0.5798, OBJ: 0.4627, TOTAL: 0.5564\n",
      "CE: 0.5008, OBJ: 0.2669, TOTAL: 0.4540\n",
      "CE: 0.1858, OBJ: 0.2047, TOTAL: 0.1896\n",
      "CE: 0.1447, OBJ: 0.3451, TOTAL: 0.1848\n",
      "CE: 0.2560, OBJ: 0.2839, TOTAL: 0.2616\n",
      "CE: 0.1782, OBJ: 0.3886, TOTAL: 0.2202\n",
      "CE: 0.3699, OBJ: 0.3873, TOTAL: 0.3734\n",
      "CE: 0.2740, OBJ: 0.1929, TOTAL: 0.2578\n",
      "CE: 0.1808, OBJ: 0.4934, TOTAL: 0.2433\n",
      "CE: 0.3200, OBJ: 0.3051, TOTAL: 0.3170\n",
      "CE: 0.5359, OBJ: 0.3758, TOTAL: 0.5039\n",
      "CE: 0.4413, OBJ: 0.3518, TOTAL: 0.4234\n",
      "CE: 0.3507, OBJ: 0.3865, TOTAL: 0.3579\n",
      "CE: 0.1103, OBJ: 0.3636, TOTAL: 0.1610\n",
      "CE: 0.4468, OBJ: 0.3594, TOTAL: 0.4293\n",
      "CE: 0.3043, OBJ: 0.3494, TOTAL: 0.3134\n",
      "CE: 0.0683, OBJ: 0.3622, TOTAL: 0.1271\n",
      "CE: 0.3384, OBJ: 0.4111, TOTAL: 0.3529\n",
      "CE: 0.2800, OBJ: 0.2599, TOTAL: 0.2760\n",
      "CE: 0.3417, OBJ: 0.2162, TOTAL: 0.3166\n",
      "CE: 0.3508, OBJ: 0.3321, TOTAL: 0.3470\n",
      "CE: 0.3215, OBJ: 0.4446, TOTAL: 0.3461\n",
      "CE: 0.4106, OBJ: 0.3104, TOTAL: 0.3905\n",
      "CE: 0.2163, OBJ: 0.3192, TOTAL: 0.2369\n",
      "CE: 0.3509, OBJ: 0.3480, TOTAL: 0.3503\n",
      "CE: 0.2022, OBJ: 0.2704, TOTAL: 0.2158\n",
      "CE: 0.1429, OBJ: 0.4168, TOTAL: 0.1976\n",
      "CE: 0.2742, OBJ: 0.3218, TOTAL: 0.2837\n",
      "CE: 0.2175, OBJ: 0.3437, TOTAL: 0.2427\n",
      "CE: 0.1273, OBJ: 0.3008, TOTAL: 0.1620\n",
      "CE: 0.3394, OBJ: 0.2255, TOTAL: 0.3166\n",
      "CE: 0.3090, OBJ: 0.3042, TOTAL: 0.3080\n",
      "CE: 0.1574, OBJ: 0.2417, TOTAL: 0.1742\n",
      "CE: 0.8173, OBJ: 0.2644, TOTAL: 0.7067\n",
      "CE: 0.4095, OBJ: 0.4310, TOTAL: 0.4138\n",
      "CE: 0.2950, OBJ: 0.2313, TOTAL: 0.2823\n",
      "CE: 0.2814, OBJ: 0.1288, TOTAL: 0.2509\n",
      "CE: 0.3754, OBJ: 0.3929, TOTAL: 0.3789\n",
      "CE: 0.3625, OBJ: 0.3650, TOTAL: 0.3630\n",
      "CE: 0.4731, OBJ: 0.4554, TOTAL: 0.4695\n",
      "CE: 0.2687, OBJ: 0.3164, TOTAL: 0.2782\n",
      "CE: 0.5149, OBJ: 0.2996, TOTAL: 0.4718\n",
      "CE: 0.5373, OBJ: 0.2836, TOTAL: 0.4865\n",
      "CE: 0.1663, OBJ: 0.2130, TOTAL: 0.1756\n",
      "CE: 0.2063, OBJ: 0.2905, TOTAL: 0.2232\n",
      "CE: 0.2912, OBJ: 0.3005, TOTAL: 0.2931\n",
      "CE: 0.1057, OBJ: 0.2596, TOTAL: 0.1365\n",
      "CE: 0.0702, OBJ: 0.2283, TOTAL: 0.1018\n",
      "CE: 0.3111, OBJ: 0.3280, TOTAL: 0.3145\n",
      "CE: 0.0902, OBJ: 0.2966, TOTAL: 0.1315\n",
      "CE: 0.4303, OBJ: 0.3587, TOTAL: 0.4160\n",
      "CE: 0.3699, OBJ: 0.3082, TOTAL: 0.3575\n",
      "CE: 0.2549, OBJ: 0.3764, TOTAL: 0.2792\n",
      "CE: 0.6422, OBJ: 0.5522, TOTAL: 0.6242\n",
      "CE: 0.2632, OBJ: 0.2591, TOTAL: 0.2624\n",
      "CE: 0.2501, OBJ: 0.3570, TOTAL: 0.2715\n",
      "CE: 0.2212, OBJ: 0.2282, TOTAL: 0.2226\n",
      "CE: 0.3572, OBJ: 0.2555, TOTAL: 0.3369\n",
      "CE: 0.4575, OBJ: 0.3472, TOTAL: 0.4354\n",
      "CE: 0.1989, OBJ: 0.3284, TOTAL: 0.2248\n",
      "CE: 0.6893, OBJ: 0.2691, TOTAL: 0.6053\n",
      "CE: 0.2145, OBJ: 0.4827, TOTAL: 0.2682\n",
      "CE: 0.1346, OBJ: 0.2700, TOTAL: 0.1617\n",
      "CE: 0.2122, OBJ: 0.2284, TOTAL: 0.2154\n",
      "CE: 0.1976, OBJ: 0.3595, TOTAL: 0.2300\n",
      "CE: 0.2564, OBJ: 0.3299, TOTAL: 0.2711\n",
      "CE: 0.3165, OBJ: 0.3463, TOTAL: 0.3225\n",
      "CE: 0.2833, OBJ: 0.2560, TOTAL: 0.2778\n",
      "CE: 0.7193, OBJ: 0.3763, TOTAL: 0.6507\n",
      "CE: 0.3153, OBJ: 0.2362, TOTAL: 0.2995\n",
      "CE: 0.3574, OBJ: 0.4092, TOTAL: 0.3678\n",
      "CE: 0.4869, OBJ: 0.3897, TOTAL: 0.4674\n",
      "CE: 0.3862, OBJ: 0.3583, TOTAL: 0.3806\n",
      "CE: 0.2154, OBJ: 0.2761, TOTAL: 0.2275\n",
      "CE: 0.4293, OBJ: 0.2266, TOTAL: 0.3887\n",
      "CE: 0.5618, OBJ: 0.2212, TOTAL: 0.4937\n",
      "CE: 0.3313, OBJ: 0.3404, TOTAL: 0.3331\n",
      "CE: 0.6510, OBJ: 0.3929, TOTAL: 0.5994\n",
      "CE: 0.2964, OBJ: 0.3249, TOTAL: 0.3021\n",
      "CE: 0.3198, OBJ: 0.3674, TOTAL: 0.3293\n",
      "CE: 0.3258, OBJ: 0.2614, TOTAL: 0.3129\n",
      "CE: 0.1909, OBJ: 0.2603, TOTAL: 0.2048\n",
      "CE: 0.4358, OBJ: 0.3253, TOTAL: 0.4137\n",
      "CE: 0.3479, OBJ: 0.2004, TOTAL: 0.3184\n",
      "CE: 0.2220, OBJ: 0.4089, TOTAL: 0.2594\n",
      "CE: 0.2777, OBJ: 0.2268, TOTAL: 0.2675\n",
      "CE: 0.0919, OBJ: 0.3234, TOTAL: 0.1382\n",
      "CE: 0.1316, OBJ: 0.3858, TOTAL: 0.1824\n",
      "CE: 0.4951, OBJ: 0.2609, TOTAL: 0.4483\n",
      "CE: 0.2062, OBJ: 0.1970, TOTAL: 0.2043\n",
      "CE: 0.3154, OBJ: 0.4323, TOTAL: 0.3388\n",
      "CE: 0.3489, OBJ: 0.4518, TOTAL: 0.3695\n",
      "CE: 0.4945, OBJ: 0.1970, TOTAL: 0.4350\n",
      "CE: 0.1913, OBJ: 0.4441, TOTAL: 0.2419\n",
      "CE: 0.3990, OBJ: 0.4603, TOTAL: 0.4112\n",
      "CE: 0.2421, OBJ: 0.4171, TOTAL: 0.2771\n",
      "CE: 0.1637, OBJ: 0.3234, TOTAL: 0.1957\n",
      "CE: 0.3723, OBJ: 0.4208, TOTAL: 0.3820\n",
      "CE: 0.2575, OBJ: 0.2032, TOTAL: 0.2466\n",
      "CE: 0.2456, OBJ: 0.2439, TOTAL: 0.2453\n",
      "CE: 0.2237, OBJ: 0.4195, TOTAL: 0.2629\n",
      "CE: 0.2723, OBJ: 0.4206, TOTAL: 0.3019\n",
      "CE: 0.6663, OBJ: 0.3470, TOTAL: 0.6025\n",
      "CE: 0.0925, OBJ: 0.1911, TOTAL: 0.1123\n",
      "CE: 0.7597, OBJ: 0.2421, TOTAL: 0.6562\n",
      "CE: 0.3199, OBJ: 0.3094, TOTAL: 0.3178\n",
      "CE: 0.2276, OBJ: 0.3298, TOTAL: 0.2480\n",
      "CE: 0.4475, OBJ: 0.3528, TOTAL: 0.4286\n",
      "CE: 0.3890, OBJ: 0.4702, TOTAL: 0.4052\n",
      "CE: 0.3008, OBJ: 0.2814, TOTAL: 0.2969\n",
      "CE: 0.1111, OBJ: 0.3296, TOTAL: 0.1548\n",
      "CE: 0.1798, OBJ: 0.2666, TOTAL: 0.1971\n",
      "CE: 0.2865, OBJ: 0.3629, TOTAL: 0.3018\n",
      "CE: 0.1955, OBJ: 0.3138, TOTAL: 0.2192\n",
      "CE: 0.1468, OBJ: 0.2727, TOTAL: 0.1720\n",
      "CE: 0.4199, OBJ: 0.2597, TOTAL: 0.3879\n",
      "CE: 0.4375, OBJ: 0.4256, TOTAL: 0.4351\n",
      "CE: 1.0868, OBJ: 0.3230, TOTAL: 0.9341\n",
      "CE: 0.1324, OBJ: 0.1949, TOTAL: 0.1449\n",
      "CE: 0.7729, OBJ: 0.3582, TOTAL: 0.6900\n",
      "CE: 0.5850, OBJ: 0.4879, TOTAL: 0.5656\n",
      "CE: 0.2764, OBJ: 0.2590, TOTAL: 0.2729\n",
      "CE: 0.2704, OBJ: 0.3855, TOTAL: 0.2934\n",
      "CE: 0.2085, OBJ: 0.3869, TOTAL: 0.2442\n",
      "CE: 0.0990, OBJ: 0.1779, TOTAL: 0.1148\n",
      "CE: 0.2386, OBJ: 0.2398, TOTAL: 0.2388\n",
      "CE: 0.1124, OBJ: 0.3292, TOTAL: 0.1558\n",
      "CE: 0.1856, OBJ: 0.3527, TOTAL: 0.2190\n",
      "CE: 0.1118, OBJ: 0.3596, TOTAL: 0.1614\n",
      "CE: 0.3482, OBJ: 0.1947, TOTAL: 0.3175\n",
      "CE: 0.0876, OBJ: 0.2826, TOTAL: 0.1266\n",
      "CE: 0.1322, OBJ: 0.3799, TOTAL: 0.1818\n",
      "CE: 0.2082, OBJ: 0.3371, TOTAL: 0.2339\n",
      "CE: 0.3434, OBJ: 0.3230, TOTAL: 0.3393\n",
      "CE: 0.3028, OBJ: 0.3684, TOTAL: 0.3159\n",
      "CE: 0.3933, OBJ: 0.3303, TOTAL: 0.3807\n",
      "CE: 0.3459, OBJ: 0.2782, TOTAL: 0.3324\n",
      "CE: 0.1616, OBJ: 0.4100, TOTAL: 0.2113\n",
      "CE: 0.7992, OBJ: 0.3458, TOTAL: 0.7085\n",
      "CE: 0.2942, OBJ: 0.4022, TOTAL: 0.3158\n",
      "CE: 0.4963, OBJ: 0.4036, TOTAL: 0.4778\n",
      "CE: 0.5679, OBJ: 0.3451, TOTAL: 0.5233\n",
      "CE: 0.2910, OBJ: 0.3097, TOTAL: 0.2947\n",
      "CE: 0.4596, OBJ: 0.3507, TOTAL: 0.4378\n",
      "CE: 0.2833, OBJ: 0.4101, TOTAL: 0.3087\n",
      "CE: 0.2624, OBJ: 0.3543, TOTAL: 0.2808\n",
      "CE: 0.2039, OBJ: 0.2500, TOTAL: 0.2131\n",
      "CE: 0.1834, OBJ: 0.4083, TOTAL: 0.2284\n",
      "CE: 0.2668, OBJ: 0.2504, TOTAL: 0.2635\n",
      "CE: 0.5002, OBJ: 0.2206, TOTAL: 0.4443\n",
      "CE: 0.2374, OBJ: 0.2376, TOTAL: 0.2374\n",
      "CE: 0.0952, OBJ: 0.2614, TOTAL: 0.1284\n",
      "CE: 0.4920, OBJ: 0.3790, TOTAL: 0.4694\n",
      "CE: 0.2900, OBJ: 0.3729, TOTAL: 0.3066\n",
      "CE: 0.3120, OBJ: 0.2302, TOTAL: 0.2956\n",
      "CE: 0.3903, OBJ: 0.3197, TOTAL: 0.3762\n",
      "CE: 0.3158, OBJ: 0.3450, TOTAL: 0.3217\n",
      "CE: 0.1981, OBJ: 0.2873, TOTAL: 0.2159\n",
      "CE: 0.2375, OBJ: 0.3507, TOTAL: 0.2601\n",
      "CE: 0.4034, OBJ: 0.3570, TOTAL: 0.3941\n",
      "CE: 0.2666, OBJ: 0.3167, TOTAL: 0.2766\n",
      "CE: 0.0800, OBJ: 0.1828, TOTAL: 0.1006\n",
      "CE: 0.3979, OBJ: 0.3609, TOTAL: 0.3905\n",
      "CE: 0.2172, OBJ: 0.2158, TOTAL: 0.2170\n",
      "CE: 0.3363, OBJ: 0.3601, TOTAL: 0.3410\n",
      "CE: 0.7039, OBJ: 0.3461, TOTAL: 0.6323\n",
      "CE: 0.1807, OBJ: 0.3605, TOTAL: 0.2167\n",
      "CE: 0.4312, OBJ: 0.1906, TOTAL: 0.3831\n",
      "CE: 0.5667, OBJ: 0.3794, TOTAL: 0.5293\n",
      "CE: 0.4741, OBJ: 0.2375, TOTAL: 0.4268\n",
      "CE: 0.3274, OBJ: 0.2576, TOTAL: 0.3135\n",
      "CE: 0.3022, OBJ: 0.4062, TOTAL: 0.3230\n",
      "CE: 0.2946, OBJ: 0.4225, TOTAL: 0.3202\n",
      "CE: 0.0847, OBJ: 0.2068, TOTAL: 0.1091\n",
      "CE: 0.2711, OBJ: 0.3488, TOTAL: 0.2866\n",
      "CE: 0.2107, OBJ: 0.3726, TOTAL: 0.2431\n",
      "CE: 0.2479, OBJ: 0.2754, TOTAL: 0.2534\n",
      "CE: 0.3680, OBJ: 0.3777, TOTAL: 0.3700\n",
      "CE: 0.2294, OBJ: 0.2791, TOTAL: 0.2393\n",
      "CE: 0.5575, OBJ: 0.3985, TOTAL: 0.5257\n",
      "CE: 0.8605, OBJ: 0.3425, TOTAL: 0.7569\n",
      "CE: 0.2197, OBJ: 0.2947, TOTAL: 0.2347\n",
      "CE: 0.3081, OBJ: 0.3920, TOTAL: 0.3249\n",
      "CE: 0.0839, OBJ: 0.2821, TOTAL: 0.1235\n",
      "CE: 0.3626, OBJ: 0.4229, TOTAL: 0.3747\n",
      "CE: 0.1839, OBJ: 0.3193, TOTAL: 0.2110\n",
      "CE: 0.4483, OBJ: 0.3034, TOTAL: 0.4193\n",
      "CE: 0.1940, OBJ: 0.4497, TOTAL: 0.2451\n",
      "CE: 0.0978, OBJ: 0.2609, TOTAL: 0.1304\n",
      "CE: 0.1611, OBJ: 0.3348, TOTAL: 0.1958\n",
      "CE: 0.1266, OBJ: 0.2942, TOTAL: 0.1601\n",
      "CE: 0.5173, OBJ: 0.5144, TOTAL: 0.5168\n",
      "CE: 0.1474, OBJ: 0.2378, TOTAL: 0.1655\n",
      "CE: 0.4120, OBJ: 0.2831, TOTAL: 0.3862\n",
      "CE: 0.4211, OBJ: 0.3005, TOTAL: 0.3970\n",
      "CE: 0.3819, OBJ: 0.2446, TOTAL: 0.3544\n",
      "CE: 0.1257, OBJ: 0.3137, TOTAL: 0.1633\n",
      "CE: 0.6281, OBJ: 0.4098, TOTAL: 0.5844\n",
      "CE: 0.2378, OBJ: 0.1971, TOTAL: 0.2296\n",
      "CE: 0.4488, OBJ: 0.4980, TOTAL: 0.4586\n",
      "CE: 0.4295, OBJ: 0.3247, TOTAL: 0.4085\n",
      "CE: 0.1129, OBJ: 0.2234, TOTAL: 0.1350\n",
      "CE: 0.3620, OBJ: 0.3922, TOTAL: 0.3680\n",
      "CE: 0.2675, OBJ: 0.2806, TOTAL: 0.2701\n",
      "CE: 0.1349, OBJ: 0.2583, TOTAL: 0.1596\n",
      "CE: 0.3182, OBJ: 0.3871, TOTAL: 0.3320\n",
      "CE: 0.4685, OBJ: 0.2574, TOTAL: 0.4263\n",
      "CE: 0.6113, OBJ: 0.4212, TOTAL: 0.5733\n",
      "CE: 0.0895, OBJ: 0.3523, TOTAL: 0.1421\n",
      "CE: 0.1240, OBJ: 0.2407, TOTAL: 0.1474\n",
      "CE: 0.2379, OBJ: 0.1760, TOTAL: 0.2255\n",
      "CE: 0.1515, OBJ: 0.3383, TOTAL: 0.1889\n",
      "CE: 0.3167, OBJ: 0.2594, TOTAL: 0.3053\n",
      "CE: 0.2824, OBJ: 0.2457, TOTAL: 0.2750\n",
      "CE: 0.4034, OBJ: 0.2441, TOTAL: 0.3715\n",
      "CE: 0.2944, OBJ: 0.3964, TOTAL: 0.3148\n",
      "CE: 0.1153, OBJ: 0.2845, TOTAL: 0.1491\n",
      "CE: 0.3974, OBJ: 0.4299, TOTAL: 0.4039\n",
      "CE: 0.2514, OBJ: 0.2579, TOTAL: 0.2527\n",
      "CE: 0.4724, OBJ: 0.3891, TOTAL: 0.4557\n",
      "CE: 0.2984, OBJ: 0.4005, TOTAL: 0.3189\n",
      "CE: 0.1471, OBJ: 0.3276, TOTAL: 0.1832\n",
      "CE: 0.2466, OBJ: 0.3271, TOTAL: 0.2627\n",
      "CE: 0.4349, OBJ: 0.4286, TOTAL: 0.4336\n",
      "CE: 0.4567, OBJ: 0.2892, TOTAL: 0.4232\n",
      "CE: 0.6412, OBJ: 0.5127, TOTAL: 0.6155\n",
      "CE: 0.6921, OBJ: 0.2941, TOTAL: 0.6125\n",
      "CE: 0.2117, OBJ: 0.2967, TOTAL: 0.2287\n",
      "CE: 0.7535, OBJ: 0.4017, TOTAL: 0.6832\n",
      "CE: 0.1761, OBJ: 0.2239, TOTAL: 0.1856\n",
      "CE: 0.2445, OBJ: 0.3428, TOTAL: 0.2642\n",
      "CE: 0.5287, OBJ: 0.4166, TOTAL: 0.5062\n",
      "CE: 0.1630, OBJ: 0.2217, TOTAL: 0.1747\n",
      "CE: 0.2713, OBJ: 0.3818, TOTAL: 0.2934\n",
      "CE: 0.1950, OBJ: 0.3357, TOTAL: 0.2232\n",
      "CE: 0.1695, OBJ: 0.2728, TOTAL: 0.1901\n",
      "CE: 0.6296, OBJ: 0.3787, TOTAL: 0.5794\n",
      "CE: 0.3135, OBJ: 0.2725, TOTAL: 0.3053\n",
      "CE: 0.6099, OBJ: 0.3429, TOTAL: 0.5565\n",
      "CE: 0.2184, OBJ: 0.3023, TOTAL: 0.2352\n",
      "CE: 0.6151, OBJ: 0.3309, TOTAL: 0.5583\n",
      "CE: 0.5256, OBJ: 0.4990, TOTAL: 0.5203\n",
      "CE: 0.2710, OBJ: 0.5452, TOTAL: 0.3259\n",
      "CE: 0.2322, OBJ: 0.2565, TOTAL: 0.2370\n",
      "CE: 0.2492, OBJ: 0.4364, TOTAL: 0.2866\n",
      "CE: 0.2097, OBJ: 0.2346, TOTAL: 0.2146\n",
      "CE: 0.2160, OBJ: 0.2988, TOTAL: 0.2325\n",
      "CE: 0.2278, OBJ: 0.2941, TOTAL: 0.2410\n",
      "CE: 0.3192, OBJ: 0.3203, TOTAL: 0.3194\n",
      "CE: 0.3799, OBJ: 0.2685, TOTAL: 0.3577\n",
      "CE: 0.7196, OBJ: 0.3900, TOTAL: 0.6537\n",
      "CE: 0.2851, OBJ: 0.2009, TOTAL: 0.2683\n",
      "CE: 0.0529, OBJ: 0.4215, TOTAL: 0.1266\n",
      "CE: 0.2454, OBJ: 0.3224, TOTAL: 0.2608\n",
      "CE: 0.4292, OBJ: 0.3100, TOTAL: 0.4054\n",
      "CE: 0.3900, OBJ: 0.3442, TOTAL: 0.3809\n",
      "CE: 0.8894, OBJ: 0.3926, TOTAL: 0.7901\n",
      "CE: 0.2427, OBJ: 0.3770, TOTAL: 0.2696\n",
      "CE: 0.1981, OBJ: 0.3244, TOTAL: 0.2234\n",
      "CE: 0.3300, OBJ: 0.2083, TOTAL: 0.3057\n",
      "CE: 0.1760, OBJ: 0.2673, TOTAL: 0.1943\n",
      "CE: 0.1718, OBJ: 0.3405, TOTAL: 0.2055\n",
      "CE: 0.5579, OBJ: 0.3632, TOTAL: 0.5189\n",
      "CE: 0.3595, OBJ: 0.2117, TOTAL: 0.3299\n",
      "CE: 0.4100, OBJ: 0.3181, TOTAL: 0.3916\n",
      "CE: 0.0729, OBJ: 0.4868, TOTAL: 0.1557\n",
      "CE: 0.2627, OBJ: 0.3269, TOTAL: 0.2756\n",
      "CE: 0.3278, OBJ: 0.1884, TOTAL: 0.2999\n",
      "CE: 0.3030, OBJ: 0.4804, TOTAL: 0.3385\n",
      "CE: 0.3561, OBJ: 0.3415, TOTAL: 0.3531\n",
      "CE: 0.3491, OBJ: 0.2127, TOTAL: 0.3218\n",
      "CE: 0.2043, OBJ: 0.5609, TOTAL: 0.2756\n",
      "CE: 0.4324, OBJ: 0.4326, TOTAL: 0.4324\n",
      "CE: 0.2909, OBJ: 0.3460, TOTAL: 0.3019\n",
      "CE: 0.3078, OBJ: 0.3479, TOTAL: 0.3159\n",
      "CE: 0.3935, OBJ: 0.3001, TOTAL: 0.3749\n",
      "CE: 0.3541, OBJ: 0.3047, TOTAL: 0.3442\n",
      "CE: 0.6402, OBJ: 0.2496, TOTAL: 0.5620\n",
      "CE: 0.3048, OBJ: 0.2849, TOTAL: 0.3008\n",
      "CE: 0.2665, OBJ: 0.3828, TOTAL: 0.2898\n",
      "CE: 0.2941, OBJ: 0.3201, TOTAL: 0.2993\n",
      "CE: 0.4318, OBJ: 0.4302, TOTAL: 0.4315\n",
      "CE: 0.1410, OBJ: 0.2376, TOTAL: 0.1603\n",
      "CE: 0.3789, OBJ: 0.3467, TOTAL: 0.3724\n",
      "CE: 0.2970, OBJ: 0.4492, TOTAL: 0.3275\n",
      "CE: 0.1880, OBJ: 0.3368, TOTAL: 0.2178\n",
      "CE: 0.1325, OBJ: 0.2982, TOTAL: 0.1657\n",
      "CE: 0.6734, OBJ: 0.3259, TOTAL: 0.6039\n",
      "CE: 0.1201, OBJ: 0.2714, TOTAL: 0.1503\n",
      "CE: 0.3399, OBJ: 0.4473, TOTAL: 0.3614\n",
      "CE: 0.2879, OBJ: 0.4456, TOTAL: 0.3194\n",
      "CE: 0.3073, OBJ: 0.3838, TOTAL: 0.3226\n",
      "CE: 0.1254, OBJ: 0.3955, TOTAL: 0.1794\n",
      "CE: 0.1925, OBJ: 0.2610, TOTAL: 0.2062\n",
      "CE: 0.2937, OBJ: 0.4423, TOTAL: 0.3234\n",
      "CE: 0.1054, OBJ: 0.3148, TOTAL: 0.1473\n",
      "CE: 0.4428, OBJ: 0.4335, TOTAL: 0.4410\n",
      "CE: 0.6931, OBJ: 0.4451, TOTAL: 0.6435\n",
      "CE: 0.2362, OBJ: 0.3044, TOTAL: 0.2499\n",
      "CE: 0.5215, OBJ: 0.3591, TOTAL: 0.4890\n",
      "CE: 0.2357, OBJ: 0.2841, TOTAL: 0.2454\n",
      "CE: 0.4475, OBJ: 0.4436, TOTAL: 0.4467\n",
      "CE: 0.1322, OBJ: 0.2290, TOTAL: 0.1516\n",
      "CE: 0.1303, OBJ: 0.3604, TOTAL: 0.1763\n",
      "CE: 0.6768, OBJ: 0.2065, TOTAL: 0.5828\n",
      "CE: 0.3233, OBJ: 0.3437, TOTAL: 0.3274\n",
      "CE: 0.2475, OBJ: 0.3488, TOTAL: 0.2678\n",
      "CE: 0.3452, OBJ: 0.3537, TOTAL: 0.3469\n",
      "CE: 0.3068, OBJ: 0.2552, TOTAL: 0.2964\n",
      "CE: 0.1362, OBJ: 0.3376, TOTAL: 0.1765\n",
      "CE: 0.1063, OBJ: 0.2439, TOTAL: 0.1338\n",
      "CE: 0.4522, OBJ: 0.3586, TOTAL: 0.4335\n",
      "CE: 0.3284, OBJ: 0.3019, TOTAL: 0.3231\n",
      "CE: 0.3388, OBJ: 0.3348, TOTAL: 0.3380\n",
      "CE: 0.2214, OBJ: 0.3615, TOTAL: 0.2494\n",
      "CE: 0.3907, OBJ: 0.2846, TOTAL: 0.3695\n",
      "CE: 0.3631, OBJ: 0.3571, TOTAL: 0.3619\n",
      "CE: 0.3114, OBJ: 0.3495, TOTAL: 0.3190\n",
      "CE: 0.2325, OBJ: 0.2939, TOTAL: 0.2448\n",
      "CE: 0.3198, OBJ: 0.2872, TOTAL: 0.3133\n",
      "CE: 0.3839, OBJ: 0.3984, TOTAL: 0.3868\n",
      "CE: 0.3188, OBJ: 0.3932, TOTAL: 0.3337\n",
      "CE: 0.3163, OBJ: 0.4686, TOTAL: 0.3467\n",
      "CE: 0.3759, OBJ: 0.3576, TOTAL: 0.3722\n",
      "CE: 0.4075, OBJ: 0.3570, TOTAL: 0.3974\n",
      "CE: 0.5230, OBJ: 0.3400, TOTAL: 0.4864\n",
      "CE: 0.2114, OBJ: 0.3323, TOTAL: 0.2356\n",
      "CE: 0.3179, OBJ: 0.2909, TOTAL: 0.3125\n",
      "CE: 0.2961, OBJ: 0.3195, TOTAL: 0.3008\n",
      "CE: 0.3465, OBJ: 0.3342, TOTAL: 0.3441\n",
      "CE: 0.5851, OBJ: 0.3700, TOTAL: 0.5421\n",
      "CE: 0.2417, OBJ: 0.3093, TOTAL: 0.2552\n",
      "CE: 0.4630, OBJ: 0.4831, TOTAL: 0.4670\n",
      "CE: 0.2345, OBJ: 0.2775, TOTAL: 0.2431\n",
      "CE: 0.5637, OBJ: 0.4814, TOTAL: 0.5472\n",
      "CE: 0.6139, OBJ: 0.5160, TOTAL: 0.5944\n",
      "CE: 0.5957, OBJ: 0.3673, TOTAL: 0.5500\n",
      "CE: 0.2729, OBJ: 0.4614, TOTAL: 0.3106\n",
      "CE: 0.4095, OBJ: 0.3104, TOTAL: 0.3897\n",
      "CE: 0.4586, OBJ: 0.4294, TOTAL: 0.4528\n",
      "CE: 0.6475, OBJ: 0.4164, TOTAL: 0.6013\n",
      "CE: 0.2444, OBJ: 0.3526, TOTAL: 0.2660\n",
      "CE: 0.1214, OBJ: 0.2874, TOTAL: 0.1546\n",
      "CE: 0.3903, OBJ: 0.2210, TOTAL: 0.3565\n",
      "CE: 0.4618, OBJ: 0.4707, TOTAL: 0.4636\n",
      "CE: 0.3233, OBJ: 0.4202, TOTAL: 0.3427\n",
      "CE: 0.7612, OBJ: 0.4223, TOTAL: 0.6934\n",
      "CE: 0.5342, OBJ: 0.3652, TOTAL: 0.5004\n",
      "CE: 0.3658, OBJ: 0.3660, TOTAL: 0.3658\n",
      "CE: 0.2494, OBJ: 0.4713, TOTAL: 0.2938\n",
      "CE: 0.1824, OBJ: 0.2764, TOTAL: 0.2012\n",
      "CE: 0.3192, OBJ: 0.2310, TOTAL: 0.3016\n",
      "CE: 0.4708, OBJ: 0.4717, TOTAL: 0.4710\n",
      "CE: 0.4243, OBJ: 0.4757, TOTAL: 0.4346\n",
      "CE: 0.2905, OBJ: 0.2751, TOTAL: 0.2874\n",
      "CE: 0.4560, OBJ: 0.3835, TOTAL: 0.4415\n",
      "CE: 0.2100, OBJ: 0.4294, TOTAL: 0.2539\n",
      "CE: 0.3499, OBJ: 0.1905, TOTAL: 0.3180\n",
      "CE: 0.6777, OBJ: 0.2757, TOTAL: 0.5973\n",
      "CE: 0.4147, OBJ: 0.2629, TOTAL: 0.3843\n",
      "CE: 0.3613, OBJ: 0.4403, TOTAL: 0.3771\n",
      "CE: 0.1381, OBJ: 0.2540, TOTAL: 0.1613\n",
      "CE: 0.4259, OBJ: 0.4201, TOTAL: 0.4248\n",
      "CE: 0.4052, OBJ: 0.1994, TOTAL: 0.3641\n",
      "CE: 0.2637, OBJ: 0.4307, TOTAL: 0.2971\n",
      "CE: 0.5631, OBJ: 0.4269, TOTAL: 0.5358\n",
      "CE: 0.1892, OBJ: 0.3878, TOTAL: 0.2289\n",
      "CE: 0.6046, OBJ: 0.3777, TOTAL: 0.5592\n",
      "CE: 0.0769, OBJ: 0.2803, TOTAL: 0.1176\n",
      "CE: 0.6983, OBJ: 0.3815, TOTAL: 0.6349\n",
      "CE: 0.2676, OBJ: 0.3182, TOTAL: 0.2777\n",
      "CE: 0.3670, OBJ: 0.4052, TOTAL: 0.3746\n",
      "CE: 0.5733, OBJ: 0.1469, TOTAL: 0.4880\n",
      "CE: 0.3475, OBJ: 0.3479, TOTAL: 0.3476\n",
      "CE: 0.3941, OBJ: 0.2324, TOTAL: 0.3617\n",
      "CE: 0.2009, OBJ: 0.2371, TOTAL: 0.2081\n",
      "CE: 0.2523, OBJ: 0.2676, TOTAL: 0.2554\n",
      "CE: 0.4814, OBJ: 0.2756, TOTAL: 0.4402\n",
      "CE: 0.1883, OBJ: 0.2993, TOTAL: 0.2105\n",
      "CE: 0.5219, OBJ: 0.2744, TOTAL: 0.4724\n",
      "CE: 0.2644, OBJ: 0.2670, TOTAL: 0.2649\n",
      "CE: 0.5139, OBJ: 0.3687, TOTAL: 0.4848\n",
      "CE: 0.3858, OBJ: 0.2177, TOTAL: 0.3522\n",
      "CE: 0.3812, OBJ: 0.2504, TOTAL: 0.3550\n",
      "CE: 0.2794, OBJ: 0.3732, TOTAL: 0.2981\n",
      "CE: 0.4239, OBJ: 0.2978, TOTAL: 0.3987\n",
      "CE: 0.3376, OBJ: 0.3093, TOTAL: 0.3320\n",
      "CE: 0.4594, OBJ: 0.5053, TOTAL: 0.4686\n",
      "CE: 0.4254, OBJ: 0.1837, TOTAL: 0.3771\n",
      "CE: 0.2073, OBJ: 0.3180, TOTAL: 0.2295\n",
      "CE: 0.3129, OBJ: 0.2819, TOTAL: 0.3067\n",
      "CE: 0.2383, OBJ: 0.4357, TOTAL: 0.2778\n",
      "CE: 0.3901, OBJ: 0.3827, TOTAL: 0.3886\n",
      "CE: 0.5712, OBJ: 0.3502, TOTAL: 0.5270\n",
      "CE: 0.2630, OBJ: 0.2301, TOTAL: 0.2564\n",
      "CE: 0.1660, OBJ: 0.4257, TOTAL: 0.2179\n",
      "CE: 0.3408, OBJ: 0.4451, TOTAL: 0.3617\n",
      "CE: 0.1661, OBJ: 0.2921, TOTAL: 0.1913\n",
      "CE: 0.1908, OBJ: 0.2673, TOTAL: 0.2061\n",
      "CE: 0.7561, OBJ: 0.4265, TOTAL: 0.6902\n",
      "CE: 0.1084, OBJ: 0.1586, TOTAL: 0.1185\n",
      "CE: 0.9351, OBJ: 0.3758, TOTAL: 0.8232\n",
      "CE: 0.4300, OBJ: 0.4028, TOTAL: 0.4245\n",
      "CE: 0.4438, OBJ: 0.2758, TOTAL: 0.4102\n",
      "CE: 0.2619, OBJ: 0.2206, TOTAL: 0.2536\n",
      "CE: 0.1766, OBJ: 0.4180, TOTAL: 0.2249\n",
      "CE: 0.4771, OBJ: 0.4243, TOTAL: 0.4665\n",
      "CE: 0.3483, OBJ: 0.3085, TOTAL: 0.3403\n",
      "CE: 0.5064, OBJ: 0.2691, TOTAL: 0.4589\n",
      "CE: 0.3920, OBJ: 0.3157, TOTAL: 0.3768\n",
      "CE: 0.2102, OBJ: 0.1626, TOTAL: 0.2006\n",
      "CE: 0.2830, OBJ: 0.3322, TOTAL: 0.2929\n",
      "CE: 0.2673, OBJ: 0.4460, TOTAL: 0.3031\n",
      "CE: 0.4527, OBJ: 0.3290, TOTAL: 0.4280\n",
      "CE: 0.3466, OBJ: 0.2729, TOTAL: 0.3319\n",
      "CE: 0.2095, OBJ: 0.2188, TOTAL: 0.2113\n",
      "CE: 0.2001, OBJ: 0.2310, TOTAL: 0.2062\n",
      "CE: 0.3401, OBJ: 0.3990, TOTAL: 0.3519\n",
      "CE: 0.9462, OBJ: 0.2244, TOTAL: 0.8018\n",
      "CE: 0.1514, OBJ: 0.2893, TOTAL: 0.1790\n",
      "CE: 0.2275, OBJ: 0.3446, TOTAL: 0.2509\n",
      "CE: 0.3012, OBJ: 0.2625, TOTAL: 0.2934\n",
      "CE: 0.3101, OBJ: 0.2033, TOTAL: 0.2887\n",
      "CE: 0.2522, OBJ: 0.2611, TOTAL: 0.2540\n",
      "CE: 0.3061, OBJ: 0.2681, TOTAL: 0.2985\n",
      "CE: 0.7149, OBJ: 0.3611, TOTAL: 0.6441\n",
      "CE: 0.3947, OBJ: 0.3024, TOTAL: 0.3762\n",
      "CE: 0.1767, OBJ: 0.3756, TOTAL: 0.2165\n",
      "CE: 0.3746, OBJ: 0.3577, TOTAL: 0.3712\n",
      "CE: 0.5321, OBJ: 0.2723, TOTAL: 0.4801\n",
      "CE: 0.0707, OBJ: 0.2828, TOTAL: 0.1131\n",
      "CE: 0.1159, OBJ: 0.2813, TOTAL: 0.1490\n",
      "CE: 0.1159, OBJ: 0.2291, TOTAL: 0.1386\n",
      "CE: 0.1378, OBJ: 0.2416, TOTAL: 0.1586\n",
      "CE: 0.1853, OBJ: 0.4466, TOTAL: 0.2376\n",
      "CE: 0.2128, OBJ: 0.2936, TOTAL: 0.2290\n",
      "CE: 0.3294, OBJ: 0.3567, TOTAL: 0.3349\n",
      "CE: 0.1432, OBJ: 0.1445, TOTAL: 0.1435\n",
      "CE: 0.2848, OBJ: 0.3704, TOTAL: 0.3019\n",
      "CE: 0.1228, OBJ: 0.3033, TOTAL: 0.1589\n",
      "CE: 0.1157, OBJ: 0.3132, TOTAL: 0.1552\n",
      "CE: 0.2278, OBJ: 0.2638, TOTAL: 0.2350\n",
      "CE: 0.3428, OBJ: 0.4256, TOTAL: 0.3594\n",
      "CE: 0.6003, OBJ: 0.3740, TOTAL: 0.5550\n",
      "CE: 0.1256, OBJ: 0.2157, TOTAL: 0.1436\n",
      "CE: 0.1516, OBJ: 0.3103, TOTAL: 0.1834\n",
      "CE: 0.6150, OBJ: 0.3885, TOTAL: 0.5697\n",
      "CE: 0.5046, OBJ: 0.2912, TOTAL: 0.4619\n",
      "CE: 0.2336, OBJ: 0.3091, TOTAL: 0.2487\n",
      "CE: 0.6073, OBJ: 0.4824, TOTAL: 0.5823\n",
      "CE: 0.1332, OBJ: 0.2496, TOTAL: 0.1565\n",
      "CE: 0.2898, OBJ: 0.2966, TOTAL: 0.2911\n",
      "CE: 0.2416, OBJ: 0.4374, TOTAL: 0.2808\n",
      "CE: 0.1931, OBJ: 0.3245, TOTAL: 0.2194\n",
      "CE: 0.3563, OBJ: 0.2625, TOTAL: 0.3375\n",
      "CE: 0.3793, OBJ: 0.3732, TOTAL: 0.3781\n",
      "CE: 0.5347, OBJ: 0.3843, TOTAL: 0.5046\n",
      "CE: 0.2073, OBJ: 0.4634, TOTAL: 0.2585\n",
      "CE: 0.2613, OBJ: 0.2712, TOTAL: 0.2633\n",
      "CE: 0.3744, OBJ: 0.3554, TOTAL: 0.3706\n",
      "CE: 1.0744, OBJ: 0.4551, TOTAL: 0.9505\n",
      "CE: 0.1723, OBJ: 0.2853, TOTAL: 0.1949\n",
      "CE: 0.4953, OBJ: 0.1671, TOTAL: 0.4296\n",
      "CE: 0.1985, OBJ: 0.3634, TOTAL: 0.2315\n",
      "CE: 0.4745, OBJ: 0.3607, TOTAL: 0.4518\n",
      "CE: 0.1828, OBJ: 0.3157, TOTAL: 0.2094\n",
      "CE: 0.2445, OBJ: 0.3000, TOTAL: 0.2556\n",
      "CE: 0.0992, OBJ: 0.2364, TOTAL: 0.1266\n",
      "CE: 0.2314, OBJ: 0.2645, TOTAL: 0.2380\n",
      "CE: 0.5191, OBJ: 0.4613, TOTAL: 0.5075\n",
      "CE: 0.2494, OBJ: 0.3856, TOTAL: 0.2767\n",
      "CE: 0.3117, OBJ: 0.3478, TOTAL: 0.3189\n",
      "CE: 0.2639, OBJ: 0.2777, TOTAL: 0.2667\n",
      "CE: 0.2822, OBJ: 0.3973, TOTAL: 0.3053\n",
      "CE: 0.2783, OBJ: 0.4905, TOTAL: 0.3208\n",
      "CE: 0.3398, OBJ: 0.4248, TOTAL: 0.3568\n",
      "CE: 0.5193, OBJ: 0.4297, TOTAL: 0.5014\n",
      "CE: 0.1404, OBJ: 0.1947, TOTAL: 0.1513\n",
      "CE: 0.5363, OBJ: 0.5010, TOTAL: 0.5293\n",
      "CE: 0.5119, OBJ: 0.3751, TOTAL: 0.4845\n",
      "CE: 0.1905, OBJ: 0.3110, TOTAL: 0.2146\n",
      "CE: 0.4016, OBJ: 0.3860, TOTAL: 0.3985\n",
      "CE: 0.2986, OBJ: 0.2995, TOTAL: 0.2988\n",
      "CE: 0.2031, OBJ: 0.4162, TOTAL: 0.2458\n",
      "CE: 0.1578, OBJ: 0.3310, TOTAL: 0.1924\n",
      "CE: 0.2995, OBJ: 0.2212, TOTAL: 0.2838\n",
      "CE: 0.4118, OBJ: 0.4007, TOTAL: 0.4096\n",
      "CE: 0.2640, OBJ: 0.2727, TOTAL: 0.2658\n",
      "CE: 0.7195, OBJ: 0.4055, TOTAL: 0.6567\n",
      "CE: 0.3881, OBJ: 0.4701, TOTAL: 0.4045\n",
      "CE: 0.0742, OBJ: 0.3029, TOTAL: 0.1200\n",
      "CE: 0.3370, OBJ: 0.2743, TOTAL: 0.3245\n",
      "CE: 0.5384, OBJ: 0.3723, TOTAL: 0.5052\n",
      "CE: 0.5241, OBJ: 0.3637, TOTAL: 0.4920\n",
      "CE: 0.0784, OBJ: 0.2350, TOTAL: 0.1097\n",
      "CE: 0.7065, OBJ: 0.3544, TOTAL: 0.6361\n",
      "CE: 0.5663, OBJ: 0.3981, TOTAL: 0.5327\n",
      "CE: 0.2491, OBJ: 0.3324, TOTAL: 0.2658\n",
      "CE: 0.6570, OBJ: 0.2863, TOTAL: 0.5829\n",
      "CE: 0.8825, OBJ: 0.4391, TOTAL: 0.7938\n",
      "CE: 0.5390, OBJ: 0.3445, TOTAL: 0.5001\n",
      "CE: 0.2613, OBJ: 0.3768, TOTAL: 0.2844\n",
      "CE: 0.2634, OBJ: 0.3568, TOTAL: 0.2821\n",
      "CE: 0.5701, OBJ: 0.2359, TOTAL: 0.5032\n",
      "CE: 0.5370, OBJ: 0.4067, TOTAL: 0.5110\n",
      "CE: 0.3822, OBJ: 0.4179, TOTAL: 0.3893\n",
      "CE: 0.2605, OBJ: 0.2786, TOTAL: 0.2641\n",
      "CE: 0.2482, OBJ: 0.3302, TOTAL: 0.2646\n",
      "CE: 0.3577, OBJ: 0.2999, TOTAL: 0.3461\n",
      "CE: 0.5272, OBJ: 0.4072, TOTAL: 0.5032\n",
      "CE: 0.3272, OBJ: 0.3227, TOTAL: 0.3263\n",
      "CE: 0.4425, OBJ: 0.2832, TOTAL: 0.4106\n",
      "CE: 0.1953, OBJ: 0.4164, TOTAL: 0.2395\n",
      "CE: 0.3559, OBJ: 0.5111, TOTAL: 0.3869\n",
      "CE: 0.3424, OBJ: 0.2224, TOTAL: 0.3184\n",
      "CE: 0.6614, OBJ: 0.4339, TOTAL: 0.6159\n",
      "CE: 0.5055, OBJ: 0.3859, TOTAL: 0.4815\n",
      "CE: 0.5222, OBJ: 0.3171, TOTAL: 0.4811\n",
      "CE: 0.3078, OBJ: 0.2110, TOTAL: 0.2884\n",
      "CE: 0.4652, OBJ: 0.4110, TOTAL: 0.4544\n",
      "CE: 0.2674, OBJ: 0.2607, TOTAL: 0.2660\n",
      "CE: 0.8551, OBJ: 0.4361, TOTAL: 0.7713\n",
      "CE: 0.1707, OBJ: 0.3118, TOTAL: 0.1990\n",
      "CE: 0.3653, OBJ: 0.3290, TOTAL: 0.3581\n",
      "CE: 0.2923, OBJ: 0.2070, TOTAL: 0.2752\n",
      "CE: 0.0922, OBJ: 0.3289, TOTAL: 0.1396\n",
      "CE: 0.1305, OBJ: 0.1621, TOTAL: 0.1368\n",
      "CE: 0.3881, OBJ: 0.3105, TOTAL: 0.3726\n",
      "CE: 0.2654, OBJ: 0.2566, TOTAL: 0.2636\n",
      "CE: 0.3851, OBJ: 0.2713, TOTAL: 0.3623\n",
      "CE: 0.1651, OBJ: 0.2129, TOTAL: 0.1746\n",
      "CE: 0.5801, OBJ: 0.3831, TOTAL: 0.5407\n",
      "CE: 0.2096, OBJ: 0.3405, TOTAL: 0.2358\n",
      "CE: 0.4693, OBJ: 0.2307, TOTAL: 0.4216\n",
      "CE: 0.4501, OBJ: 0.4182, TOTAL: 0.4437\n",
      "CE: 0.1963, OBJ: 0.3423, TOTAL: 0.2255\n",
      "CE: 0.3517, OBJ: 0.2658, TOTAL: 0.3345\n",
      "CE: 0.6219, OBJ: 0.4411, TOTAL: 0.5858\n",
      "CE: 0.3943, OBJ: 0.2135, TOTAL: 0.3581\n",
      "CE: 0.0712, OBJ: 0.3373, TOTAL: 0.1244\n",
      "CE: 0.2113, OBJ: 0.3536, TOTAL: 0.2397\n",
      "CE: 0.4926, OBJ: 0.2381, TOTAL: 0.4417\n",
      "CE: 0.4953, OBJ: 0.3613, TOTAL: 0.4685\n",
      "CE: 0.5062, OBJ: 0.3037, TOTAL: 0.4657\n",
      "CE: 0.2459, OBJ: 0.2593, TOTAL: 0.2486\n",
      "CE: 0.5079, OBJ: 0.3856, TOTAL: 0.4834\n",
      "CE: 0.5420, OBJ: 0.2747, TOTAL: 0.4885\n",
      "CE: 0.2881, OBJ: 0.3793, TOTAL: 0.3064\n",
      "CE: 0.3705, OBJ: 0.2103, TOTAL: 0.3385\n",
      "CE: 0.2029, OBJ: 0.2540, TOTAL: 0.2131\n",
      "CE: 0.3616, OBJ: 0.4710, TOTAL: 0.3835\n",
      "CE: 0.5304, OBJ: 0.3585, TOTAL: 0.4960\n",
      "CE: 0.3470, OBJ: 0.4187, TOTAL: 0.3613\n",
      "CE: 0.4166, OBJ: 0.3091, TOTAL: 0.3951\n",
      "CE: 0.3946, OBJ: 0.4174, TOTAL: 0.3991\n",
      "CE: 0.1630, OBJ: 0.3566, TOTAL: 0.2018\n",
      "CE: 0.5787, OBJ: 0.3177, TOTAL: 0.5265\n",
      "CE: 0.4356, OBJ: 0.3318, TOTAL: 0.4148\n",
      "CE: 0.2295, OBJ: 0.2665, TOTAL: 0.2369\n",
      "CE: 0.4865, OBJ: 0.3053, TOTAL: 0.4502\n",
      "CE: 0.6552, OBJ: 0.2899, TOTAL: 0.5822\n",
      "CE: 0.3901, OBJ: 0.1908, TOTAL: 0.3502\n",
      "CE: 0.0336, OBJ: 0.2257, TOTAL: 0.0720\n",
      "CE: 0.2639, OBJ: 0.2804, TOTAL: 0.2672\n",
      "CE: 0.4362, OBJ: 0.4523, TOTAL: 0.4394\n",
      "CE: 0.4538, OBJ: 0.3286, TOTAL: 0.4288\n",
      "CE: 0.2020, OBJ: 0.3346, TOTAL: 0.2285\n",
      "CE: 0.1811, OBJ: 0.3586, TOTAL: 0.2166\n",
      "CE: 0.2919, OBJ: 0.2068, TOTAL: 0.2749\n",
      "CE: 0.4373, OBJ: 0.2645, TOTAL: 0.4028\n",
      "CE: 0.2577, OBJ: 0.3096, TOTAL: 0.2681\n",
      "CE: 0.2566, OBJ: 0.2696, TOTAL: 0.2592\n",
      "CE: 0.2224, OBJ: 0.4415, TOTAL: 0.2662\n",
      "CE: 0.3794, OBJ: 0.3748, TOTAL: 0.3785\n",
      "CE: 0.0600, OBJ: 0.2682, TOTAL: 0.1017\n",
      "CE: 0.3066, OBJ: 0.3612, TOTAL: 0.3175\n",
      "CE: 0.7869, OBJ: 0.4168, TOTAL: 0.7129\n",
      "CE: 0.4630, OBJ: 0.3770, TOTAL: 0.4458\n",
      "CE: 0.5954, OBJ: 0.3910, TOTAL: 0.5545\n",
      "CE: 0.1616, OBJ: 0.3867, TOTAL: 0.2066\n",
      "CE: 0.2865, OBJ: 0.3065, TOTAL: 0.2905\n",
      "CE: 0.0762, OBJ: 0.3546, TOTAL: 0.1319\n",
      "CE: 0.3073, OBJ: 0.4840, TOTAL: 0.3426\n",
      "CE: 0.3064, OBJ: 0.4048, TOTAL: 0.3260\n",
      "CE: 0.2428, OBJ: 0.2231, TOTAL: 0.2389\n",
      "CE: 0.4413, OBJ: 0.4154, TOTAL: 0.4361\n",
      "CE: 0.1864, OBJ: 0.3533, TOTAL: 0.2197\n",
      "CE: 0.6180, OBJ: 0.5539, TOTAL: 0.6052\n",
      "CE: 0.3747, OBJ: 0.2250, TOTAL: 0.3448\n",
      "CE: 0.2265, OBJ: 0.3892, TOTAL: 0.2590\n",
      "CE: 0.2926, OBJ: 0.2876, TOTAL: 0.2916\n",
      "CE: 0.5837, OBJ: 0.3241, TOTAL: 0.5317\n",
      "CE: 0.7629, OBJ: 0.4220, TOTAL: 0.6947\n",
      "CE: 0.7958, OBJ: 0.3017, TOTAL: 0.6970\n",
      "CE: 0.1438, OBJ: 0.3603, TOTAL: 0.1871\n",
      "CE: 0.3030, OBJ: 0.3739, TOTAL: 0.3172\n",
      "CE: 0.3234, OBJ: 0.2819, TOTAL: 0.3151\n",
      "CE: 0.5393, OBJ: 0.3768, TOTAL: 0.5068\n",
      "CE: 0.1033, OBJ: 0.2862, TOTAL: 0.1399\n",
      "CE: 0.2567, OBJ: 0.2398, TOTAL: 0.2533\n",
      "CE: 0.1087, OBJ: 0.4066, TOTAL: 0.1682\n",
      "CE: 0.4814, OBJ: 0.1867, TOTAL: 0.4224\n",
      "CE: 0.3676, OBJ: 0.3273, TOTAL: 0.3595\n",
      "CE: 0.1721, OBJ: 0.2061, TOTAL: 0.1789\n",
      "CE: 0.3542, OBJ: 0.4755, TOTAL: 0.3785\n",
      "CE: 0.2534, OBJ: 0.3117, TOTAL: 0.2651\n",
      "CE: 0.2598, OBJ: 0.3569, TOTAL: 0.2792\n",
      "CE: 0.5312, OBJ: 0.4819, TOTAL: 0.5213\n",
      "CE: 0.2707, OBJ: 0.1826, TOTAL: 0.2531\n",
      "CE: 0.1956, OBJ: 0.3533, TOTAL: 0.2272\n",
      "CE: 0.5150, OBJ: 0.5143, TOTAL: 0.5149\n",
      "CE: 0.3783, OBJ: 0.3363, TOTAL: 0.3699\n",
      "CE: 0.2688, OBJ: 0.3386, TOTAL: 0.2828\n",
      "CE: 0.2533, OBJ: 0.3959, TOTAL: 0.2819\n",
      "CE: 0.1307, OBJ: 0.4692, TOTAL: 0.1984\n",
      "CE: 0.2147, OBJ: 0.4297, TOTAL: 0.2577\n",
      "CE: 0.1122, OBJ: 0.3447, TOTAL: 0.1587\n",
      "CE: 0.1845, OBJ: 0.2902, TOTAL: 0.2056\n",
      "CE: 0.2905, OBJ: 0.2728, TOTAL: 0.2870\n",
      "CE: 0.2527, OBJ: 0.2081, TOTAL: 0.2438\n",
      "CE: 0.1381, OBJ: 0.3703, TOTAL: 0.1846\n",
      "CE: 0.1764, OBJ: 0.4189, TOTAL: 0.2249\n",
      "CE: 0.2317, OBJ: 0.3677, TOTAL: 0.2589\n",
      "CE: 0.1902, OBJ: 0.3505, TOTAL: 0.2222\n",
      "CE: 0.3492, OBJ: 0.3371, TOTAL: 0.3468\n",
      "CE: 0.1469, OBJ: 0.2159, TOTAL: 0.1607\n",
      "CE: 0.5572, OBJ: 0.3918, TOTAL: 0.5241\n",
      "CE: 0.5830, OBJ: 0.5433, TOTAL: 0.5751\n",
      "CE: 0.1866, OBJ: 0.2995, TOTAL: 0.2091\n",
      "CE: 0.5433, OBJ: 0.3396, TOTAL: 0.5026\n",
      "CE: 0.2501, OBJ: 0.3687, TOTAL: 0.2738\n",
      "CE: 0.6343, OBJ: 0.3532, TOTAL: 0.5781\n",
      "CE: 0.3788, OBJ: 0.2908, TOTAL: 0.3612\n",
      "CE: 0.2441, OBJ: 0.3540, TOTAL: 0.2661\n",
      "CE: 0.5413, OBJ: 0.4566, TOTAL: 0.5243\n",
      "CE: 0.1431, OBJ: 0.2005, TOTAL: 0.1546\n",
      "CE: 0.1362, OBJ: 0.3515, TOTAL: 0.1793\n",
      "CE: 0.2557, OBJ: 0.3976, TOTAL: 0.2841\n",
      "CE: 0.2358, OBJ: 0.3180, TOTAL: 0.2522\n",
      "CE: 0.3623, OBJ: 0.3904, TOTAL: 0.3679\n",
      "CE: 0.7675, OBJ: 0.4607, TOTAL: 0.7062\n",
      "CE: 0.4819, OBJ: 0.2891, TOTAL: 0.4433\n",
      "CE: 0.3295, OBJ: 0.3516, TOTAL: 0.3339\n",
      "CE: 0.6576, OBJ: 0.4263, TOTAL: 0.6113\n",
      "CE: 0.4082, OBJ: 0.2458, TOTAL: 0.3757\n",
      "CE: 0.1253, OBJ: 0.3113, TOTAL: 0.1625\n",
      "CE: 0.1620, OBJ: 0.2638, TOTAL: 0.1824\n",
      "CE: 0.2591, OBJ: 0.4136, TOTAL: 0.2900\n",
      "CE: 0.2073, OBJ: 0.3791, TOTAL: 0.2417\n",
      "CE: 0.2775, OBJ: 0.3692, TOTAL: 0.2959\n",
      "CE: 0.3379, OBJ: 0.2645, TOTAL: 0.3232\n",
      "CE: 0.4697, OBJ: 0.4218, TOTAL: 0.4601\n",
      "CE: 0.3142, OBJ: 0.5678, TOTAL: 0.3650\n",
      "CE: 0.1775, OBJ: 0.3868, TOTAL: 0.2193\n",
      "CE: 0.4995, OBJ: 0.2917, TOTAL: 0.4580\n",
      "CE: 0.3565, OBJ: 0.3800, TOTAL: 0.3612\n",
      "CE: 0.1917, OBJ: 0.1863, TOTAL: 0.1906\n",
      "CE: 0.1039, OBJ: 0.3759, TOTAL: 0.1583\n",
      "CE: 0.0602, OBJ: 0.2487, TOTAL: 0.0979\n",
      "CE: 0.1746, OBJ: 0.3656, TOTAL: 0.2128\n",
      "CE: 0.3469, OBJ: 0.2546, TOTAL: 0.3285\n",
      "CE: 0.3994, OBJ: 0.3496, TOTAL: 0.3894\n",
      "CE: 0.1769, OBJ: 0.1736, TOTAL: 0.1763\n",
      "CE: 0.4644, OBJ: 0.4119, TOTAL: 0.4539\n",
      "CE: 0.2532, OBJ: 0.2242, TOTAL: 0.2474\n",
      "CE: 0.3634, OBJ: 0.3379, TOTAL: 0.3583\n",
      "CE: 0.1244, OBJ: 0.3548, TOTAL: 0.1705\n",
      "CE: 0.3668, OBJ: 0.3178, TOTAL: 0.3570\n",
      "CE: 0.6060, OBJ: 0.4751, TOTAL: 0.5798\n",
      "CE: 0.4475, OBJ: 0.3973, TOTAL: 0.4374\n",
      "CE: 0.1322, OBJ: 0.2283, TOTAL: 0.1514\n",
      "CE: 0.4183, OBJ: 0.2880, TOTAL: 0.3922\n",
      "CE: 0.4891, OBJ: 0.2834, TOTAL: 0.4480\n",
      "CE: 0.3925, OBJ: 0.3586, TOTAL: 0.3857\n",
      "CE: 0.2229, OBJ: 0.3430, TOTAL: 0.2469\n",
      "CE: 0.1212, OBJ: 0.2074, TOTAL: 0.1384\n",
      "CE: 0.5002, OBJ: 0.3131, TOTAL: 0.4628\n",
      "CE: 0.2535, OBJ: 0.5110, TOTAL: 0.3050\n",
      "CE: 0.3844, OBJ: 0.3195, TOTAL: 0.3714\n",
      "CE: 0.3596, OBJ: 0.2508, TOTAL: 0.3379\n",
      "CE: 0.2288, OBJ: 0.2677, TOTAL: 0.2366\n",
      "CE: 0.5176, OBJ: 0.4499, TOTAL: 0.5041\n",
      "CE: 0.2091, OBJ: 0.4817, TOTAL: 0.2636\n",
      "CE: 0.5397, OBJ: 0.3677, TOTAL: 0.5053\n",
      "CE: 0.2559, OBJ: 0.2862, TOTAL: 0.2619\n",
      "CE: 0.1030, OBJ: 0.3979, TOTAL: 0.1620\n",
      "CE: 0.7944, OBJ: 0.3133, TOTAL: 0.6982\n",
      "CE: 0.4563, OBJ: 0.2546, TOTAL: 0.4160\n",
      "CE: 0.4401, OBJ: 0.3758, TOTAL: 0.4273\n",
      "CE: 0.2128, OBJ: 0.3649, TOTAL: 0.2432\n",
      "CE: 0.1459, OBJ: 0.2726, TOTAL: 0.1713\n",
      "CE: 0.1933, OBJ: 0.2474, TOTAL: 0.2041\n",
      "CE: 0.4803, OBJ: 0.3113, TOTAL: 0.4465\n",
      "CE: 0.6165, OBJ: 0.4554, TOTAL: 0.5842\n",
      "CE: 0.4987, OBJ: 0.4116, TOTAL: 0.4813\n",
      "CE: 0.1934, OBJ: 0.2866, TOTAL: 0.2121\n",
      "CE: 0.6883, OBJ: 0.4617, TOTAL: 0.6430\n",
      "CE: 0.5182, OBJ: 0.3725, TOTAL: 0.4890\n",
      "CE: 0.3030, OBJ: 0.4046, TOTAL: 0.3233\n",
      "CE: 0.2449, OBJ: 0.3259, TOTAL: 0.2611\n",
      "CE: 0.2166, OBJ: 0.2309, TOTAL: 0.2194\n",
      "CE: 0.5096, OBJ: 0.3866, TOTAL: 0.4850\n",
      "CE: 0.4981, OBJ: 0.3367, TOTAL: 0.4658\n",
      "CE: 0.0835, OBJ: 0.2504, TOTAL: 0.1169\n",
      "CE: 0.2717, OBJ: 0.3608, TOTAL: 0.2895\n",
      "CE: 0.3842, OBJ: 0.3569, TOTAL: 0.3788\n",
      "CE: 0.5708, OBJ: 0.2494, TOTAL: 0.5065\n",
      "CE: 0.2242, OBJ: 0.3583, TOTAL: 0.2511\n",
      "CE: 0.3316, OBJ: 0.3461, TOTAL: 0.3345\n",
      "CE: 0.3703, OBJ: 0.3685, TOTAL: 0.3700\n",
      "CE: 0.6611, OBJ: 0.4832, TOTAL: 0.6255\n",
      "CE: 0.6619, OBJ: 0.4335, TOTAL: 0.6162\n",
      "CE: 0.2976, OBJ: 0.2864, TOTAL: 0.2954\n",
      "CE: 0.7413, OBJ: 0.4457, TOTAL: 0.6822\n",
      "CE: 0.3828, OBJ: 0.3294, TOTAL: 0.3721\n",
      "CE: 0.4078, OBJ: 0.3015, TOTAL: 0.3866\n",
      "CE: 0.2090, OBJ: 0.2567, TOTAL: 0.2185\n",
      "CE: 0.2436, OBJ: 0.2480, TOTAL: 0.2445\n",
      "CE: 0.0789, OBJ: 0.2951, TOTAL: 0.1222\n",
      "CE: 0.2377, OBJ: 0.3695, TOTAL: 0.2641\n",
      "CE: 0.3166, OBJ: 0.5999, TOTAL: 0.3732\n",
      "CE: 0.3360, OBJ: 0.2482, TOTAL: 0.3185\n",
      "CE: 0.3398, OBJ: 0.3134, TOTAL: 0.3345\n",
      "CE: 0.2770, OBJ: 0.3753, TOTAL: 0.2966\n",
      "CE: 0.2658, OBJ: 0.3589, TOTAL: 0.2844\n",
      "CE: 0.2555, OBJ: 0.2816, TOTAL: 0.2607\n",
      "CE: 0.4645, OBJ: 0.2670, TOTAL: 0.4250\n",
      "CE: 0.3189, OBJ: 0.3707, TOTAL: 0.3292\n",
      "CE: 0.4243, OBJ: 0.3351, TOTAL: 0.4064\n",
      "CE: 0.3424, OBJ: 0.3364, TOTAL: 0.3412\n",
      "CE: 0.2938, OBJ: 0.3174, TOTAL: 0.2985\n",
      "CE: 0.2142, OBJ: 0.2874, TOTAL: 0.2289\n",
      "CE: 0.8688, OBJ: 0.3965, TOTAL: 0.7743\n",
      "CE: 0.3334, OBJ: 0.2485, TOTAL: 0.3164\n",
      "CE: 0.2524, OBJ: 0.3958, TOTAL: 0.2810\n",
      "CE: 0.5209, OBJ: 0.3225, TOTAL: 0.4812\n",
      "CE: 0.3247, OBJ: 0.3268, TOTAL: 0.3251\n",
      "CE: 0.3337, OBJ: 0.3430, TOTAL: 0.3356\n",
      "CE: 0.4172, OBJ: 0.3735, TOTAL: 0.4085\n",
      "CE: 0.2438, OBJ: 0.2911, TOTAL: 0.2532\n",
      "CE: 0.2453, OBJ: 0.3416, TOTAL: 0.2645\n",
      "CE: 0.5932, OBJ: 0.4184, TOTAL: 0.5582\n",
      "CE: 0.4415, OBJ: 0.3554, TOTAL: 0.4243\n",
      "CE: 0.2000, OBJ: 0.2426, TOTAL: 0.2085\n",
      "CE: 0.2076, OBJ: 0.3856, TOTAL: 0.2432\n",
      "CE: 0.1977, OBJ: 0.4707, TOTAL: 0.2523\n",
      "CE: 0.2486, OBJ: 0.2517, TOTAL: 0.2492\n",
      "CE: 0.2430, OBJ: 0.3026, TOTAL: 0.2550\n",
      "CE: 0.4266, OBJ: 0.2910, TOTAL: 0.3995\n",
      "CE: 0.1509, OBJ: 0.2185, TOTAL: 0.1644\n",
      "CE: 0.1387, OBJ: 0.3463, TOTAL: 0.1802\n",
      "CE: 0.4584, OBJ: 0.4477, TOTAL: 0.4562\n",
      "CE: 0.5159, OBJ: 0.3321, TOTAL: 0.4791\n",
      "CE: 0.2328, OBJ: 0.2787, TOTAL: 0.2420\n",
      "CE: 0.2519, OBJ: 0.1903, TOTAL: 0.2396\n",
      "CE: 0.2400, OBJ: 0.3795, TOTAL: 0.2679\n",
      "CE: 0.3940, OBJ: 0.1603, TOTAL: 0.3473\n",
      "CE: 0.1773, OBJ: 0.4817, TOTAL: 0.2382\n",
      "CE: 0.2852, OBJ: 0.3796, TOTAL: 0.3041\n",
      "CE: 0.2215, OBJ: 0.2813, TOTAL: 0.2335\n",
      "CE: 0.2972, OBJ: 0.3887, TOTAL: 0.3155\n",
      "CE: 0.3286, OBJ: 0.3406, TOTAL: 0.3310\n",
      "CE: 0.4389, OBJ: 0.4799, TOTAL: 0.4471\n",
      "CE: 0.3992, OBJ: 0.3659, TOTAL: 0.3926\n",
      "CE: 0.5424, OBJ: 0.2852, TOTAL: 0.4910\n",
      "CE: 0.7791, OBJ: 0.3867, TOTAL: 0.7006\n",
      "CE: 0.3314, OBJ: 0.3448, TOTAL: 0.3341\n",
      "CE: 0.5522, OBJ: 0.2277, TOTAL: 0.4873\n",
      "CE: 0.4342, OBJ: 0.3723, TOTAL: 0.4218\n",
      "CE: 0.2564, OBJ: 0.3810, TOTAL: 0.2813\n",
      "CE: 0.3151, OBJ: 0.3741, TOTAL: 0.3269\n",
      "CE: 0.1441, OBJ: 0.4085, TOTAL: 0.1969\n",
      "CE: 0.9493, OBJ: 0.3326, TOTAL: 0.8260\n",
      "CE: 0.3570, OBJ: 0.3625, TOTAL: 0.3581\n",
      "CE: 0.1693, OBJ: 0.2546, TOTAL: 0.1863\n",
      "CE: 0.1687, OBJ: 0.2421, TOTAL: 0.1834\n",
      "CE: 0.4488, OBJ: 0.3600, TOTAL: 0.4311\n",
      "CE: 0.7614, OBJ: 0.4419, TOTAL: 0.6975\n",
      "CE: 0.1568, OBJ: 0.2222, TOTAL: 0.1698\n",
      "CE: 0.1772, OBJ: 0.2520, TOTAL: 0.1922\n",
      "CE: 0.2231, OBJ: 0.2246, TOTAL: 0.2234\n",
      "CE: 0.2993, OBJ: 0.4061, TOTAL: 0.3207\n",
      "CE: 0.2614, OBJ: 0.3497, TOTAL: 0.2791\n",
      "CE: 0.5611, OBJ: 0.2844, TOTAL: 0.5058\n",
      "CE: 0.1999, OBJ: 0.2443, TOTAL: 0.2088\n",
      "CE: 0.3305, OBJ: 0.2958, TOTAL: 0.3235\n",
      "CE: 0.0808, OBJ: 0.1709, TOTAL: 0.0988\n",
      "CE: 0.2753, OBJ: 0.3777, TOTAL: 0.2958\n",
      "CE: 0.1678, OBJ: 0.3324, TOTAL: 0.2007\n",
      "CE: 0.1489, OBJ: 0.3891, TOTAL: 0.1969\n",
      "CE: 0.5242, OBJ: 0.2964, TOTAL: 0.4786\n",
      "CE: 0.6738, OBJ: 0.4896, TOTAL: 0.6369\n",
      "CE: 0.2250, OBJ: 0.2247, TOTAL: 0.2249\n",
      "CE: 0.4314, OBJ: 0.4179, TOTAL: 0.4287\n",
      "CE: 0.3235, OBJ: 0.2961, TOTAL: 0.3180\n",
      "CE: 0.2697, OBJ: 0.3959, TOTAL: 0.2950\n",
      "CE: 0.5615, OBJ: 0.1767, TOTAL: 0.4845\n",
      "CE: 0.5471, OBJ: 0.3355, TOTAL: 0.5047\n",
      "CE: 0.1965, OBJ: 0.3076, TOTAL: 0.2187\n",
      "CE: 0.1989, OBJ: 0.3319, TOTAL: 0.2255\n",
      "CE: 0.1366, OBJ: 0.4134, TOTAL: 0.1920\n",
      "CE: 0.3136, OBJ: 0.3906, TOTAL: 0.3290\n",
      "CE: 0.3752, OBJ: 0.2512, TOTAL: 0.3504\n",
      "CE: 0.5405, OBJ: 0.3983, TOTAL: 0.5120\n",
      "CE: 0.4274, OBJ: 0.3852, TOTAL: 0.4190\n",
      "CE: 0.1609, OBJ: 0.3824, TOTAL: 0.2052\n",
      "CE: 0.2003, OBJ: 0.2915, TOTAL: 0.2186\n",
      "CE: 0.1910, OBJ: 0.3453, TOTAL: 0.2219\n",
      "CE: 0.0521, OBJ: 0.2680, TOTAL: 0.0953\n",
      "CE: 0.3423, OBJ: 0.4049, TOTAL: 0.3548\n",
      "CE: 0.2093, OBJ: 0.3617, TOTAL: 0.2398\n",
      "CE: 0.2864, OBJ: 0.2521, TOTAL: 0.2795\n",
      "CE: 0.3100, OBJ: 0.3473, TOTAL: 0.3175\n",
      "CE: 0.4655, OBJ: 0.2596, TOTAL: 0.4243\n",
      "CE: 0.0563, OBJ: 0.1990, TOTAL: 0.0848\n",
      "CE: 0.4182, OBJ: 0.5090, TOTAL: 0.4364\n",
      "CE: 0.5848, OBJ: 0.2689, TOTAL: 0.5216\n",
      "CE: 0.3861, OBJ: 0.4224, TOTAL: 0.3934\n",
      "CE: 0.1565, OBJ: 0.2679, TOTAL: 0.1787\n",
      "CE: 1.1081, OBJ: 0.4467, TOTAL: 0.9758\n",
      "CE: 0.4837, OBJ: 0.3455, TOTAL: 0.4561\n",
      "CE: 0.2336, OBJ: 0.2896, TOTAL: 0.2448\n",
      "CE: 0.7884, OBJ: 0.4247, TOTAL: 0.7157\n",
      "CE: 0.3837, OBJ: 0.1964, TOTAL: 0.3462\n",
      "CE: 0.1933, OBJ: 0.3341, TOTAL: 0.2215\n",
      "CE: 0.5424, OBJ: 0.3230, TOTAL: 0.4985\n",
      "CE: 0.5647, OBJ: 0.4354, TOTAL: 0.5389\n",
      "CE: 0.2064, OBJ: 0.4488, TOTAL: 0.2549\n",
      "CE: 0.2251, OBJ: 0.2989, TOTAL: 0.2398\n",
      "CE: 0.4824, OBJ: 0.5255, TOTAL: 0.4911\n",
      "CE: 0.1249, OBJ: 0.2726, TOTAL: 0.1545\n",
      "CE: 0.3430, OBJ: 0.3756, TOTAL: 0.3495\n",
      "CE: 0.3663, OBJ: 0.4277, TOTAL: 0.3786\n",
      "CE: 0.1602, OBJ: 0.1936, TOTAL: 0.1669\n",
      "CE: 0.4208, OBJ: 0.3768, TOTAL: 0.4120\n",
      "CE: 0.1506, OBJ: 0.2325, TOTAL: 0.1670\n",
      "CE: 0.2541, OBJ: 0.2445, TOTAL: 0.2522\n",
      "CE: 0.2795, OBJ: 0.3018, TOTAL: 0.2840\n",
      "CE: 0.3548, OBJ: 0.1655, TOTAL: 0.3169\n",
      "CE: 0.1648, OBJ: 0.3047, TOTAL: 0.1928\n",
      "CE: 0.1763, OBJ: 0.3915, TOTAL: 0.2193\n",
      "CE: 0.3964, OBJ: 0.3222, TOTAL: 0.3816\n",
      "CE: 0.2934, OBJ: 0.2705, TOTAL: 0.2888\n",
      "CE: 0.3434, OBJ: 0.3764, TOTAL: 0.3500\n",
      "CE: 0.2637, OBJ: 0.2556, TOTAL: 0.2621\n",
      "CE: 0.2437, OBJ: 0.4525, TOTAL: 0.2854\n",
      "CE: 0.1684, OBJ: 0.4108, TOTAL: 0.2169\n",
      "CE: 0.3501, OBJ: 0.2856, TOTAL: 0.3372\n",
      "CE: 0.5189, OBJ: 0.2699, TOTAL: 0.4691\n",
      "CE: 0.2824, OBJ: 0.3630, TOTAL: 0.2985\n",
      "CE: 0.2464, OBJ: 0.3498, TOTAL: 0.2671\n",
      "CE: 0.1121, OBJ: 0.2747, TOTAL: 0.1446\n",
      "CE: 0.3270, OBJ: 0.1631, TOTAL: 0.2942\n",
      "CE: 0.2010, OBJ: 0.3019, TOTAL: 0.2212\n",
      "CE: 0.2341, OBJ: 0.4248, TOTAL: 0.2723\n",
      "CE: 0.1562, OBJ: 0.2318, TOTAL: 0.1713\n",
      "CE: 0.4274, OBJ: 0.2967, TOTAL: 0.4013\n",
      "CE: 0.3595, OBJ: 0.4005, TOTAL: 0.3677\n",
      "CE: 0.7485, OBJ: 0.3381, TOTAL: 0.6664\n",
      "CE: 0.6266, OBJ: 0.3189, TOTAL: 0.5651\n",
      "CE: 0.7704, OBJ: 0.4995, TOTAL: 0.7163\n",
      "CE: 0.2017, OBJ: 0.4429, TOTAL: 0.2499\n",
      "CE: 0.4014, OBJ: 0.3240, TOTAL: 0.3860\n",
      "CE: 0.3529, OBJ: 0.3252, TOTAL: 0.3474\n",
      "CE: 0.3021, OBJ: 0.3069, TOTAL: 0.3030\n",
      "CE: 0.1977, OBJ: 0.4533, TOTAL: 0.2488\n",
      "CE: 0.3212, OBJ: 0.4151, TOTAL: 0.3400\n",
      "CE: 0.2281, OBJ: 0.3152, TOTAL: 0.2456\n",
      "CE: 0.3922, OBJ: 0.2339, TOTAL: 0.3605\n",
      "CE: 0.3384, OBJ: 0.4620, TOTAL: 0.3631\n",
      "CE: 0.1984, OBJ: 0.2671, TOTAL: 0.2121\n",
      "CE: 0.4866, OBJ: 0.2606, TOTAL: 0.4414\n",
      "CE: 0.5080, OBJ: 0.3704, TOTAL: 0.4805\n",
      "CE: 0.4065, OBJ: 0.3601, TOTAL: 0.3972\n",
      "CE: 0.4672, OBJ: 0.2790, TOTAL: 0.4295\n",
      "CE: 0.7171, OBJ: 0.3266, TOTAL: 0.6390\n",
      "CE: 0.2553, OBJ: 0.3348, TOTAL: 0.2712\n",
      "CE: 0.2407, OBJ: 0.3360, TOTAL: 0.2598\n",
      "CE: 0.4689, OBJ: 0.4002, TOTAL: 0.4551\n",
      "CE: 0.5050, OBJ: 0.4981, TOTAL: 0.5036\n",
      "CE: 0.4424, OBJ: 0.4873, TOTAL: 0.4514\n",
      "CE: 0.4493, OBJ: 0.3417, TOTAL: 0.4278\n",
      "CE: 0.2960, OBJ: 0.3442, TOTAL: 0.3056\n",
      "CE: 0.3253, OBJ: 0.3684, TOTAL: 0.3339\n",
      "CE: 0.2464, OBJ: 0.2886, TOTAL: 0.2548\n",
      "CE: 0.3682, OBJ: 0.3803, TOTAL: 0.3706\n",
      "CE: 0.2304, OBJ: 0.3909, TOTAL: 0.2625\n",
      "CE: 0.6669, OBJ: 0.2901, TOTAL: 0.5916\n",
      "CE: 0.1072, OBJ: 0.3083, TOTAL: 0.1474\n",
      "CE: 0.1240, OBJ: 0.2394, TOTAL: 0.1471\n",
      "CE: 0.2802, OBJ: 0.2483, TOTAL: 0.2738\n",
      "CE: 0.2983, OBJ: 0.2399, TOTAL: 0.2866\n",
      "CE: 0.0902, OBJ: 0.4058, TOTAL: 0.1533\n",
      "CE: 0.6407, OBJ: 0.4710, TOTAL: 0.6068\n",
      "CE: 0.2955, OBJ: 0.2639, TOTAL: 0.2892\n",
      "CE: 0.2161, OBJ: 0.3133, TOTAL: 0.2356\n",
      "CE: 0.2060, OBJ: 0.3357, TOTAL: 0.2320\n",
      "CE: 0.7023, OBJ: 0.3935, TOTAL: 0.6405\n",
      "CE: 0.3108, OBJ: 0.3500, TOTAL: 0.3186\n",
      "CE: 0.4805, OBJ: 0.3583, TOTAL: 0.4560\n",
      "CE: 0.3436, OBJ: 0.4375, TOTAL: 0.3624\n",
      "CE: 0.3694, OBJ: 0.4840, TOTAL: 0.3923\n",
      "CE: 0.5160, OBJ: 0.2306, TOTAL: 0.4589\n",
      "CE: 0.2029, OBJ: 0.3306, TOTAL: 0.2285\n",
      "CE: 0.1449, OBJ: 0.4102, TOTAL: 0.1979\n",
      "CE: 0.9748, OBJ: 0.5409, TOTAL: 0.8880\n",
      "CE: 0.3582, OBJ: 0.4267, TOTAL: 0.3719\n",
      "CE: 0.3660, OBJ: 0.3611, TOTAL: 0.3650\n",
      "CE: 0.6236, OBJ: 0.3558, TOTAL: 0.5700\n",
      "CE: 0.3802, OBJ: 0.3140, TOTAL: 0.3670\n",
      "CE: 0.8318, OBJ: 0.5391, TOTAL: 0.7733\n",
      "CE: 0.4341, OBJ: 0.2672, TOTAL: 0.4007\n",
      "CE: 0.3931, OBJ: 0.2340, TOTAL: 0.3613\n",
      "CE: 0.1630, OBJ: 0.4012, TOTAL: 0.2107\n",
      "CE: 0.0923, OBJ: 0.2352, TOTAL: 0.1209\n",
      "CE: 0.2227, OBJ: 0.3438, TOTAL: 0.2469\n",
      "CE: 0.4542, OBJ: 0.3432, TOTAL: 0.4320\n",
      "CE: 0.3579, OBJ: 0.3292, TOTAL: 0.3522\n",
      "CE: 0.4524, OBJ: 0.1955, TOTAL: 0.4010\n",
      "CE: 0.3457, OBJ: 0.3542, TOTAL: 0.3474\n",
      "CE: 0.7428, OBJ: 0.5021, TOTAL: 0.6947\n",
      "CE: 0.1702, OBJ: 0.3313, TOTAL: 0.2024\n",
      "CE: 0.8501, OBJ: 0.3181, TOTAL: 0.7437\n",
      "CE: 0.4840, OBJ: 0.2273, TOTAL: 0.4327\n",
      "CE: 0.3137, OBJ: 0.4295, TOTAL: 0.3369\n",
      "CE: 0.1988, OBJ: 0.2495, TOTAL: 0.2089\n",
      "CE: 0.4402, OBJ: 0.3415, TOTAL: 0.4204\n",
      "CE: 0.2813, OBJ: 0.3144, TOTAL: 0.2879\n",
      "CE: 0.0863, OBJ: 0.3467, TOTAL: 0.1384\n",
      "CE: 0.9228, OBJ: 0.3137, TOTAL: 0.8010\n",
      "CE: 0.5050, OBJ: 0.2806, TOTAL: 0.4601\n",
      "CE: 0.4077, OBJ: 0.4602, TOTAL: 0.4182\n",
      "CE: 0.0687, OBJ: 0.3424, TOTAL: 0.1234\n",
      "CE: 0.2761, OBJ: 0.4822, TOTAL: 0.3173\n",
      "CE: 0.3493, OBJ: 0.5307, TOTAL: 0.3856\n",
      "CE: 0.2493, OBJ: 0.4014, TOTAL: 0.2797\n",
      "CE: 0.2520, OBJ: 0.4475, TOTAL: 0.2911\n",
      "CE: 0.2162, OBJ: 0.3365, TOTAL: 0.2402\n",
      "CE: 0.2774, OBJ: 0.4381, TOTAL: 0.3096\n",
      "CE: 0.1951, OBJ: 0.2180, TOTAL: 0.1997\n",
      "CE: 0.1984, OBJ: 0.4862, TOTAL: 0.2560\n",
      "CE: 0.3702, OBJ: 0.4836, TOTAL: 0.3929\n",
      "CE: 0.2098, OBJ: 0.2102, TOTAL: 0.2098\n",
      "CE: 0.3656, OBJ: 0.3427, TOTAL: 0.3611\n",
      "CE: 0.3239, OBJ: 0.2897, TOTAL: 0.3171\n",
      "CE: 0.4644, OBJ: 0.3651, TOTAL: 0.4445\n",
      "CE: 0.4705, OBJ: 0.3488, TOTAL: 0.4462\n",
      "CE: 0.3239, OBJ: 0.4111, TOTAL: 0.3413\n",
      "CE: 0.5495, OBJ: 0.4487, TOTAL: 0.5294\n",
      "CE: 0.5112, OBJ: 0.3792, TOTAL: 0.4848\n",
      "CE: 0.1938, OBJ: 0.3624, TOTAL: 0.2275\n",
      "CE: 0.3601, OBJ: 0.3782, TOTAL: 0.3637\n",
      "CE: 0.3431, OBJ: 0.2963, TOTAL: 0.3337\n",
      "CE: 0.1452, OBJ: 0.2787, TOTAL: 0.1719\n",
      "CE: 0.2348, OBJ: 0.1804, TOTAL: 0.2239\n",
      "CE: 0.1297, OBJ: 0.3042, TOTAL: 0.1646\n",
      "CE: 0.2710, OBJ: 0.4169, TOTAL: 0.3002\n",
      "CE: 0.2828, OBJ: 0.3384, TOTAL: 0.2939\n",
      "CE: 0.2667, OBJ: 0.2531, TOTAL: 0.2640\n",
      "CE: 0.2713, OBJ: 0.4374, TOTAL: 0.3045\n",
      "CE: 0.3183, OBJ: 0.3932, TOTAL: 0.3333\n",
      "CE: 0.2084, OBJ: 0.3159, TOTAL: 0.2299\n",
      "CE: 0.3642, OBJ: 0.3212, TOTAL: 0.3556\n",
      "CE: 0.5461, OBJ: 0.2310, TOTAL: 0.4831\n",
      "CE: 0.3468, OBJ: 0.3629, TOTAL: 0.3500\n",
      "CE: 0.3983, OBJ: 0.3542, TOTAL: 0.3895\n",
      "CE: 0.2885, OBJ: 0.4166, TOTAL: 0.3141\n",
      "CE: 0.3250, OBJ: 0.3991, TOTAL: 0.3398\n",
      "CE: 0.3213, OBJ: 0.3746, TOTAL: 0.3319\n",
      "CE: 0.2483, OBJ: 0.3967, TOTAL: 0.2779\n",
      "CE: 0.6455, OBJ: 0.2598, TOTAL: 0.5683\n",
      "CE: 0.1935, OBJ: 0.4489, TOTAL: 0.2446\n",
      "CE: 0.1118, OBJ: 0.1588, TOTAL: 0.1212\n",
      "CE: 0.4610, OBJ: 0.3947, TOTAL: 0.4478\n",
      "CE: 0.3002, OBJ: 0.4845, TOTAL: 0.3370\n",
      "CE: 0.2339, OBJ: 0.2689, TOTAL: 0.2409\n",
      "CE: 0.3176, OBJ: 0.5211, TOTAL: 0.3583\n",
      "CE: 0.3161, OBJ: 0.2834, TOTAL: 0.3095\n",
      "CE: 0.4618, OBJ: 0.2864, TOTAL: 0.4267\n",
      "CE: 0.2892, OBJ: 0.3061, TOTAL: 0.2926\n",
      "CE: 0.1347, OBJ: 0.3886, TOTAL: 0.1855\n",
      "CE: 0.1170, OBJ: 0.5303, TOTAL: 0.1996\n",
      "CE: 0.3339, OBJ: 0.2598, TOTAL: 0.3191\n",
      "CE: 0.2128, OBJ: 0.2920, TOTAL: 0.2286\n",
      "CE: 0.3397, OBJ: 0.3442, TOTAL: 0.3406\n",
      "CE: 0.7969, OBJ: 0.3159, TOTAL: 0.7007\n",
      "CE: 0.3009, OBJ: 0.2465, TOTAL: 0.2900\n",
      "CE: 0.2040, OBJ: 0.2559, TOTAL: 0.2143\n",
      "CE: 0.2816, OBJ: 0.1938, TOTAL: 0.2640\n",
      "CE: 0.4185, OBJ: 0.2922, TOTAL: 0.3932\n",
      "CE: 0.1110, OBJ: 0.1303, TOTAL: 0.1149\n",
      "CE: 0.2339, OBJ: 0.3534, TOTAL: 0.2578\n",
      "CE: 0.1660, OBJ: 0.2801, TOTAL: 0.1888\n",
      "CE: 0.2822, OBJ: 0.2567, TOTAL: 0.2771\n",
      "CE: 0.5018, OBJ: 0.3505, TOTAL: 0.4716\n",
      "CE: 0.4964, OBJ: 0.3116, TOTAL: 0.4595\n",
      "CE: 0.2125, OBJ: 0.2627, TOTAL: 0.2225\n",
      "CE: 0.1247, OBJ: 0.2426, TOTAL: 0.1483\n",
      "CE: 0.1615, OBJ: 0.2384, TOTAL: 0.1769\n",
      "CE: 0.4802, OBJ: 0.2779, TOTAL: 0.4397\n",
      "CE: 0.3117, OBJ: 0.4614, TOTAL: 0.3416\n",
      "CE: 0.3040, OBJ: 0.3582, TOTAL: 0.3148\n",
      "CE: 0.5136, OBJ: 0.3123, TOTAL: 0.4733\n",
      "CE: 0.5706, OBJ: 0.3510, TOTAL: 0.5267\n",
      "CE: 0.3079, OBJ: 0.3295, TOTAL: 0.3122\n",
      "CE: 0.2822, OBJ: 0.2873, TOTAL: 0.2832\n",
      "CE: 0.6377, OBJ: 0.4336, TOTAL: 0.5969\n",
      "CE: 0.0686, OBJ: 0.2673, TOTAL: 0.1083\n",
      "CE: 0.5266, OBJ: 0.4212, TOTAL: 0.5055\n",
      "CE: 0.7087, OBJ: 0.3465, TOTAL: 0.6363\n",
      "CE: 0.3038, OBJ: 0.2883, TOTAL: 0.3007\n",
      "CE: 0.1062, OBJ: 0.2880, TOTAL: 0.1426\n",
      "CE: 0.0682, OBJ: 0.2295, TOTAL: 0.1004\n",
      "CE: 0.1693, OBJ: 0.1553, TOTAL: 0.1665\n",
      "CE: 0.3934, OBJ: 0.4033, TOTAL: 0.3954\n",
      "CE: 0.1644, OBJ: 0.2387, TOTAL: 0.1793\n",
      "CE: 0.4858, OBJ: 0.3228, TOTAL: 0.4532\n",
      "CE: 0.5670, OBJ: 0.3530, TOTAL: 0.5242\n",
      "CE: 0.2772, OBJ: 0.3020, TOTAL: 0.2822\n",
      "CE: 0.1978, OBJ: 0.3301, TOTAL: 0.2242\n",
      "CE: 0.3088, OBJ: 0.5874, TOTAL: 0.3645\n",
      "CE: 0.3276, OBJ: 0.2917, TOTAL: 0.3204\n",
      "CE: 0.4364, OBJ: 0.2492, TOTAL: 0.3989\n",
      "CE: 0.5866, OBJ: 0.2453, TOTAL: 0.5183\n",
      "CE: 0.4732, OBJ: 0.2376, TOTAL: 0.4261\n",
      "CE: 0.0760, OBJ: 0.2575, TOTAL: 0.1123\n",
      "CE: 0.2230, OBJ: 0.1349, TOTAL: 0.2054\n",
      "CE: 0.2796, OBJ: 0.2694, TOTAL: 0.2775\n",
      "CE: 0.1465, OBJ: 0.2450, TOTAL: 0.1662\n",
      "CE: 0.1833, OBJ: 0.3366, TOTAL: 0.2140\n",
      "CE: 0.2869, OBJ: 0.3166, TOTAL: 0.2928\n",
      "CE: 0.5242, OBJ: 0.3216, TOTAL: 0.4837\n",
      "CE: 0.2235, OBJ: 0.4179, TOTAL: 0.2624\n",
      "CE: 0.4180, OBJ: 0.2944, TOTAL: 0.3933\n",
      "CE: 0.7929, OBJ: 0.3490, TOTAL: 0.7041\n",
      "CE: 0.3495, OBJ: 0.3744, TOTAL: 0.3545\n",
      "CE: 0.4616, OBJ: 0.3387, TOTAL: 0.4370\n",
      "CE: 0.4426, OBJ: 0.4055, TOTAL: 0.4352\n",
      "CE: 0.0632, OBJ: 0.2552, TOTAL: 0.1016\n",
      "CE: 0.3146, OBJ: 0.4805, TOTAL: 0.3478\n",
      "CE: 0.2675, OBJ: 0.3285, TOTAL: 0.2797\n",
      "CE: 0.2717, OBJ: 0.2531, TOTAL: 0.2679\n",
      "CE: 0.5336, OBJ: 0.3081, TOTAL: 0.4885\n",
      "CE: 0.4068, OBJ: 0.4366, TOTAL: 0.4128\n",
      "CE: 0.1975, OBJ: 0.1752, TOTAL: 0.1930\n",
      "CE: 0.3888, OBJ: 0.4115, TOTAL: 0.3933\n",
      "CE: 0.2220, OBJ: 0.4111, TOTAL: 0.2598\n",
      "CE: 0.3238, OBJ: 0.3542, TOTAL: 0.3299\n",
      "CE: 0.2961, OBJ: 0.3782, TOTAL: 0.3125\n",
      "CE: 0.6057, OBJ: 0.3785, TOTAL: 0.5603\n",
      "CE: 0.2264, OBJ: 0.3675, TOTAL: 0.2546\n",
      "CE: 0.3163, OBJ: 0.4271, TOTAL: 0.3385\n",
      "CE: 0.3275, OBJ: 0.3524, TOTAL: 0.3325\n",
      "CE: 0.2696, OBJ: 0.2870, TOTAL: 0.2731\n",
      "CE: 0.2402, OBJ: 0.2444, TOTAL: 0.2411\n",
      "CE: 0.5022, OBJ: 0.2383, TOTAL: 0.4494\n",
      "CE: 0.2917, OBJ: 0.2816, TOTAL: 0.2897\n",
      "CE: 0.3948, OBJ: 0.3117, TOTAL: 0.3782\n",
      "CE: 0.2972, OBJ: 0.4326, TOTAL: 0.3243\n",
      "CE: 0.2150, OBJ: 0.2599, TOTAL: 0.2240\n",
      "CE: 0.0923, OBJ: 0.2586, TOTAL: 0.1255\n",
      "CE: 0.3436, OBJ: 0.3860, TOTAL: 0.3521\n",
      "CE: 0.3547, OBJ: 0.3960, TOTAL: 0.3629\n",
      "CE: 0.3445, OBJ: 0.3129, TOTAL: 0.3382\n",
      "CE: 0.3078, OBJ: 0.3120, TOTAL: 0.3086\n",
      "CE: 0.3195, OBJ: 0.4199, TOTAL: 0.3396\n",
      "CE: 0.0682, OBJ: 0.2168, TOTAL: 0.0979\n",
      "CE: 0.8057, OBJ: 0.5178, TOTAL: 0.7481\n",
      "CE: 0.3476, OBJ: 0.2218, TOTAL: 0.3224\n",
      "CE: 0.4763, OBJ: 0.3165, TOTAL: 0.4443\n",
      "CE: 0.3745, OBJ: 0.3616, TOTAL: 0.3719\n",
      "CE: 0.3158, OBJ: 0.4033, TOTAL: 0.3333\n",
      "CE: 0.1582, OBJ: 0.1780, TOTAL: 0.1621\n",
      "CE: 0.3850, OBJ: 0.2932, TOTAL: 0.3666\n",
      "CE: 0.4112, OBJ: 0.5891, TOTAL: 0.4468\n",
      "CE: 0.5058, OBJ: 0.3044, TOTAL: 0.4656\n",
      "CE: 0.2594, OBJ: 0.3661, TOTAL: 0.2807\n",
      "CE: 0.6193, OBJ: 0.3069, TOTAL: 0.5568\n",
      "CE: 0.3476, OBJ: 0.3640, TOTAL: 0.3509\n",
      "CE: 0.1778, OBJ: 0.2642, TOTAL: 0.1951\n",
      "CE: 0.1965, OBJ: 0.2749, TOTAL: 0.2122\n",
      "CE: 0.1675, OBJ: 0.2140, TOTAL: 0.1768\n",
      "CE: 0.3799, OBJ: 0.2395, TOTAL: 0.3518\n",
      "CE: 0.1706, OBJ: 0.2180, TOTAL: 0.1801\n",
      "CE: 0.4525, OBJ: 0.3224, TOTAL: 0.4265\n",
      "CE: 0.6071, OBJ: 0.4793, TOTAL: 0.5815\n",
      "CE: 0.2578, OBJ: 0.1959, TOTAL: 0.2454\n",
      "CE: 0.3910, OBJ: 0.1945, TOTAL: 0.3517\n",
      "CE: 0.6256, OBJ: 0.4574, TOTAL: 0.5920\n",
      "CE: 0.3159, OBJ: 0.4210, TOTAL: 0.3369\n",
      "CE: 0.4868, OBJ: 0.2368, TOTAL: 0.4368\n",
      "CE: 0.5069, OBJ: 0.4647, TOTAL: 0.4985\n",
      "CE: 0.2585, OBJ: 0.2093, TOTAL: 0.2487\n",
      "CE: 0.1261, OBJ: 0.2171, TOTAL: 0.1443\n",
      "CE: 0.4956, OBJ: 0.2715, TOTAL: 0.4508\n",
      "CE: 0.2341, OBJ: 0.2232, TOTAL: 0.2319\n",
      "CE: 0.2515, OBJ: 0.2417, TOTAL: 0.2495\n",
      "CE: 0.2923, OBJ: 0.3512, TOTAL: 0.3040\n",
      "CE: 0.1707, OBJ: 0.2621, TOTAL: 0.1890\n",
      "CE: 0.2070, OBJ: 0.3165, TOTAL: 0.2289\n",
      "CE: 0.6078, OBJ: 0.3631, TOTAL: 0.5588\n",
      "CE: 0.3474, OBJ: 0.4007, TOTAL: 0.3580\n",
      "CE: 0.1029, OBJ: 0.3215, TOTAL: 0.1466\n",
      "CE: 0.5790, OBJ: 0.4138, TOTAL: 0.5459\n",
      "CE: 0.3956, OBJ: 0.3251, TOTAL: 0.3815\n",
      "CE: 0.2543, OBJ: 0.3744, TOTAL: 0.2783\n",
      "CE: 0.0932, OBJ: 0.3643, TOTAL: 0.1475\n",
      "CE: 0.4597, OBJ: 0.2781, TOTAL: 0.4233\n",
      "CE: 0.1241, OBJ: 0.2045, TOTAL: 0.1402\n",
      "CE: 0.4070, OBJ: 0.3822, TOTAL: 0.4021\n",
      "CE: 0.2170, OBJ: 0.2730, TOTAL: 0.2282\n",
      "CE: 0.4219, OBJ: 0.2701, TOTAL: 0.3915\n",
      "CE: 0.5168, OBJ: 0.2321, TOTAL: 0.4598\n",
      "CE: 0.3436, OBJ: 0.3287, TOTAL: 0.3406\n",
      "CE: 0.2623, OBJ: 0.3723, TOTAL: 0.2843\n",
      "CE: 0.6047, OBJ: 0.4617, TOTAL: 0.5761\n",
      "CE: 0.2228, OBJ: 0.3993, TOTAL: 0.2581\n",
      "CE: 0.4812, OBJ: 0.3746, TOTAL: 0.4599\n",
      "CE: 0.5617, OBJ: 0.3656, TOTAL: 0.5225\n",
      "CE: 0.5706, OBJ: 0.3214, TOTAL: 0.5207\n",
      "CE: 0.7549, OBJ: 0.3802, TOTAL: 0.6799\n",
      "CE: 0.5330, OBJ: 0.4205, TOTAL: 0.5105\n",
      "CE: 0.1048, OBJ: 0.2132, TOTAL: 0.1265\n",
      "CE: 0.1232, OBJ: 0.4915, TOTAL: 0.1969\n",
      "CE: 0.3625, OBJ: 0.4096, TOTAL: 0.3719\n",
      "CE: 0.4490, OBJ: 0.3081, TOTAL: 0.4208\n",
      "CE: 0.1731, OBJ: 0.2998, TOTAL: 0.1985\n",
      "CE: 0.2974, OBJ: 0.3363, TOTAL: 0.3052\n",
      "CE: 0.6436, OBJ: 0.3971, TOTAL: 0.5943\n",
      "CE: 0.3915, OBJ: 0.3372, TOTAL: 0.3806\n",
      "CE: 0.2654, OBJ: 0.3159, TOTAL: 0.2755\n",
      "CE: 0.1384, OBJ: 0.2525, TOTAL: 0.1612\n",
      "CE: 0.3092, OBJ: 0.3192, TOTAL: 0.3112\n",
      "CE: 0.5218, OBJ: 0.4918, TOTAL: 0.5158\n",
      "CE: 0.1797, OBJ: 0.4267, TOTAL: 0.2291\n",
      "CE: 0.2976, OBJ: 0.1858, TOTAL: 0.2752\n",
      "CE: 0.1974, OBJ: 0.4406, TOTAL: 0.2460\n",
      "CE: 0.2375, OBJ: 0.3449, TOTAL: 0.2590\n",
      "CE: 0.1541, OBJ: 0.2249, TOTAL: 0.1683\n",
      "CE: 0.3938, OBJ: 0.3852, TOTAL: 0.3921\n",
      "CE: 0.2868, OBJ: 0.4106, TOTAL: 0.3116\n",
      "CE: 0.2306, OBJ: 0.2679, TOTAL: 0.2380\n",
      "CE: 0.4449, OBJ: 0.3775, TOTAL: 0.4314\n",
      "CE: 0.3112, OBJ: 0.3219, TOTAL: 0.3133\n",
      "CE: 0.2757, OBJ: 0.4054, TOTAL: 0.3017\n",
      "CE: 0.2186, OBJ: 0.3329, TOTAL: 0.2415\n",
      "CE: 0.2830, OBJ: 0.4079, TOTAL: 0.3080\n",
      "CE: 0.3118, OBJ: 0.3428, TOTAL: 0.3180\n",
      "CE: 0.5156, OBJ: 0.4232, TOTAL: 0.4971\n",
      "CE: 0.1081, OBJ: 0.2799, TOTAL: 0.1425\n",
      "CE: 0.3937, OBJ: 0.4381, TOTAL: 0.4026\n",
      "CE: 0.2880, OBJ: 0.2129, TOTAL: 0.2730\n",
      "CE: 0.2543, OBJ: 0.2536, TOTAL: 0.2541\n",
      "CE: 0.1694, OBJ: 0.2850, TOTAL: 0.1925\n",
      "CE: 0.3011, OBJ: 0.3186, TOTAL: 0.3046\n",
      "CE: 0.1662, OBJ: 0.2065, TOTAL: 0.1742\n",
      "CE: 0.3686, OBJ: 0.4210, TOTAL: 0.3791\n",
      "CE: 0.3319, OBJ: 0.4024, TOTAL: 0.3460\n",
      "CE: 0.2573, OBJ: 0.2326, TOTAL: 0.2524\n",
      "CE: 0.1601, OBJ: 0.1988, TOTAL: 0.1679\n",
      "CE: 0.3370, OBJ: 0.2473, TOTAL: 0.3191\n",
      "CE: 0.1536, OBJ: 0.3349, TOTAL: 0.1899\n",
      "CE: 0.3419, OBJ: 0.5095, TOTAL: 0.3754\n",
      "CE: 0.2840, OBJ: 0.2777, TOTAL: 0.2828\n",
      "CE: 0.2227, OBJ: 0.3249, TOTAL: 0.2432\n",
      "CE: 0.2182, OBJ: 0.2531, TOTAL: 0.2252\n",
      "CE: 0.5874, OBJ: 0.4294, TOTAL: 0.5558\n",
      "CE: 0.2306, OBJ: 0.2921, TOTAL: 0.2429\n",
      "CE: 0.0818, OBJ: 0.1472, TOTAL: 0.0949\n",
      "CE: 0.5937, OBJ: 0.3561, TOTAL: 0.5462\n",
      "CE: 0.4667, OBJ: 0.3421, TOTAL: 0.4418\n",
      "CE: 0.5090, OBJ: 0.3667, TOTAL: 0.4805\n",
      "CE: 0.2831, OBJ: 0.3041, TOTAL: 0.2873\n",
      "CE: 0.6055, OBJ: 0.3056, TOTAL: 0.5455\n",
      "CE: 0.3394, OBJ: 0.4996, TOTAL: 0.3715\n",
      "CE: 0.3665, OBJ: 0.2183, TOTAL: 0.3368\n",
      "CE: 0.2921, OBJ: 0.2628, TOTAL: 0.2862\n",
      "CE: 0.3305, OBJ: 0.2778, TOTAL: 0.3199\n",
      "CE: 0.4127, OBJ: 0.3267, TOTAL: 0.3955\n",
      "CE: 0.4980, OBJ: 0.2809, TOTAL: 0.4546\n",
      "CE: 0.3862, OBJ: 0.2158, TOTAL: 0.3521\n",
      "CE: 0.2025, OBJ: 0.2586, TOTAL: 0.2137\n",
      "CE: 0.2200, OBJ: 0.3693, TOTAL: 0.2498\n",
      "CE: 0.1883, OBJ: 0.2401, TOTAL: 0.1986\n",
      "CE: 0.1295, OBJ: 0.3397, TOTAL: 0.1715\n",
      "CE: 0.3286, OBJ: 0.3524, TOTAL: 0.3333\n",
      "CE: 0.4855, OBJ: 0.2987, TOTAL: 0.4482\n",
      "CE: 0.3973, OBJ: 0.3730, TOTAL: 0.3925\n",
      "CE: 0.1492, OBJ: 0.2736, TOTAL: 0.1741\n",
      "CE: 0.0878, OBJ: 0.3610, TOTAL: 0.1425\n",
      "CE: 0.2476, OBJ: 0.3554, TOTAL: 0.2692\n",
      "CE: 0.2207, OBJ: 0.3485, TOTAL: 0.2462\n",
      "CE: 0.2064, OBJ: 0.3402, TOTAL: 0.2332\n",
      "CE: 0.7041, OBJ: 0.2779, TOTAL: 0.6189\n",
      "CE: 0.1111, OBJ: 0.3950, TOTAL: 0.1679\n",
      "CE: 0.3359, OBJ: 0.4166, TOTAL: 0.3520\n",
      "CE: 0.2965, OBJ: 0.3534, TOTAL: 0.3079\n",
      "CE: 0.5234, OBJ: 0.3821, TOTAL: 0.4951\n",
      "CE: 0.3447, OBJ: 0.2800, TOTAL: 0.3318\n",
      "CE: 0.4670, OBJ: 0.3784, TOTAL: 0.4493\n",
      "CE: 0.0958, OBJ: 0.1125, TOTAL: 0.0991\n",
      "CE: 0.2610, OBJ: 0.3166, TOTAL: 0.2721\n",
      "CE: 0.5540, OBJ: 0.3332, TOTAL: 0.5098\n",
      "CE: 0.3747, OBJ: 0.3380, TOTAL: 0.3674\n",
      "CE: 0.4667, OBJ: 0.2976, TOTAL: 0.4329\n",
      "CE: 0.3780, OBJ: 0.3806, TOTAL: 0.3786\n",
      "CE: 0.1242, OBJ: 0.2117, TOTAL: 0.1417\n",
      "CE: 0.3029, OBJ: 0.3376, TOTAL: 0.3098\n",
      "CE: 0.6879, OBJ: 0.3067, TOTAL: 0.6117\n",
      "CE: 0.3348, OBJ: 0.3266, TOTAL: 0.3332\n",
      "CE: 0.2536, OBJ: 0.2877, TOTAL: 0.2604\n",
      "CE: 0.2433, OBJ: 0.3327, TOTAL: 0.2611\n",
      "CE: 0.8417, OBJ: 0.2251, TOTAL: 0.7184\n",
      "CE: 0.4087, OBJ: 0.2140, TOTAL: 0.3697\n",
      "CE: 0.6606, OBJ: 0.3124, TOTAL: 0.5910\n",
      "CE: 0.4230, OBJ: 0.4167, TOTAL: 0.4217\n",
      "CE: 0.3081, OBJ: 0.3370, TOTAL: 0.3139\n",
      "CE: 0.3286, OBJ: 0.3603, TOTAL: 0.3349\n",
      "CE: 0.3629, OBJ: 0.2837, TOTAL: 0.3471\n",
      "CE: 0.3283, OBJ: 0.2686, TOTAL: 0.3164\n",
      "CE: 0.2050, OBJ: 0.2026, TOTAL: 0.2045\n",
      "CE: 0.4205, OBJ: 0.2801, TOTAL: 0.3924\n",
      "CE: 1.0833, OBJ: 0.5032, TOTAL: 0.9673\n",
      "CE: 0.1843, OBJ: 0.1941, TOTAL: 0.1863\n",
      "CE: 0.1771, OBJ: 0.3974, TOTAL: 0.2212\n",
      "CE: 0.2105, OBJ: 0.3688, TOTAL: 0.2421\n",
      "CE: 0.4867, OBJ: 0.4060, TOTAL: 0.4706\n",
      "CE: 0.0336, OBJ: 0.2081, TOTAL: 0.0685\n",
      "CE: 0.1866, OBJ: 0.3266, TOTAL: 0.2146\n",
      "CE: 0.4430, OBJ: 0.2948, TOTAL: 0.4134\n",
      "CE: 0.1715, OBJ: 0.3637, TOTAL: 0.2100\n",
      "CE: 0.3254, OBJ: 0.3645, TOTAL: 0.3332\n",
      "CE: 0.5581, OBJ: 0.3534, TOTAL: 0.5172\n",
      "CE: 0.2894, OBJ: 0.4075, TOTAL: 0.3130\n",
      "CE: 0.1819, OBJ: 0.4450, TOTAL: 0.2346\n",
      "CE: 0.1136, OBJ: 0.4382, TOTAL: 0.1785\n",
      "CE: 0.1508, OBJ: 0.4024, TOTAL: 0.2012\n",
      "CE: 0.3261, OBJ: 0.2456, TOTAL: 0.3100\n",
      "CE: 0.2778, OBJ: 0.3335, TOTAL: 0.2889\n",
      "CE: 0.4484, OBJ: 0.2664, TOTAL: 0.4120\n",
      "CE: 0.2251, OBJ: 0.4351, TOTAL: 0.2671\n",
      "CE: 0.3619, OBJ: 0.3500, TOTAL: 0.3595\n",
      "CE: 0.6557, OBJ: 0.2567, TOTAL: 0.5759\n",
      "CE: 0.3481, OBJ: 0.4084, TOTAL: 0.3602\n",
      "CE: 0.3857, OBJ: 0.3174, TOTAL: 0.3720\n",
      "CE: 0.5578, OBJ: 0.4404, TOTAL: 0.5343\n",
      "CE: 0.2716, OBJ: 0.1937, TOTAL: 0.2560\n",
      "CE: 0.4638, OBJ: 0.2802, TOTAL: 0.4271\n",
      "CE: 0.1415, OBJ: 0.3490, TOTAL: 0.1830\n",
      "CE: 0.2673, OBJ: 0.4898, TOTAL: 0.3118\n",
      "CE: 0.4372, OBJ: 0.3736, TOTAL: 0.4245\n",
      "CE: 0.2083, OBJ: 0.4440, TOTAL: 0.2555\n",
      "CE: 0.4260, OBJ: 0.4562, TOTAL: 0.4320\n",
      "CE: 0.1664, OBJ: 0.2700, TOTAL: 0.1871\n",
      "CE: 0.2657, OBJ: 0.3325, TOTAL: 0.2791\n",
      "CE: 0.5175, OBJ: 0.3517, TOTAL: 0.4843\n",
      "CE: 0.2147, OBJ: 0.2672, TOTAL: 0.2252\n",
      "CE: 0.3379, OBJ: 0.3212, TOTAL: 0.3346\n",
      "CE: 0.4453, OBJ: 0.3456, TOTAL: 0.4253\n",
      "CE: 0.0803, OBJ: 0.2117, TOTAL: 0.1066\n",
      "CE: 0.3563, OBJ: 0.2714, TOTAL: 0.3393\n",
      "CE: 0.4961, OBJ: 0.3984, TOTAL: 0.4766\n",
      "CE: 0.3428, OBJ: 0.2187, TOTAL: 0.3180\n",
      "CE: 0.2855, OBJ: 0.2510, TOTAL: 0.2786\n",
      "CE: 0.2452, OBJ: 0.3441, TOTAL: 0.2650\n",
      "CE: 0.4162, OBJ: 0.3584, TOTAL: 0.4047\n",
      "CE: 0.2411, OBJ: 0.3014, TOTAL: 0.2532\n",
      "CE: 0.3772, OBJ: 0.3741, TOTAL: 0.3766\n",
      "CE: 0.2129, OBJ: 0.2313, TOTAL: 0.2166\n",
      "CE: 0.0754, OBJ: 0.1826, TOTAL: 0.0968\n",
      "CE: 0.5141, OBJ: 0.2351, TOTAL: 0.4583\n",
      "CE: 0.4069, OBJ: 0.4374, TOTAL: 0.4130\n",
      "CE: 0.1256, OBJ: 0.2077, TOTAL: 0.1420\n",
      "CE: 0.2340, OBJ: 0.2894, TOTAL: 0.2451\n",
      "CE: 0.2845, OBJ: 0.3493, TOTAL: 0.2975\n",
      "CE: 0.1060, OBJ: 0.4077, TOTAL: 0.1663\n",
      "CE: 0.3165, OBJ: 0.2430, TOTAL: 0.3018\n",
      "CE: 0.1003, OBJ: 0.1932, TOTAL: 0.1189\n",
      "CE: 0.3073, OBJ: 0.1346, TOTAL: 0.2727\n",
      "CE: 0.3667, OBJ: 0.3927, TOTAL: 0.3719\n",
      "CE: 0.4323, OBJ: 0.2695, TOTAL: 0.3997\n",
      "CE: 0.2431, OBJ: 0.2137, TOTAL: 0.2372\n",
      "CE: 0.2316, OBJ: 0.2882, TOTAL: 0.2429\n",
      "CE: 0.5315, OBJ: 0.2918, TOTAL: 0.4835\n",
      "CE: 0.2202, OBJ: 0.4069, TOTAL: 0.2576\n",
      "CE: 0.2262, OBJ: 0.2536, TOTAL: 0.2317\n",
      "CE: 0.4409, OBJ: 0.5261, TOTAL: 0.4579\n",
      "CE: 0.1478, OBJ: 0.4159, TOTAL: 0.2014\n",
      "CE: 0.3267, OBJ: 0.3346, TOTAL: 0.3283\n",
      "CE: 0.1651, OBJ: 0.1827, TOTAL: 0.1686\n",
      "CE: 0.3380, OBJ: 0.3497, TOTAL: 0.3404\n",
      "CE: 0.3264, OBJ: 0.3162, TOTAL: 0.3243\n",
      "CE: 0.1782, OBJ: 0.3152, TOTAL: 0.2056\n",
      "CE: 0.2686, OBJ: 0.4273, TOTAL: 0.3004\n",
      "CE: 0.2287, OBJ: 0.2792, TOTAL: 0.2388\n",
      "CE: 0.5107, OBJ: 0.2550, TOTAL: 0.4596\n",
      "CE: 0.7332, OBJ: 0.3474, TOTAL: 0.6560\n",
      "CE: 0.6436, OBJ: 0.3232, TOTAL: 0.5796\n",
      "CE: 0.1609, OBJ: 0.1755, TOTAL: 0.1638\n",
      "CE: 0.4289, OBJ: 0.3505, TOTAL: 0.4132\n",
      "CE: 0.1282, OBJ: 0.2299, TOTAL: 0.1485\n",
      "CE: 0.0524, OBJ: 0.1129, TOTAL: 0.0645\n",
      "CE: 0.4405, OBJ: 0.4219, TOTAL: 0.4367\n",
      "CE: 0.2624, OBJ: 0.3087, TOTAL: 0.2717\n",
      "CE: 0.1759, OBJ: 0.4016, TOTAL: 0.2210\n",
      "CE: 0.1606, OBJ: 0.1588, TOTAL: 0.1603\n",
      "CE: 0.1299, OBJ: 0.1619, TOTAL: 0.1363\n",
      "CE: 0.3355, OBJ: 0.3853, TOTAL: 0.3454\n",
      "CE: 0.3408, OBJ: 0.3770, TOTAL: 0.3481\n",
      "CE: 0.2973, OBJ: 0.2955, TOTAL: 0.2970\n",
      "CE: 0.3855, OBJ: 0.3428, TOTAL: 0.3770\n",
      "CE: 0.2325, OBJ: 0.4459, TOTAL: 0.2752\n",
      "CE: 0.4587, OBJ: 0.4776, TOTAL: 0.4624\n",
      "CE: 0.2443, OBJ: 0.2218, TOTAL: 0.2398\n",
      "CE: 0.2810, OBJ: 0.1988, TOTAL: 0.2646\n",
      "CE: 0.3971, OBJ: 0.3117, TOTAL: 0.3800\n",
      "CE: 0.3275, OBJ: 0.2409, TOTAL: 0.3101\n",
      "CE: 0.0912, OBJ: 0.3283, TOTAL: 0.1386\n",
      "CE: 0.1914, OBJ: 0.2566, TOTAL: 0.2045\n",
      "CE: 0.1890, OBJ: 0.3767, TOTAL: 0.2265\n",
      "CE: 0.4570, OBJ: 0.2401, TOTAL: 0.4136\n",
      "CE: 0.3892, OBJ: 0.3964, TOTAL: 0.3907\n",
      "CE: 0.4701, OBJ: 0.3275, TOTAL: 0.4416\n",
      "CE: 0.6470, OBJ: 0.2577, TOTAL: 0.5692\n",
      "CE: 0.3936, OBJ: 0.3209, TOTAL: 0.3791\n",
      "CE: 0.1070, OBJ: 0.3091, TOTAL: 0.1474\n",
      "CE: 0.3568, OBJ: 0.2669, TOTAL: 0.3388\n",
      "CE: 0.4288, OBJ: 0.2344, TOTAL: 0.3899\n",
      "CE: 0.1634, OBJ: 0.3740, TOTAL: 0.2055\n",
      "CE: 0.3365, OBJ: 0.1832, TOTAL: 0.3058\n",
      "CE: 0.6565, OBJ: 0.4484, TOTAL: 0.6149\n",
      "CE: 0.3133, OBJ: 0.2989, TOTAL: 0.3105\n",
      "CE: 0.2600, OBJ: 0.2992, TOTAL: 0.2678\n",
      "CE: 0.1337, OBJ: 0.2137, TOTAL: 0.1497\n",
      "CE: 0.0513, OBJ: 0.1741, TOTAL: 0.0758\n",
      "CE: 0.2519, OBJ: 0.2554, TOTAL: 0.2526\n",
      "CE: 0.3095, OBJ: 0.3121, TOTAL: 0.3100\n",
      "CE: 0.3132, OBJ: 0.1786, TOTAL: 0.2863\n",
      "CE: 0.6120, OBJ: 0.3661, TOTAL: 0.5628\n",
      "CE: 0.4883, OBJ: 0.3579, TOTAL: 0.4622\n",
      "CE: 0.1812, OBJ: 0.2188, TOTAL: 0.1887\n",
      "CE: 0.2270, OBJ: 0.2974, TOTAL: 0.2411\n",
      "CE: 0.6302, OBJ: 0.3383, TOTAL: 0.5718\n",
      "CE: 0.1381, OBJ: 0.3168, TOTAL: 0.1739\n",
      "CE: 0.2872, OBJ: 0.4508, TOTAL: 0.3199\n",
      "CE: 0.3605, OBJ: 0.2450, TOTAL: 0.3374\n",
      "CE: 0.2747, OBJ: 0.4167, TOTAL: 0.3031\n",
      "CE: 0.3077, OBJ: 0.1911, TOTAL: 0.2844\n",
      "CE: 0.3161, OBJ: 0.3868, TOTAL: 0.3302\n",
      "CE: 0.3694, OBJ: 0.4894, TOTAL: 0.3934\n",
      "CE: 0.4647, OBJ: 0.3452, TOTAL: 0.4408\n",
      "CE: 0.5310, OBJ: 0.2373, TOTAL: 0.4723\n",
      "CE: 0.4198, OBJ: 0.3745, TOTAL: 0.4107\n",
      "CE: 0.4352, OBJ: 0.2797, TOTAL: 0.4041\n",
      "CE: 0.3449, OBJ: 0.3140, TOTAL: 0.3387\n",
      "CE: 0.3034, OBJ: 0.3943, TOTAL: 0.3216\n",
      "CE: 0.2422, OBJ: 0.1912, TOTAL: 0.2320\n",
      "CE: 0.5398, OBJ: 0.4166, TOTAL: 0.5151\n",
      "CE: 0.2076, OBJ: 0.2461, TOTAL: 0.2153\n",
      "CE: 0.4248, OBJ: 0.2727, TOTAL: 0.3944\n",
      "CE: 0.1085, OBJ: 0.2675, TOTAL: 0.1403\n",
      "CE: 0.8546, OBJ: 0.2793, TOTAL: 0.7395\n",
      "CE: 0.0673, OBJ: 0.2418, TOTAL: 0.1022\n",
      "CE: 0.1759, OBJ: 0.3588, TOTAL: 0.2125\n",
      "CE: 0.1863, OBJ: 0.1859, TOTAL: 0.1862\n",
      "CE: 0.3544, OBJ: 0.3276, TOTAL: 0.3490\n",
      "CE: 0.2005, OBJ: 0.3449, TOTAL: 0.2294\n",
      "CE: 0.1846, OBJ: 0.2643, TOTAL: 0.2005\n",
      "CE: 0.1819, OBJ: 0.3958, TOTAL: 0.2247\n",
      "CE: 0.2220, OBJ: 0.3928, TOTAL: 0.2562\n",
      "CE: 0.3471, OBJ: 0.4947, TOTAL: 0.3766\n",
      "CE: 0.5341, OBJ: 0.4757, TOTAL: 0.5224\n",
      "CE: 0.3027, OBJ: 0.3273, TOTAL: 0.3076\n",
      "CE: 0.4506, OBJ: 0.2893, TOTAL: 0.4183\n",
      "CE: 0.3165, OBJ: 0.4026, TOTAL: 0.3337\n",
      "CE: 0.3337, OBJ: 0.2995, TOTAL: 0.3269\n",
      "CE: 0.5219, OBJ: 0.4193, TOTAL: 0.5014\n",
      "CE: 0.3644, OBJ: 0.2708, TOTAL: 0.3457\n",
      "CE: 0.1273, OBJ: 0.2642, TOTAL: 0.1547\n",
      "CE: 0.5387, OBJ: 0.4248, TOTAL: 0.5159\n",
      "CE: 0.5999, OBJ: 0.1399, TOTAL: 0.5079\n",
      "CE: 0.1939, OBJ: 0.2909, TOTAL: 0.2133\n",
      "CE: 0.1644, OBJ: 0.2669, TOTAL: 0.1849\n",
      "CE: 0.1750, OBJ: 0.3556, TOTAL: 0.2111\n",
      "CE: 0.1085, OBJ: 0.3867, TOTAL: 0.1642\n",
      "CE: 0.8154, OBJ: 0.5283, TOTAL: 0.7580\n",
      "CE: 0.5346, OBJ: 0.3532, TOTAL: 0.4983\n",
      "CE: 0.4495, OBJ: 0.4500, TOTAL: 0.4496\n",
      "CE: 0.1987, OBJ: 0.3294, TOTAL: 0.2248\n",
      "CE: 0.5147, OBJ: 0.3364, TOTAL: 0.4790\n",
      "CE: 0.3243, OBJ: 0.3898, TOTAL: 0.3374\n",
      "CE: 0.6143, OBJ: 0.3685, TOTAL: 0.5651\n",
      "CE: 0.2595, OBJ: 0.3530, TOTAL: 0.2782\n",
      "CE: 0.1620, OBJ: 0.3968, TOTAL: 0.2089\n",
      "CE: 0.2034, OBJ: 0.2688, TOTAL: 0.2165\n",
      "CE: 0.4491, OBJ: 0.2270, TOTAL: 0.4047\n",
      "CE: 0.1549, OBJ: 0.3327, TOTAL: 0.1905\n",
      "CE: 0.6038, OBJ: 0.4075, TOTAL: 0.5645\n",
      "CE: 0.5529, OBJ: 0.3844, TOTAL: 0.5192\n",
      "CE: 0.3133, OBJ: 0.3451, TOTAL: 0.3197\n",
      "CE: 0.3389, OBJ: 0.2805, TOTAL: 0.3272\n",
      "CE: 0.2933, OBJ: 0.3571, TOTAL: 0.3061\n",
      "CE: 0.4414, OBJ: 0.3000, TOTAL: 0.4131\n",
      "CE: 0.4725, OBJ: 0.3376, TOTAL: 0.4455\n",
      "CE: 0.1924, OBJ: 0.3147, TOTAL: 0.2169\n",
      "CE: 0.4572, OBJ: 0.3351, TOTAL: 0.4327\n",
      "CE: 0.5468, OBJ: 0.4175, TOTAL: 0.5210\n",
      "CE: 0.2017, OBJ: 0.2378, TOTAL: 0.2089\n",
      "CE: 1.0171, OBJ: 0.3067, TOTAL: 0.8750\n",
      "CE: 0.3750, OBJ: 0.2871, TOTAL: 0.3574\n",
      "CE: 0.2586, OBJ: 0.2488, TOTAL: 0.2567\n",
      "CE: 0.2530, OBJ: 0.2315, TOTAL: 0.2487\n",
      "CE: 0.2887, OBJ: 0.2326, TOTAL: 0.2775\n",
      "CE: 0.6621, OBJ: 0.3153, TOTAL: 0.5927\n",
      "CE: 0.0836, OBJ: 0.2770, TOTAL: 0.1223\n",
      "CE: 0.3182, OBJ: 0.3116, TOTAL: 0.3169\n",
      "CE: 0.1436, OBJ: 0.3203, TOTAL: 0.1790\n",
      "CE: 0.1359, OBJ: 0.3891, TOTAL: 0.1866\n",
      "CE: 0.1035, OBJ: 0.2560, TOTAL: 0.1340\n",
      "CE: 0.3941, OBJ: 0.3334, TOTAL: 0.3819\n",
      "CE: 0.4196, OBJ: 0.3654, TOTAL: 0.4087\n",
      "CE: 0.1210, OBJ: 0.3210, TOTAL: 0.1610\n",
      "CE: 0.3762, OBJ: 0.3891, TOTAL: 0.3788\n",
      "CE: 0.1476, OBJ: 0.3480, TOTAL: 0.1877\n",
      "CE: 0.4612, OBJ: 0.3286, TOTAL: 0.4347\n",
      "CE: 0.4569, OBJ: 0.2384, TOTAL: 0.4132\n",
      "CE: 0.2412, OBJ: 0.3497, TOTAL: 0.2629\n",
      "CE: 0.2026, OBJ: 0.3272, TOTAL: 0.2275\n",
      "CE: 0.2889, OBJ: 0.3332, TOTAL: 0.2978\n",
      "CE: 0.2740, OBJ: 0.3994, TOTAL: 0.2991\n",
      "CE: 0.4606, OBJ: 0.3758, TOTAL: 0.4436\n",
      "CE: 0.1559, OBJ: 0.1553, TOTAL: 0.1558\n",
      "CE: 0.2123, OBJ: 0.1617, TOTAL: 0.2022\n",
      "CE: 0.1703, OBJ: 0.3610, TOTAL: 0.2084\n",
      "CE: 0.1493, OBJ: 0.3096, TOTAL: 0.1814\n",
      "CE: 0.6142, OBJ: 0.3863, TOTAL: 0.5686\n",
      "CE: 0.1426, OBJ: 0.4087, TOTAL: 0.1958\n",
      "CE: 0.2733, OBJ: 0.3387, TOTAL: 0.2864\n",
      "CE: 0.2921, OBJ: 0.4078, TOTAL: 0.3152\n",
      "CE: 0.1968, OBJ: 0.2682, TOTAL: 0.2111\n",
      "CE: 0.3559, OBJ: 0.2513, TOTAL: 0.3350\n",
      "CE: 0.4732, OBJ: 0.1911, TOTAL: 0.4168\n",
      "CE: 0.1623, OBJ: 0.3507, TOTAL: 0.2000\n",
      "CE: 0.6701, OBJ: 0.3964, TOTAL: 0.6154\n",
      "CE: 0.0959, OBJ: 0.2561, TOTAL: 0.1279\n",
      "CE: 0.4121, OBJ: 0.3444, TOTAL: 0.3986\n",
      "CE: 0.1626, OBJ: 0.3601, TOTAL: 0.2021\n",
      "CE: 0.2575, OBJ: 0.3440, TOTAL: 0.2748\n",
      "CE: 0.2841, OBJ: 0.2832, TOTAL: 0.2839\n",
      "CE: 0.2531, OBJ: 0.3247, TOTAL: 0.2674\n",
      "CE: 0.4354, OBJ: 0.3800, TOTAL: 0.4243\n",
      "CE: 0.4874, OBJ: 0.3970, TOTAL: 0.4693\n",
      "CE: 0.2336, OBJ: 0.2349, TOTAL: 0.2339\n",
      "CE: 0.1232, OBJ: 0.3950, TOTAL: 0.1775\n",
      "CE: 0.2141, OBJ: 0.3317, TOTAL: 0.2376\n",
      "CE: 0.3520, OBJ: 0.3599, TOTAL: 0.3536\n",
      "CE: 0.2240, OBJ: 0.2674, TOTAL: 0.2327\n",
      "CE: 0.2175, OBJ: 0.5638, TOTAL: 0.2867\n",
      "CE: 0.4443, OBJ: 0.3133, TOTAL: 0.4181\n",
      "CE: 0.4034, OBJ: 0.4226, TOTAL: 0.4072\n",
      "CE: 0.3649, OBJ: 0.2859, TOTAL: 0.3491\n",
      "CE: 0.2401, OBJ: 0.2942, TOTAL: 0.2509\n",
      "CE: 0.1411, OBJ: 0.3061, TOTAL: 0.1741\n",
      "CE: 0.3287, OBJ: 0.4631, TOTAL: 0.3556\n",
      "CE: 0.2496, OBJ: 0.4149, TOTAL: 0.2827\n",
      "CE: 0.1362, OBJ: 0.4092, TOTAL: 0.1908\n",
      "CE: 0.3116, OBJ: 0.2360, TOTAL: 0.2965\n",
      "CE: 0.4431, OBJ: 0.3338, TOTAL: 0.4212\n",
      "CE: 0.1083, OBJ: 0.2422, TOTAL: 0.1351\n",
      "CE: 0.3777, OBJ: 0.3719, TOTAL: 0.3765\n",
      "CE: 0.2499, OBJ: 0.4520, TOTAL: 0.2904\n",
      "CE: 0.3179, OBJ: 0.4340, TOTAL: 0.3411\n",
      "CE: 0.4842, OBJ: 0.3670, TOTAL: 0.4608\n",
      "CE: 0.2957, OBJ: 0.2647, TOTAL: 0.2895\n",
      "CE: 0.1556, OBJ: 0.3014, TOTAL: 0.1848\n",
      "CE: 0.1757, OBJ: 0.3119, TOTAL: 0.2029\n",
      "CE: 0.3069, OBJ: 0.2661, TOTAL: 0.2987\n",
      "CE: 0.4821, OBJ: 0.4550, TOTAL: 0.4767\n",
      "CE: 0.4018, OBJ: 0.2958, TOTAL: 0.3806\n",
      "CE: 0.5716, OBJ: 0.2619, TOTAL: 0.5097\n",
      "CE: 0.0840, OBJ: 0.1570, TOTAL: 0.0986\n",
      "CE: 0.5245, OBJ: 0.2842, TOTAL: 0.4764\n",
      "CE: 0.6653, OBJ: 0.3127, TOTAL: 0.5948\n",
      "CE: 0.1091, OBJ: 0.3123, TOTAL: 0.1497\n",
      "CE: 0.4071, OBJ: 0.2198, TOTAL: 0.3697\n",
      "CE: 0.2368, OBJ: 0.3825, TOTAL: 0.2659\n",
      "CE: 0.5085, OBJ: 0.3916, TOTAL: 0.4851\n",
      "CE: 0.0941, OBJ: 0.3114, TOTAL: 0.1375\n",
      "CE: 0.2184, OBJ: 0.3054, TOTAL: 0.2358\n",
      "CE: 0.3814, OBJ: 0.3703, TOTAL: 0.3792\n",
      "CE: 0.0999, OBJ: 0.3719, TOTAL: 0.1543\n",
      "CE: 0.1497, OBJ: 0.3855, TOTAL: 0.1968\n",
      "CE: 0.1519, OBJ: 0.3354, TOTAL: 0.1886\n",
      "CE: 0.2508, OBJ: 0.2685, TOTAL: 0.2543\n",
      "CE: 0.1621, OBJ: 0.2052, TOTAL: 0.1707\n",
      "CE: 0.6450, OBJ: 0.4570, TOTAL: 0.6074\n",
      "CE: 0.6169, OBJ: 0.3008, TOTAL: 0.5537\n",
      "CE: 0.0799, OBJ: 0.2921, TOTAL: 0.1224\n",
      "CE: 0.1509, OBJ: 0.2747, TOTAL: 0.1756\n",
      "CE: 0.4527, OBJ: 0.3587, TOTAL: 0.4339\n",
      "CE: 0.6021, OBJ: 0.2262, TOTAL: 0.5269\n",
      "CE: 0.5487, OBJ: 0.3597, TOTAL: 0.5109\n",
      "CE: 0.2915, OBJ: 0.3885, TOTAL: 0.3109\n",
      "CE: 0.3632, OBJ: 0.2228, TOTAL: 0.3352\n",
      "CE: 0.2169, OBJ: 0.1938, TOTAL: 0.2123\n",
      "CE: 0.2511, OBJ: 0.2217, TOTAL: 0.2453\n",
      "CE: 0.2728, OBJ: 0.4091, TOTAL: 0.3000\n",
      "CE: 0.4051, OBJ: 0.4389, TOTAL: 0.4119\n",
      "CE: 0.1882, OBJ: 0.4212, TOTAL: 0.2348\n",
      "CE: 0.6085, OBJ: 0.3814, TOTAL: 0.5631\n",
      "CE: 0.2450, OBJ: 0.3475, TOTAL: 0.2655\n",
      "CE: 0.1041, OBJ: 0.2937, TOTAL: 0.1421\n",
      "CE: 0.2813, OBJ: 0.3242, TOTAL: 0.2899\n",
      "CE: 0.5884, OBJ: 0.3031, TOTAL: 0.5313\n",
      "CE: 0.4355, OBJ: 0.3513, TOTAL: 0.4186\n",
      "CE: 0.4136, OBJ: 0.3414, TOTAL: 0.3992\n",
      "CE: 0.1729, OBJ: 0.2210, TOTAL: 0.1825\n",
      "CE: 0.1706, OBJ: 0.3510, TOTAL: 0.2067\n",
      "CE: 0.2724, OBJ: 0.2747, TOTAL: 0.2728\n",
      "CE: 0.6381, OBJ: 0.3280, TOTAL: 0.5761\n",
      "CE: 0.1706, OBJ: 0.4716, TOTAL: 0.2308\n",
      "CE: 0.3043, OBJ: 0.3983, TOTAL: 0.3231\n",
      "CE: 0.3305, OBJ: 0.4616, TOTAL: 0.3567\n",
      "CE: 0.3753, OBJ: 0.3520, TOTAL: 0.3706\n",
      "CE: 0.2564, OBJ: 0.3688, TOTAL: 0.2789\n",
      "CE: 0.1826, OBJ: 0.2486, TOTAL: 0.1958\n",
      "CE: 0.0865, OBJ: 0.4050, TOTAL: 0.1502\n",
      "CE: 0.0975, OBJ: 0.3584, TOTAL: 0.1497\n",
      "CE: 0.3080, OBJ: 0.3535, TOTAL: 0.3171\n",
      "CE: 0.1360, OBJ: 0.3660, TOTAL: 0.1820\n",
      "CE: 0.1956, OBJ: 0.2913, TOTAL: 0.2147\n",
      "CE: 0.2442, OBJ: 0.2637, TOTAL: 0.2481\n",
      "CE: 0.2988, OBJ: 0.2581, TOTAL: 0.2907\n",
      "CE: 0.1966, OBJ: 0.3941, TOTAL: 0.2361\n",
      "CE: 0.1830, OBJ: 0.3656, TOTAL: 0.2195\n",
      "CE: 0.4195, OBJ: 0.4358, TOTAL: 0.4227\n",
      "CE: 0.1956, OBJ: 0.2103, TOTAL: 0.1985\n",
      "CE: 0.1071, OBJ: 0.3969, TOTAL: 0.1650\n",
      "CE: 0.3845, OBJ: 0.3688, TOTAL: 0.3814\n",
      "CE: 0.5774, OBJ: 0.2478, TOTAL: 0.5115\n",
      "CE: 0.3680, OBJ: 0.2592, TOTAL: 0.3462\n",
      "CE: 0.4475, OBJ: 0.2852, TOTAL: 0.4151\n",
      "CE: 0.6580, OBJ: 0.3735, TOTAL: 0.6011\n",
      "CE: 0.6791, OBJ: 0.3883, TOTAL: 0.6209\n",
      "CE: 0.1794, OBJ: 0.3031, TOTAL: 0.2042\n",
      "CE: 0.2993, OBJ: 0.2657, TOTAL: 0.2926\n",
      "CE: 0.2470, OBJ: 0.2859, TOTAL: 0.2548\n",
      "CE: 0.3179, OBJ: 0.3967, TOTAL: 0.3336\n",
      "CE: 0.2891, OBJ: 0.3211, TOTAL: 0.2955\n",
      "CE: 0.1164, OBJ: 0.2549, TOTAL: 0.1441\n",
      "CE: 0.5642, OBJ: 0.3906, TOTAL: 0.5295\n",
      "CE: 0.3176, OBJ: 0.3790, TOTAL: 0.3299\n",
      "CE: 0.5906, OBJ: 0.2558, TOTAL: 0.5237\n",
      "CE: 0.4684, OBJ: 0.3572, TOTAL: 0.4462\n",
      "CE: 0.4666, OBJ: 0.3282, TOTAL: 0.4389\n",
      "CE: 0.3149, OBJ: 0.4031, TOTAL: 0.3326\n",
      "CE: 0.4229, OBJ: 0.4152, TOTAL: 0.4214\n",
      "CE: 0.1288, OBJ: 0.2941, TOTAL: 0.1618\n",
      "CE: 0.8813, OBJ: 0.3637, TOTAL: 0.7778\n",
      "CE: 0.4403, OBJ: 0.2387, TOTAL: 0.4000\n",
      "CE: 0.5699, OBJ: 0.2915, TOTAL: 0.5142\n",
      "CE: 0.4019, OBJ: 0.3002, TOTAL: 0.3816\n",
      "CE: 0.4486, OBJ: 0.3125, TOTAL: 0.4214\n",
      "CE: 0.2840, OBJ: 0.5336, TOTAL: 0.3339\n",
      "CE: 0.3967, OBJ: 0.2472, TOTAL: 0.3668\n",
      "CE: 0.4099, OBJ: 0.3575, TOTAL: 0.3995\n",
      "CE: 0.2790, OBJ: 0.2530, TOTAL: 0.2738\n",
      "CE: 0.3512, OBJ: 0.3514, TOTAL: 0.3512\n",
      "CE: 0.2802, OBJ: 0.2760, TOTAL: 0.2794\n",
      "CE: 0.3166, OBJ: 0.3309, TOTAL: 0.3195\n",
      "CE: 0.4661, OBJ: 0.4131, TOTAL: 0.4555\n",
      "CE: 0.4033, OBJ: 0.3053, TOTAL: 0.3837\n",
      "CE: 0.8516, OBJ: 0.3512, TOTAL: 0.7515\n",
      "CE: 0.4035, OBJ: 0.3324, TOTAL: 0.3893\n",
      "CE: 0.2085, OBJ: 0.4200, TOTAL: 0.2508\n",
      "CE: 0.6344, OBJ: 0.3360, TOTAL: 0.5747\n",
      "CE: 0.0966, OBJ: 0.2354, TOTAL: 0.1244\n",
      "CE: 0.5551, OBJ: 0.3165, TOTAL: 0.5074\n",
      "CE: 0.5839, OBJ: 0.2192, TOTAL: 0.5110\n",
      "CE: 0.4041, OBJ: 0.3382, TOTAL: 0.3909\n",
      "CE: 0.3341, OBJ: 0.3480, TOTAL: 0.3369\n",
      "CE: 0.4003, OBJ: 0.3675, TOTAL: 0.3937\n",
      "CE: 0.2757, OBJ: 0.3847, TOTAL: 0.2975\n",
      "CE: 0.1030, OBJ: 0.3712, TOTAL: 0.1566\n",
      "CE: 0.3066, OBJ: 0.3488, TOTAL: 0.3150\n",
      "CE: 0.2227, OBJ: 0.4356, TOTAL: 0.2653\n",
      "CE: 0.4846, OBJ: 0.3730, TOTAL: 0.4623\n",
      "CE: 0.2534, OBJ: 0.2322, TOTAL: 0.2492\n",
      "CE: 0.4362, OBJ: 0.4275, TOTAL: 0.4344\n",
      "CE: 0.3234, OBJ: 0.2645, TOTAL: 0.3116\n",
      "CE: 0.1945, OBJ: 0.3187, TOTAL: 0.2193\n",
      "CE: 0.4530, OBJ: 0.1701, TOTAL: 0.3964\n",
      "CE: 0.4764, OBJ: 0.4592, TOTAL: 0.4729\n",
      "CE: 0.2681, OBJ: 0.3667, TOTAL: 0.2878\n",
      "CE: 0.2987, OBJ: 0.2949, TOTAL: 0.2979\n",
      "CE: 0.3773, OBJ: 0.3655, TOTAL: 0.3750\n",
      "CE: 0.3731, OBJ: 0.2970, TOTAL: 0.3579\n",
      "CE: 0.6519, OBJ: 0.3772, TOTAL: 0.5970\n",
      "CE: 0.4060, OBJ: 0.2759, TOTAL: 0.3800\n",
      "CE: 0.1164, OBJ: 0.5186, TOTAL: 0.1969\n",
      "CE: 0.2476, OBJ: 0.1270, TOTAL: 0.2235\n",
      "CE: 0.5160, OBJ: 0.4087, TOTAL: 0.4946\n",
      "CE: 0.3565, OBJ: 0.4765, TOTAL: 0.3805\n",
      "CE: 0.4877, OBJ: 0.3798, TOTAL: 0.4661\n",
      "CE: 0.5577, OBJ: 0.2007, TOTAL: 0.4863\n",
      "CE: 0.2773, OBJ: 0.4073, TOTAL: 0.3033\n",
      "CE: 0.5911, OBJ: 0.3583, TOTAL: 0.5446\n",
      "CE: 0.1705, OBJ: 0.3638, TOTAL: 0.2091\n",
      "CE: 0.7340, OBJ: 0.4346, TOTAL: 0.6741\n",
      "CE: 0.1068, OBJ: 0.2667, TOTAL: 0.1388\n",
      "CE: 0.2608, OBJ: 0.2939, TOTAL: 0.2674\n",
      "CE: 0.0748, OBJ: 0.1928, TOTAL: 0.0984\n",
      "CE: 0.2946, OBJ: 0.4238, TOTAL: 0.3204\n",
      "CE: 0.2423, OBJ: 0.3996, TOTAL: 0.2738\n",
      "CE: 0.3021, OBJ: 0.4431, TOTAL: 0.3303\n",
      "CE: 0.1926, OBJ: 0.3986, TOTAL: 0.2338\n",
      "CE: 0.4582, OBJ: 0.2302, TOTAL: 0.4126\n",
      "CE: 0.1636, OBJ: 0.4324, TOTAL: 0.2174\n",
      "CE: 0.1363, OBJ: 0.3460, TOTAL: 0.1782\n",
      "CE: 0.5405, OBJ: 0.2817, TOTAL: 0.4887\n",
      "CE: 0.2059, OBJ: 0.3166, TOTAL: 0.2281\n",
      "CE: 0.2067, OBJ: 0.2461, TOTAL: 0.2145\n",
      "CE: 0.3773, OBJ: 0.2846, TOTAL: 0.3588\n",
      "CE: 0.2752, OBJ: 0.2503, TOTAL: 0.2702\n",
      "CE: 0.1904, OBJ: 0.2651, TOTAL: 0.2054\n",
      "CE: 0.3093, OBJ: 0.4651, TOTAL: 0.3405\n",
      "CE: 0.1767, OBJ: 0.3137, TOTAL: 0.2041\n",
      "CE: 0.3903, OBJ: 0.2636, TOTAL: 0.3650\n",
      "CE: 0.0837, OBJ: 0.3196, TOTAL: 0.1308\n",
      "CE: 0.1059, OBJ: 0.5189, TOTAL: 0.1885\n",
      "CE: 0.1215, OBJ: 0.2442, TOTAL: 0.1461\n",
      "CE: 0.3800, OBJ: 0.3144, TOTAL: 0.3669\n",
      "CE: 0.1358, OBJ: 0.5484, TOTAL: 0.2183\n",
      "CE: 0.2891, OBJ: 0.3364, TOTAL: 0.2985\n",
      "CE: 0.1247, OBJ: 0.2000, TOTAL: 0.1398\n",
      "CE: 0.1987, OBJ: 0.3411, TOTAL: 0.2272\n",
      "CE: 0.5984, OBJ: 0.3133, TOTAL: 0.5413\n",
      "CE: 0.3874, OBJ: 0.3299, TOTAL: 0.3759\n",
      "CE: 0.5413, OBJ: 0.4441, TOTAL: 0.5218\n",
      "CE: 0.5237, OBJ: 0.3006, TOTAL: 0.4791\n",
      "CE: 0.4687, OBJ: 0.3494, TOTAL: 0.4449\n",
      "CE: 0.1844, OBJ: 0.2563, TOTAL: 0.1988\n",
      "CE: 0.3818, OBJ: 0.4817, TOTAL: 0.4018\n",
      "CE: 0.2261, OBJ: 0.3844, TOTAL: 0.2578\n",
      "CE: 0.1559, OBJ: 0.2820, TOTAL: 0.1811\n",
      "CE: 0.7359, OBJ: 0.3527, TOTAL: 0.6592\n",
      "CE: 0.3524, OBJ: 0.2755, TOTAL: 0.3370\n",
      "CE: 0.1652, OBJ: 0.2452, TOTAL: 0.1812\n",
      "CE: 0.2768, OBJ: 0.2820, TOTAL: 0.2778\n",
      "CE: 0.3195, OBJ: 0.3398, TOTAL: 0.3236\n",
      "CE: 0.4190, OBJ: 0.2539, TOTAL: 0.3860\n",
      "CE: 0.3338, OBJ: 0.3002, TOTAL: 0.3270\n",
      "CE: 0.1218, OBJ: 0.4136, TOTAL: 0.1802\n",
      "CE: 0.3727, OBJ: 0.3182, TOTAL: 0.3618\n",
      "CE: 0.4108, OBJ: 0.3337, TOTAL: 0.3954\n",
      "CE: 0.1240, OBJ: 0.2824, TOTAL: 0.1557\n",
      "CE: 0.1794, OBJ: 0.2585, TOTAL: 0.1952\n",
      "CE: 0.2520, OBJ: 0.4210, TOTAL: 0.2858\n",
      "CE: 0.1561, OBJ: 0.3169, TOTAL: 0.1882\n",
      "CE: 0.4490, OBJ: 0.1567, TOTAL: 0.3906\n",
      "CE: 0.1116, OBJ: 0.2395, TOTAL: 0.1372\n",
      "CE: 0.1576, OBJ: 0.2266, TOTAL: 0.1714\n",
      "CE: 0.3322, OBJ: 0.3434, TOTAL: 0.3345\n",
      "CE: 0.0651, OBJ: 0.1840, TOTAL: 0.0889\n",
      "CE: 0.5135, OBJ: 0.3059, TOTAL: 0.4720\n",
      "CE: 0.0736, OBJ: 0.2594, TOTAL: 0.1108\n",
      "CE: 0.4087, OBJ: 0.3450, TOTAL: 0.3959\n",
      "CE: 0.2571, OBJ: 0.3883, TOTAL: 0.2833\n",
      "CE: 0.1773, OBJ: 0.2219, TOTAL: 0.1862\n",
      "CE: 0.2970, OBJ: 0.4662, TOTAL: 0.3309\n",
      "CE: 0.6532, OBJ: 0.2432, TOTAL: 0.5712\n",
      "CE: 0.1333, OBJ: 0.2209, TOTAL: 0.1509\n",
      "CE: 0.2438, OBJ: 0.4876, TOTAL: 0.2926\n",
      "CE: 0.4991, OBJ: 0.4030, TOTAL: 0.4798\n",
      "CE: 0.3032, OBJ: 0.3709, TOTAL: 0.3167\n",
      "CE: 0.2270, OBJ: 0.5683, TOTAL: 0.2953\n",
      "CE: 0.3142, OBJ: 0.4363, TOTAL: 0.3386\n",
      "CE: 0.3084, OBJ: 0.2771, TOTAL: 0.3021\n",
      "CE: 0.3164, OBJ: 0.2238, TOTAL: 0.2979\n",
      "CE: 0.4464, OBJ: 0.4632, TOTAL: 0.4498\n",
      "CE: 0.1394, OBJ: 0.3242, TOTAL: 0.1764\n",
      "CE: 0.2123, OBJ: 0.2855, TOTAL: 0.2269\n",
      "CE: 0.3316, OBJ: 0.3840, TOTAL: 0.3421\n",
      "CE: 0.0919, OBJ: 0.3163, TOTAL: 0.1368\n",
      "CE: 0.4687, OBJ: 0.2910, TOTAL: 0.4332\n",
      "CE: 0.3927, OBJ: 0.3154, TOTAL: 0.3772\n",
      "CE: 0.4704, OBJ: 0.2477, TOTAL: 0.4259\n",
      "CE: 0.1842, OBJ: 0.2388, TOTAL: 0.1951\n",
      "CE: 0.3227, OBJ: 0.3708, TOTAL: 0.3324\n",
      "CE: 0.2483, OBJ: 0.2892, TOTAL: 0.2565\n",
      "CE: 0.0831, OBJ: 0.2358, TOTAL: 0.1137\n",
      "CE: 0.3526, OBJ: 0.3299, TOTAL: 0.3480\n",
      "CE: 0.6455, OBJ: 0.2792, TOTAL: 0.5722\n",
      "CE: 0.4548, OBJ: 0.2953, TOTAL: 0.4229\n",
      "CE: 0.3915, OBJ: 0.2997, TOTAL: 0.3732\n",
      "CE: 0.6371, OBJ: 0.2829, TOTAL: 0.5662\n",
      "CE: 0.4150, OBJ: 0.1586, TOTAL: 0.3637\n",
      "CE: 0.3113, OBJ: 0.2860, TOTAL: 0.3063\n",
      "CE: 0.4855, OBJ: 0.3232, TOTAL: 0.4530\n",
      "CE: 0.2448, OBJ: 0.3988, TOTAL: 0.2756\n",
      "CE: 0.3134, OBJ: 0.3669, TOTAL: 0.3241\n",
      "CE: 0.1242, OBJ: 0.2807, TOTAL: 0.1555\n",
      "CE: 0.3104, OBJ: 0.3776, TOTAL: 0.3238\n",
      "CE: 0.3107, OBJ: 0.4977, TOTAL: 0.3481\n",
      "CE: 0.2353, OBJ: 0.1722, TOTAL: 0.2226\n",
      "CE: 0.1890, OBJ: 0.3653, TOTAL: 0.2242\n",
      "CE: 0.0972, OBJ: 0.2517, TOTAL: 0.1281\n",
      "CE: 0.5194, OBJ: 0.3706, TOTAL: 0.4896\n",
      "CE: 0.2357, OBJ: 0.3090, TOTAL: 0.2503\n",
      "CE: 0.5618, OBJ: 0.3690, TOTAL: 0.5233\n",
      "CE: 0.2813, OBJ: 0.4224, TOTAL: 0.3096\n",
      "CE: 0.1750, OBJ: 0.5563, TOTAL: 0.2513\n",
      "CE: 0.5695, OBJ: 0.3080, TOTAL: 0.5172\n",
      "CE: 0.6771, OBJ: 0.5917, TOTAL: 0.6600\n",
      "CE: 0.1734, OBJ: 0.2402, TOTAL: 0.1867\n",
      "CE: 0.4242, OBJ: 0.4642, TOTAL: 0.4322\n",
      "CE: 0.2713, OBJ: 0.2733, TOTAL: 0.2717\n",
      "CE: 0.1671, OBJ: 0.3954, TOTAL: 0.2128\n",
      "CE: 0.2720, OBJ: 0.2850, TOTAL: 0.2746\n",
      "CE: 0.4470, OBJ: 0.2766, TOTAL: 0.4130\n",
      "CE: 0.3644, OBJ: 0.2481, TOTAL: 0.3411\n",
      "CE: 0.5569, OBJ: 0.2635, TOTAL: 0.4982\n",
      "CE: 0.2180, OBJ: 0.3243, TOTAL: 0.2392\n",
      "CE: 0.5484, OBJ: 0.2114, TOTAL: 0.4810\n",
      "CE: 0.5795, OBJ: 0.2518, TOTAL: 0.5140\n",
      "CE: 0.3574, OBJ: 0.4248, TOTAL: 0.3709\n",
      "CE: 0.4016, OBJ: 0.3587, TOTAL: 0.3930\n",
      "CE: 0.2130, OBJ: 0.2869, TOTAL: 0.2278\n",
      "CE: 0.0969, OBJ: 0.2409, TOTAL: 0.1257\n",
      "CE: 0.3726, OBJ: 0.2114, TOTAL: 0.3403\n",
      "CE: 0.5725, OBJ: 0.2954, TOTAL: 0.5171\n",
      "CE: 0.4253, OBJ: 0.3277, TOTAL: 0.4058\n",
      "CE: 0.6985, OBJ: 0.4488, TOTAL: 0.6486\n",
      "CE: 0.4742, OBJ: 0.3072, TOTAL: 0.4408\n",
      "CE: 0.1586, OBJ: 0.3691, TOTAL: 0.2007\n",
      "CE: 0.4574, OBJ: 0.4134, TOTAL: 0.4486\n",
      "CE: 0.4613, OBJ: 0.3659, TOTAL: 0.4422\n",
      "CE: 0.2746, OBJ: 0.2636, TOTAL: 0.2724\n",
      "CE: 0.5262, OBJ: 0.3818, TOTAL: 0.4974\n",
      "CE: 0.5987, OBJ: 0.3875, TOTAL: 0.5564\n",
      "CE: 0.4481, OBJ: 0.3083, TOTAL: 0.4201\n",
      "CE: 0.2526, OBJ: 0.3813, TOTAL: 0.2783\n",
      "CE: 0.7282, OBJ: 0.4363, TOTAL: 0.6698\n",
      "CE: 0.1571, OBJ: 0.2447, TOTAL: 0.1746\n",
      "CE: 0.1030, OBJ: 0.2598, TOTAL: 0.1343\n",
      "CE: 0.4717, OBJ: 0.3765, TOTAL: 0.4526\n",
      "CE: 0.3040, OBJ: 0.2246, TOTAL: 0.2881\n",
      "CE: 0.1942, OBJ: 0.1870, TOTAL: 0.1927\n",
      "CE: 0.3565, OBJ: 0.3536, TOTAL: 0.3560\n",
      "CE: 0.1858, OBJ: 0.3281, TOTAL: 0.2142\n",
      "CE: 0.1281, OBJ: 0.2462, TOTAL: 0.1517\n",
      "CE: 0.2208, OBJ: 0.2270, TOTAL: 0.2220\n",
      "CE: 0.2185, OBJ: 0.2830, TOTAL: 0.2314\n",
      "CE: 0.2495, OBJ: 0.4115, TOTAL: 0.2819\n",
      "CE: 0.2504, OBJ: 0.2220, TOTAL: 0.2447\n",
      "CE: 0.2211, OBJ: 0.3540, TOTAL: 0.2477\n",
      "CE: 0.4999, OBJ: 0.3168, TOTAL: 0.4633\n",
      "CE: 0.1699, OBJ: 0.3337, TOTAL: 0.2027\n",
      "CE: 0.4809, OBJ: 0.4112, TOTAL: 0.4670\n",
      "CE: 0.3285, OBJ: 0.2102, TOTAL: 0.3048\n",
      "CE: 0.3959, OBJ: 0.2887, TOTAL: 0.3744\n",
      "CE: 0.2701, OBJ: 0.2052, TOTAL: 0.2571\n",
      "CE: 0.1733, OBJ: 0.2608, TOTAL: 0.1908\n",
      "CE: 0.2152, OBJ: 0.4548, TOTAL: 0.2631\n",
      "CE: 0.6557, OBJ: 0.4500, TOTAL: 0.6145\n",
      "CE: 0.2110, OBJ: 0.3162, TOTAL: 0.2320\n",
      "CE: 0.2321, OBJ: 0.1888, TOTAL: 0.2234\n",
      "CE: 0.4087, OBJ: 0.2636, TOTAL: 0.3797\n",
      "CE: 0.1577, OBJ: 0.2219, TOTAL: 0.1706\n",
      "CE: 0.1004, OBJ: 0.2706, TOTAL: 0.1345\n",
      "CE: 0.4207, OBJ: 0.3178, TOTAL: 0.4001\n",
      "CE: 0.4182, OBJ: 0.3667, TOTAL: 0.4079\n",
      "CE: 0.1584, OBJ: 0.3982, TOTAL: 0.2063\n",
      "CE: 0.2617, OBJ: 0.3043, TOTAL: 0.2702\n",
      "CE: 0.1344, OBJ: 0.1738, TOTAL: 0.1423\n",
      "CE: 0.4086, OBJ: 0.2580, TOTAL: 0.3785\n",
      "CE: 0.2520, OBJ: 0.4494, TOTAL: 0.2915\n",
      "CE: 0.2873, OBJ: 0.3477, TOTAL: 0.2994\n",
      "CE: 0.4102, OBJ: 0.3326, TOTAL: 0.3947\n",
      "CE: 0.2405, OBJ: 0.3684, TOTAL: 0.2660\n",
      "CE: 0.3850, OBJ: 0.3046, TOTAL: 0.3689\n",
      "CE: 0.2759, OBJ: 0.4006, TOTAL: 0.3009\n",
      "CE: 0.2274, OBJ: 0.3436, TOTAL: 0.2506\n",
      "CE: 0.3747, OBJ: 0.2177, TOTAL: 0.3433\n",
      "CE: 0.2525, OBJ: 0.3334, TOTAL: 0.2687\n",
      "CE: 0.4377, OBJ: 0.2516, TOTAL: 0.4005\n",
      "CE: 0.3683, OBJ: 0.2053, TOTAL: 0.3357\n",
      "CE: 0.3042, OBJ: 0.2604, TOTAL: 0.2955\n",
      "CE: 0.3537, OBJ: 0.2540, TOTAL: 0.3338\n",
      "CE: 0.3977, OBJ: 0.2694, TOTAL: 0.3720\n",
      "CE: 0.2377, OBJ: 0.2673, TOTAL: 0.2436\n",
      "CE: 0.3504, OBJ: 0.4952, TOTAL: 0.3793\n",
      "CE: 0.1366, OBJ: 0.1706, TOTAL: 0.1434\n",
      "CE: 0.5401, OBJ: 0.3576, TOTAL: 0.5036\n",
      "CE: 0.2871, OBJ: 0.1612, TOTAL: 0.2619\n",
      "CE: 0.3348, OBJ: 0.3233, TOTAL: 0.3325\n",
      "CE: 0.2492, OBJ: 0.1904, TOTAL: 0.2374\n",
      "CE: 0.4245, OBJ: 0.3270, TOTAL: 0.4050\n",
      "CE: 0.2809, OBJ: 0.2943, TOTAL: 0.2836\n",
      "CE: 0.2629, OBJ: 0.3606, TOTAL: 0.2824\n",
      "CE: 0.7563, OBJ: 0.3351, TOTAL: 0.6721\n",
      "CE: 0.4405, OBJ: 0.3057, TOTAL: 0.4135\n",
      "CE: 0.3540, OBJ: 0.2743, TOTAL: 0.3381\n",
      "CE: 0.3745, OBJ: 0.2458, TOTAL: 0.3488\n",
      "CE: 0.2945, OBJ: 0.3473, TOTAL: 0.3050\n",
      "CE: 0.3103, OBJ: 0.4272, TOTAL: 0.3337\n",
      "CE: 0.1815, OBJ: 0.2864, TOTAL: 0.2025\n",
      "CE: 0.1254, OBJ: 0.3748, TOTAL: 0.1753\n",
      "CE: 0.2079, OBJ: 0.4419, TOTAL: 0.2547\n",
      "CE: 0.2508, OBJ: 0.2296, TOTAL: 0.2465\n",
      "CE: 0.3588, OBJ: 0.3181, TOTAL: 0.3507\n",
      "CE: 0.4008, OBJ: 0.3525, TOTAL: 0.3911\n",
      "CE: 0.4112, OBJ: 0.4057, TOTAL: 0.4101\n",
      "CE: 0.4287, OBJ: 0.3648, TOTAL: 0.4160\n",
      "CE: 0.2226, OBJ: 0.2809, TOTAL: 0.2343\n",
      "CE: 0.2994, OBJ: 0.2686, TOTAL: 0.2933\n",
      "CE: 0.2785, OBJ: 0.3231, TOTAL: 0.2874\n",
      "CE: 0.2643, OBJ: 0.2941, TOTAL: 0.2703\n",
      "CE: 0.3129, OBJ: 0.2517, TOTAL: 0.3006\n",
      "CE: 0.3366, OBJ: 0.3904, TOTAL: 0.3474\n",
      "CE: 0.4290, OBJ: 0.3079, TOTAL: 0.4047\n",
      "CE: 0.5844, OBJ: 0.3294, TOTAL: 0.5334\n",
      "CE: 0.3875, OBJ: 0.4425, TOTAL: 0.3985\n",
      "CE: 0.2375, OBJ: 0.3208, TOTAL: 0.2541\n",
      "CE: 0.0881, OBJ: 0.1246, TOTAL: 0.0954\n",
      "CE: 0.4851, OBJ: 0.3184, TOTAL: 0.4517\n",
      "CE: 0.1485, OBJ: 0.3225, TOTAL: 0.1833\n",
      "CE: 0.2491, OBJ: 0.2186, TOTAL: 0.2430\n",
      "CE: 0.5066, OBJ: 0.4229, TOTAL: 0.4898\n",
      "CE: 0.3906, OBJ: 0.3715, TOTAL: 0.3868\n",
      "CE: 0.4647, OBJ: 0.3702, TOTAL: 0.4458\n",
      "CE: 0.3104, OBJ: 0.3255, TOTAL: 0.3134\n",
      "CE: 0.3470, OBJ: 0.3153, TOTAL: 0.3407\n",
      "CE: 0.2073, OBJ: 0.1920, TOTAL: 0.2043\n",
      "CE: 0.1005, OBJ: 0.3675, TOTAL: 0.1539\n",
      "CE: 0.1886, OBJ: 0.3423, TOTAL: 0.2193\n",
      "CE: 0.3241, OBJ: 0.3118, TOTAL: 0.3216\n",
      "CE: 0.3924, OBJ: 0.3456, TOTAL: 0.3831\n",
      "CE: 0.1992, OBJ: 0.2405, TOTAL: 0.2075\n",
      "CE: 0.3447, OBJ: 0.6145, TOTAL: 0.3986\n",
      "CE: 0.4613, OBJ: 0.2311, TOTAL: 0.4153\n",
      "CE: 0.3609, OBJ: 0.4410, TOTAL: 0.3769\n",
      "CE: 0.2644, OBJ: 0.3371, TOTAL: 0.2790\n",
      "CE: 0.3589, OBJ: 0.2655, TOTAL: 0.3402\n",
      "CE: 0.3045, OBJ: 0.3376, TOTAL: 0.3111\n",
      "CE: 0.2922, OBJ: 0.1955, TOTAL: 0.2729\n",
      "CE: 0.1765, OBJ: 0.3017, TOTAL: 0.2015\n",
      "CE: 0.3180, OBJ: 0.3412, TOTAL: 0.3227\n",
      "CE: 0.5591, OBJ: 0.3028, TOTAL: 0.5078\n",
      "CE: 0.3287, OBJ: 0.3098, TOTAL: 0.3249\n",
      "CE: 0.1659, OBJ: 0.3486, TOTAL: 0.2024\n",
      "CE: 0.3408, OBJ: 0.2778, TOTAL: 0.3282\n",
      "CE: 0.3606, OBJ: 0.3815, TOTAL: 0.3648\n",
      "CE: 0.4982, OBJ: 0.3275, TOTAL: 0.4641\n",
      "CE: 0.5111, OBJ: 0.3635, TOTAL: 0.4816\n",
      "CE: 0.5836, OBJ: 0.2719, TOTAL: 0.5212\n",
      "CE: 0.2437, OBJ: 0.3922, TOTAL: 0.2734\n",
      "CE: 0.6673, OBJ: 0.4172, TOTAL: 0.6173\n",
      "CE: 0.1782, OBJ: 0.2545, TOTAL: 0.1935\n",
      "CE: 0.3924, OBJ: 0.3509, TOTAL: 0.3841\n",
      "CE: 0.3840, OBJ: 0.3918, TOTAL: 0.3856\n",
      "CE: 0.2551, OBJ: 0.3373, TOTAL: 0.2716\n",
      "CE: 0.1361, OBJ: 0.2855, TOTAL: 0.1660\n",
      "CE: 0.5790, OBJ: 0.2264, TOTAL: 0.5085\n",
      "CE: 0.3111, OBJ: 0.3051, TOTAL: 0.3099\n",
      "CE: 0.2767, OBJ: 0.3927, TOTAL: 0.2999\n",
      "CE: 0.3258, OBJ: 0.2704, TOTAL: 0.3147\n",
      "CE: 0.3071, OBJ: 0.3425, TOTAL: 0.3142\n",
      "CE: 0.3855, OBJ: 0.3914, TOTAL: 0.3867\n",
      "CE: 0.1224, OBJ: 0.2555, TOTAL: 0.1490\n",
      "CE: 0.3096, OBJ: 0.2777, TOTAL: 0.3032\n",
      "CE: 0.2575, OBJ: 0.2013, TOTAL: 0.2463\n",
      "CE: 0.2259, OBJ: 0.2991, TOTAL: 0.2405\n",
      "CE: 0.1329, OBJ: 0.3770, TOTAL: 0.1817\n",
      "CE: 0.1799, OBJ: 0.1993, TOTAL: 0.1838\n",
      "CE: 0.4246, OBJ: 0.3532, TOTAL: 0.4103\n",
      "CE: 0.1436, OBJ: 0.4024, TOTAL: 0.1953\n",
      "CE: 0.4105, OBJ: 0.3482, TOTAL: 0.3981\n",
      "CE: 0.0987, OBJ: 0.3245, TOTAL: 0.1438\n",
      "CE: 0.3161, OBJ: 0.3488, TOTAL: 0.3227\n",
      "CE: 0.1770, OBJ: 0.2177, TOTAL: 0.1851\n",
      "CE: 0.2315, OBJ: 0.3076, TOTAL: 0.2467\n",
      "CE: 0.5004, OBJ: 0.2315, TOTAL: 0.4467\n",
      "CE: 0.3332, OBJ: 0.4310, TOTAL: 0.3528\n",
      "CE: 0.2832, OBJ: 0.4682, TOTAL: 0.3202\n",
      "CE: 0.1579, OBJ: 0.3057, TOTAL: 0.1874\n",
      "CE: 0.2565, OBJ: 0.4281, TOTAL: 0.2908\n",
      "CE: 0.2340, OBJ: 0.3096, TOTAL: 0.2491\n",
      "CE: 0.3065, OBJ: 0.4643, TOTAL: 0.3381\n",
      "CE: 0.3592, OBJ: 0.3577, TOTAL: 0.3589\n",
      "CE: 0.1532, OBJ: 0.1802, TOTAL: 0.1586\n",
      "CE: 0.4105, OBJ: 0.4593, TOTAL: 0.4202\n",
      "CE: 0.2014, OBJ: 0.3240, TOTAL: 0.2259\n",
      "CE: 0.4799, OBJ: 0.1657, TOTAL: 0.4171\n",
      "CE: 0.3260, OBJ: 0.2368, TOTAL: 0.3081\n",
      "CE: 0.5032, OBJ: 0.3575, TOTAL: 0.4740\n",
      "CE: 0.2311, OBJ: 0.5549, TOTAL: 0.2959\n",
      "CE: 0.2661, OBJ: 0.3632, TOTAL: 0.2855\n",
      "CE: 0.2981, OBJ: 0.3274, TOTAL: 0.3039\n",
      "CE: 0.5194, OBJ: 0.2826, TOTAL: 0.4721\n",
      "CE: 0.1987, OBJ: 0.5421, TOTAL: 0.2674\n",
      "CE: 0.2955, OBJ: 0.3422, TOTAL: 0.3048\n",
      "CE: 0.3363, OBJ: 0.3229, TOTAL: 0.3336\n",
      "CE: 0.2065, OBJ: 0.5133, TOTAL: 0.2679\n",
      "CE: 0.5851, OBJ: 0.2717, TOTAL: 0.5224\n",
      "CE: 0.0295, OBJ: 0.1396, TOTAL: 0.0515\n",
      "CE: 0.2088, OBJ: 0.4014, TOTAL: 0.2473\n",
      "CE: 0.1544, OBJ: 0.4374, TOTAL: 0.2110\n",
      "CE: 0.2380, OBJ: 0.3556, TOTAL: 0.2615\n",
      "CE: 0.4320, OBJ: 0.2645, TOTAL: 0.3985\n",
      "CE: 0.3221, OBJ: 0.3097, TOTAL: 0.3196\n",
      "CE: 0.5008, OBJ: 0.1984, TOTAL: 0.4403\n",
      "CE: 0.3998, OBJ: 0.4491, TOTAL: 0.4097\n",
      "CE: 0.5875, OBJ: 0.3261, TOTAL: 0.5352\n",
      "CE: 0.3323, OBJ: 0.3813, TOTAL: 0.3421\n",
      "CE: 0.3651, OBJ: 0.5213, TOTAL: 0.3963\n",
      "CE: 0.2983, OBJ: 0.3025, TOTAL: 0.2991\n",
      "CE: 0.1760, OBJ: 0.2232, TOTAL: 0.1855\n",
      "CE: 0.3769, OBJ: 0.2984, TOTAL: 0.3612\n",
      "CE: 0.6332, OBJ: 0.3668, TOTAL: 0.5799\n",
      "CE: 0.4241, OBJ: 0.2729, TOTAL: 0.3938\n",
      "CE: 0.4296, OBJ: 0.4148, TOTAL: 0.4266\n",
      "CE: 0.1547, OBJ: 0.3257, TOTAL: 0.1889\n",
      "CE: 0.3397, OBJ: 0.2171, TOTAL: 0.3152\n",
      "CE: 0.4185, OBJ: 0.2696, TOTAL: 0.3887\n",
      "CE: 0.3481, OBJ: 0.3948, TOTAL: 0.3574\n",
      "CE: 0.4758, OBJ: 0.3932, TOTAL: 0.4593\n",
      "CE: 0.2020, OBJ: 0.2429, TOTAL: 0.2102\n",
      "CE: 0.2547, OBJ: 0.2426, TOTAL: 0.2522\n",
      "CE: 0.3465, OBJ: 0.3051, TOTAL: 0.3382\n",
      "CE: 0.4117, OBJ: 0.4510, TOTAL: 0.4196\n",
      "CE: 0.3051, OBJ: 0.3098, TOTAL: 0.3060\n",
      "CE: 0.1867, OBJ: 0.4263, TOTAL: 0.2346\n",
      "CE: 0.0602, OBJ: 0.2275, TOTAL: 0.0937\n",
      "CE: 0.3038, OBJ: 0.4002, TOTAL: 0.3230\n",
      "CE: 0.1762, OBJ: 0.3619, TOTAL: 0.2133\n",
      "CE: 0.1436, OBJ: 0.2086, TOTAL: 0.1566\n",
      "CE: 0.2900, OBJ: 0.3800, TOTAL: 0.3080\n",
      "CE: 0.4087, OBJ: 0.3014, TOTAL: 0.3872\n",
      "CE: 0.3593, OBJ: 0.2904, TOTAL: 0.3455\n",
      "CE: 0.2799, OBJ: 0.2631, TOTAL: 0.2765\n",
      "CE: 0.2430, OBJ: 0.2487, TOTAL: 0.2441\n",
      "CE: 0.2818, OBJ: 0.4066, TOTAL: 0.3067\n",
      "CE: 0.5228, OBJ: 0.3776, TOTAL: 0.4937\n",
      "CE: 0.1632, OBJ: 0.2864, TOTAL: 0.1879\n",
      "CE: 0.3077, OBJ: 0.2133, TOTAL: 0.2888\n",
      "CE: 0.7320, OBJ: 0.4771, TOTAL: 0.6810\n",
      "CE: 0.4721, OBJ: 0.3132, TOTAL: 0.4403\n",
      "CE: 0.2512, OBJ: 0.3965, TOTAL: 0.2803\n",
      "CE: 0.3181, OBJ: 0.3580, TOTAL: 0.3261\n",
      "CE: 0.5455, OBJ: 0.4689, TOTAL: 0.5302\n",
      "CE: 0.2020, OBJ: 0.2798, TOTAL: 0.2176\n",
      "CE: 0.5427, OBJ: 0.3969, TOTAL: 0.5135\n",
      "CE: 0.2333, OBJ: 0.3449, TOTAL: 0.2556\n",
      "CE: 0.1794, OBJ: 0.2995, TOTAL: 0.2034\n",
      "CE: 0.6003, OBJ: 0.4083, TOTAL: 0.5619\n",
      "CE: 0.0626, OBJ: 0.2028, TOTAL: 0.0907\n",
      "CE: 0.2638, OBJ: 0.5082, TOTAL: 0.3127\n",
      "CE: 0.2017, OBJ: 0.2149, TOTAL: 0.2043\n",
      "CE: 0.2871, OBJ: 0.4245, TOTAL: 0.3146\n",
      "CE: 0.1712, OBJ: 0.3450, TOTAL: 0.2059\n",
      "CE: 0.1150, OBJ: 0.4269, TOTAL: 0.1774\n",
      "CE: 0.3031, OBJ: 0.2853, TOTAL: 0.2995\n",
      "CE: 0.1630, OBJ: 0.2768, TOTAL: 0.1857\n",
      "CE: 0.3266, OBJ: 0.2692, TOTAL: 0.3151\n",
      "CE: 0.4084, OBJ: 0.2448, TOTAL: 0.3757\n",
      "CE: 0.4264, OBJ: 0.3478, TOTAL: 0.4107\n",
      "CE: 0.7662, OBJ: 0.3836, TOTAL: 0.6897\n",
      "CE: 0.2355, OBJ: 0.2185, TOTAL: 0.2321\n",
      "CE: 0.3737, OBJ: 0.4122, TOTAL: 0.3814\n",
      "CE: 0.2965, OBJ: 0.2158, TOTAL: 0.2803\n",
      "CE: 0.4173, OBJ: 0.2695, TOTAL: 0.3877\n",
      "CE: 0.3725, OBJ: 0.3778, TOTAL: 0.3735\n",
      "CE: 0.4803, OBJ: 0.3615, TOTAL: 0.4566\n",
      "CE: 0.3484, OBJ: 0.3720, TOTAL: 0.3531\n",
      "CE: 0.2112, OBJ: 0.4123, TOTAL: 0.2514\n",
      "CE: 0.3723, OBJ: 0.3462, TOTAL: 0.3670\n",
      "CE: 0.4467, OBJ: 0.4778, TOTAL: 0.4529\n",
      "CE: 0.4410, OBJ: 0.4408, TOTAL: 0.4409\n",
      "CE: 0.1482, OBJ: 0.1758, TOTAL: 0.1537\n",
      "CE: 0.1157, OBJ: 0.2525, TOTAL: 0.1431\n",
      "CE: 0.2268, OBJ: 0.4285, TOTAL: 0.2671\n",
      "CE: 0.2547, OBJ: 0.4039, TOTAL: 0.2845\n",
      "CE: 0.5081, OBJ: 0.3744, TOTAL: 0.4814\n",
      "CE: 0.0969, OBJ: 0.4457, TOTAL: 0.1667\n",
      "CE: 0.1338, OBJ: 0.2655, TOTAL: 0.1601\n",
      "CE: 0.2562, OBJ: 0.2244, TOTAL: 0.2499\n",
      "CE: 0.3825, OBJ: 0.3231, TOTAL: 0.3706\n",
      "CE: 0.2710, OBJ: 0.2580, TOTAL: 0.2684\n",
      "CE: 0.2551, OBJ: 0.2515, TOTAL: 0.2544\n",
      "CE: 0.1573, OBJ: 0.3342, TOTAL: 0.1927\n",
      "CE: 0.2170, OBJ: 0.2440, TOTAL: 0.2224\n",
      "CE: 0.1836, OBJ: 0.2713, TOTAL: 0.2012\n",
      "CE: 0.1497, OBJ: 0.4343, TOTAL: 0.2066\n",
      "CE: 0.0850, OBJ: 0.2782, TOTAL: 0.1237\n",
      "CE: 0.6979, OBJ: 0.2671, TOTAL: 0.6118\n",
      "CE: 0.5739, OBJ: 0.3900, TOTAL: 0.5371\n",
      "CE: 0.5951, OBJ: 0.2983, TOTAL: 0.5357\n",
      "CE: 0.3696, OBJ: 0.2837, TOTAL: 0.3524\n",
      "CE: 0.2848, OBJ: 0.3189, TOTAL: 0.2917\n",
      "CE: 0.3264, OBJ: 0.2393, TOTAL: 0.3089\n",
      "CE: 0.3694, OBJ: 0.3395, TOTAL: 0.3634\n",
      "CE: 0.2923, OBJ: 0.3421, TOTAL: 0.3023\n",
      "CE: 0.3239, OBJ: 0.3002, TOTAL: 0.3191\n",
      "CE: 0.3767, OBJ: 0.2305, TOTAL: 0.3475\n",
      "CE: 0.1408, OBJ: 0.3691, TOTAL: 0.1865\n",
      "CE: 0.3633, OBJ: 0.3003, TOTAL: 0.3507\n",
      "CE: 0.5736, OBJ: 0.3696, TOTAL: 0.5328\n",
      "CE: 0.7246, OBJ: 0.4348, TOTAL: 0.6666\n",
      "CE: 0.3704, OBJ: 0.2003, TOTAL: 0.3364\n",
      "CE: 0.1409, OBJ: 0.2789, TOTAL: 0.1685\n",
      "CE: 0.2320, OBJ: 0.3758, TOTAL: 0.2608\n",
      "CE: 0.3788, OBJ: 0.4709, TOTAL: 0.3973\n",
      "CE: 0.4839, OBJ: 0.4000, TOTAL: 0.4671\n",
      "CE: 0.1333, OBJ: 0.2357, TOTAL: 0.1538\n",
      "CE: 0.2473, OBJ: 0.2796, TOTAL: 0.2538\n",
      "CE: 0.1360, OBJ: 0.1917, TOTAL: 0.1472\n",
      "CE: 0.1271, OBJ: 0.4047, TOTAL: 0.1826\n",
      "CE: 0.3039, OBJ: 0.3375, TOTAL: 0.3106\n",
      "CE: 0.2144, OBJ: 0.2063, TOTAL: 0.2127\n",
      "CE: 0.0259, OBJ: 0.2980, TOTAL: 0.0803\n",
      "CE: 0.1111, OBJ: 0.2097, TOTAL: 0.1308\n",
      "CE: 0.1780, OBJ: 0.2363, TOTAL: 0.1897\n",
      "CE: 0.2061, OBJ: 0.1694, TOTAL: 0.1988\n",
      "CE: 0.1084, OBJ: 0.2172, TOTAL: 0.1302\n",
      "CE: 0.1974, OBJ: 0.3922, TOTAL: 0.2363\n",
      "CE: 0.1013, OBJ: 0.1783, TOTAL: 0.1167\n",
      "CE: 0.3118, OBJ: 0.5471, TOTAL: 0.3589\n",
      "CE: 0.1308, OBJ: 0.3953, TOTAL: 0.1837\n",
      "CE: 0.3759, OBJ: 0.3677, TOTAL: 0.3743\n",
      "CE: 0.2932, OBJ: 0.2606, TOTAL: 0.2867\n",
      "CE: 0.2943, OBJ: 0.3731, TOTAL: 0.3100\n",
      "CE: 0.3808, OBJ: 0.3178, TOTAL: 0.3682\n",
      "CE: 0.3221, OBJ: 0.2539, TOTAL: 0.3084\n",
      "CE: 0.2683, OBJ: 0.2545, TOTAL: 0.2656\n",
      "CE: 0.1797, OBJ: 0.3497, TOTAL: 0.2137\n",
      "CE: 0.4756, OBJ: 0.3426, TOTAL: 0.4490\n",
      "CE: 0.6097, OBJ: 0.3705, TOTAL: 0.5618\n",
      "CE: 0.1476, OBJ: 0.4001, TOTAL: 0.1981\n",
      "CE: 0.6341, OBJ: 0.2830, TOTAL: 0.5638\n",
      "CE: 0.2577, OBJ: 0.2384, TOTAL: 0.2539\n",
      "CE: 0.0960, OBJ: 0.2329, TOTAL: 0.1234\n",
      "CE: 0.3362, OBJ: 0.3037, TOTAL: 0.3297\n",
      "CE: 0.2935, OBJ: 0.2671, TOTAL: 0.2882\n",
      "CE: 0.2710, OBJ: 0.5008, TOTAL: 0.3170\n",
      "CE: 0.5517, OBJ: 0.3403, TOTAL: 0.5094\n",
      "CE: 0.2470, OBJ: 0.2054, TOTAL: 0.2387\n",
      "CE: 0.3042, OBJ: 0.3789, TOTAL: 0.3191\n",
      "CE: 0.3707, OBJ: 0.3139, TOTAL: 0.3593\n",
      "CE: 0.2545, OBJ: 0.3639, TOTAL: 0.2764\n",
      "CE: 0.5349, OBJ: 0.3809, TOTAL: 0.5041\n",
      "CE: 0.1415, OBJ: 0.2182, TOTAL: 0.1568\n",
      "CE: 0.1580, OBJ: 0.4636, TOTAL: 0.2191\n",
      "CE: 0.3205, OBJ: 0.4264, TOTAL: 0.3417\n",
      "CE: 0.2731, OBJ: 0.2778, TOTAL: 0.2740\n",
      "CE: 0.4086, OBJ: 0.3455, TOTAL: 0.3960\n",
      "CE: 0.3356, OBJ: 0.3618, TOTAL: 0.3409\n",
      "CE: 0.2516, OBJ: 0.2966, TOTAL: 0.2606\n",
      "CE: 0.3760, OBJ: 0.3776, TOTAL: 0.3764\n",
      "CE: 0.4587, OBJ: 0.3684, TOTAL: 0.4407\n",
      "CE: 0.0810, OBJ: 0.3005, TOTAL: 0.1249\n",
      "CE: 0.1367, OBJ: 0.3237, TOTAL: 0.1741\n",
      "CE: 0.2534, OBJ: 0.3337, TOTAL: 0.2695\n",
      "CE: 0.2003, OBJ: 0.1424, TOTAL: 0.1887\n",
      "CE: 0.1360, OBJ: 0.2034, TOTAL: 0.1495\n",
      "CE: 0.3621, OBJ: 0.2962, TOTAL: 0.3489\n",
      "CE: 0.4882, OBJ: 0.2727, TOTAL: 0.4451\n",
      "CE: 0.2360, OBJ: 0.4724, TOTAL: 0.2833\n",
      "CE: 0.1550, OBJ: 0.3918, TOTAL: 0.2023\n",
      "CE: 0.2380, OBJ: 0.2368, TOTAL: 0.2378\n",
      "CE: 0.3103, OBJ: 0.2394, TOTAL: 0.2961\n",
      "CE: 0.2764, OBJ: 0.3695, TOTAL: 0.2950\n",
      "CE: 0.2207, OBJ: 0.3361, TOTAL: 0.2438\n",
      "CE: 0.5082, OBJ: 0.2721, TOTAL: 0.4610\n",
      "CE: 0.2270, OBJ: 0.3321, TOTAL: 0.2480\n",
      "CE: 0.2977, OBJ: 0.5780, TOTAL: 0.3538\n",
      "CE: 0.1181, OBJ: 0.3108, TOTAL: 0.1567\n",
      "CE: 0.7872, OBJ: 0.4003, TOTAL: 0.7098\n",
      "CE: 0.2105, OBJ: 0.3641, TOTAL: 0.2412\n",
      "CE: 0.2466, OBJ: 0.2947, TOTAL: 0.2562\n",
      "CE: 0.3604, OBJ: 0.2834, TOTAL: 0.3450\n",
      "CE: 0.1345, OBJ: 0.2533, TOTAL: 0.1583\n",
      "CE: 0.1015, OBJ: 0.2805, TOTAL: 0.1373\n",
      "CE: 0.2999, OBJ: 0.3784, TOTAL: 0.3156\n",
      "CE: 0.6727, OBJ: 0.2679, TOTAL: 0.5917\n",
      "CE: 0.2873, OBJ: 0.2830, TOTAL: 0.2864\n",
      "CE: 0.3864, OBJ: 0.5545, TOTAL: 0.4200\n",
      "CE: 0.3145, OBJ: 0.3914, TOTAL: 0.3299\n",
      "CE: 0.1756, OBJ: 0.2576, TOTAL: 0.1920\n",
      "CE: 0.6467, OBJ: 0.3916, TOTAL: 0.5957\n",
      "CE: 0.3667, OBJ: 0.4671, TOTAL: 0.3868\n",
      "CE: 0.4209, OBJ: 0.4369, TOTAL: 0.4241\n",
      "CE: 0.6229, OBJ: 0.3855, TOTAL: 0.5754\n",
      "CE: 0.3239, OBJ: 0.2579, TOTAL: 0.3107\n",
      "CE: 0.4282, OBJ: 0.4132, TOTAL: 0.4252\n",
      "CE: 0.3377, OBJ: 0.4593, TOTAL: 0.3620\n",
      "CE: 0.5537, OBJ: 0.4635, TOTAL: 0.5356\n",
      "CE: 0.6260, OBJ: 0.3210, TOTAL: 0.5650\n",
      "CE: 0.2296, OBJ: 0.2224, TOTAL: 0.2282\n",
      "CE: 0.2773, OBJ: 0.3375, TOTAL: 0.2893\n",
      "CE: 0.3765, OBJ: 0.3316, TOTAL: 0.3675\n",
      "CE: 0.6169, OBJ: 0.2297, TOTAL: 0.5395\n",
      "CE: 0.3065, OBJ: 0.3536, TOTAL: 0.3159\n",
      "CE: 0.2589, OBJ: 0.2481, TOTAL: 0.2567\n",
      "CE: 0.1535, OBJ: 0.3049, TOTAL: 0.1838\n",
      "CE: 0.2793, OBJ: 0.3431, TOTAL: 0.2920\n",
      "CE: 0.1573, OBJ: 0.4337, TOTAL: 0.2126\n",
      "CE: 0.1206, OBJ: 0.3605, TOTAL: 0.1686\n",
      "CE: 0.4579, OBJ: 0.3414, TOTAL: 0.4346\n",
      "CE: 0.8957, OBJ: 0.3371, TOTAL: 0.7840\n",
      "CE: 0.4175, OBJ: 0.4019, TOTAL: 0.4144\n",
      "CE: 0.7261, OBJ: 0.5514, TOTAL: 0.6912\n",
      "CE: 0.3653, OBJ: 0.4114, TOTAL: 0.3745\n",
      "CE: 0.1754, OBJ: 0.3071, TOTAL: 0.2018\n",
      "CE: 0.2121, OBJ: 0.2438, TOTAL: 0.2184\n",
      "CE: 0.2635, OBJ: 0.2265, TOTAL: 0.2561\n",
      "CE: 0.4180, OBJ: 0.3063, TOTAL: 0.3956\n",
      "CE: 0.0941, OBJ: 0.3115, TOTAL: 0.1376\n",
      "CE: 0.3125, OBJ: 0.3590, TOTAL: 0.3218\n",
      "CE: 0.1892, OBJ: 0.2714, TOTAL: 0.2056\n",
      "CE: 0.3431, OBJ: 0.2434, TOTAL: 0.3232\n",
      "CE: 0.1867, OBJ: 0.2797, TOTAL: 0.2053\n",
      "CE: 0.2652, OBJ: 0.3836, TOTAL: 0.2889\n",
      "CE: 0.3481, OBJ: 0.3545, TOTAL: 0.3493\n",
      "CE: 0.4797, OBJ: 0.2896, TOTAL: 0.4417\n",
      "CE: 0.3619, OBJ: 0.4466, TOTAL: 0.3789\n",
      "CE: 0.2911, OBJ: 0.3444, TOTAL: 0.3018\n",
      "CE: 0.4215, OBJ: 0.2481, TOTAL: 0.3868\n",
      "CE: 0.3132, OBJ: 0.4014, TOTAL: 0.3308\n",
      "CE: 0.3151, OBJ: 0.2880, TOTAL: 0.3097\n",
      "CE: 0.1758, OBJ: 0.1753, TOTAL: 0.1757\n",
      "CE: 0.1598, OBJ: 0.1726, TOTAL: 0.1624\n",
      "CE: 0.7572, OBJ: 0.2699, TOTAL: 0.6598\n",
      "CE: 0.4772, OBJ: 0.4306, TOTAL: 0.4678\n",
      "CE: 0.4894, OBJ: 0.2742, TOTAL: 0.4464\n",
      "CE: 0.2511, OBJ: 0.2929, TOTAL: 0.2595\n",
      "CE: 0.5078, OBJ: 0.2185, TOTAL: 0.4500\n",
      "CE: 0.4599, OBJ: 0.2008, TOTAL: 0.4081\n",
      "CE: 0.3153, OBJ: 0.1771, TOTAL: 0.2877\n",
      "CE: 0.0647, OBJ: 0.4042, TOTAL: 0.1326\n",
      "CE: 0.2762, OBJ: 0.5036, TOTAL: 0.3217\n",
      "CE: 0.2372, OBJ: 0.2817, TOTAL: 0.2461\n",
      "CE: 0.1391, OBJ: 0.2657, TOTAL: 0.1644\n",
      "CE: 0.3465, OBJ: 0.2839, TOTAL: 0.3339\n",
      "CE: 0.4051, OBJ: 0.1896, TOTAL: 0.3620\n",
      "CE: 0.3049, OBJ: 0.5254, TOTAL: 0.3490\n",
      "CE: 0.3356, OBJ: 0.2724, TOTAL: 0.3229\n",
      "CE: 0.7767, OBJ: 0.3450, TOTAL: 0.6903\n",
      "CE: 0.4080, OBJ: 0.3456, TOTAL: 0.3955\n",
      "CE: 0.2974, OBJ: 0.3091, TOTAL: 0.2997\n",
      "CE: 0.2289, OBJ: 0.2134, TOTAL: 0.2258\n",
      "CE: 0.4777, OBJ: 0.4720, TOTAL: 0.4766\n",
      "CE: 0.2026, OBJ: 0.3554, TOTAL: 0.2331\n",
      "CE: 0.5175, OBJ: 0.2838, TOTAL: 0.4707\n",
      "CE: 0.0944, OBJ: 0.2282, TOTAL: 0.1211\n",
      "CE: 0.9323, OBJ: 0.5220, TOTAL: 0.8502\n",
      "CE: 0.3794, OBJ: 0.3506, TOTAL: 0.3736\n",
      "CE: 0.5244, OBJ: 0.3337, TOTAL: 0.4863\n",
      "CE: 0.2096, OBJ: 0.4378, TOTAL: 0.2552\n",
      "CE: 0.3281, OBJ: 0.2364, TOTAL: 0.3098\n",
      "CE: 0.5021, OBJ: 0.4180, TOTAL: 0.4853\n",
      "CE: 0.4196, OBJ: 0.2415, TOTAL: 0.3840\n",
      "CE: 0.1079, OBJ: 0.2646, TOTAL: 0.1393\n",
      "CE: 0.4525, OBJ: 0.3081, TOTAL: 0.4236\n",
      "CE: 0.2588, OBJ: 0.2595, TOTAL: 0.2590\n",
      "CE: 0.4608, OBJ: 0.3623, TOTAL: 0.4411\n",
      "CE: 0.2273, OBJ: 0.1870, TOTAL: 0.2192\n",
      "CE: 0.1633, OBJ: 0.2851, TOTAL: 0.1877\n",
      "CE: 0.1416, OBJ: 0.3685, TOTAL: 0.1870\n",
      "CE: 0.2662, OBJ: 0.2392, TOTAL: 0.2608\n",
      "CE: 0.3920, OBJ: 0.3185, TOTAL: 0.3773\n",
      "CE: 0.2336, OBJ: 0.3791, TOTAL: 0.2627\n",
      "CE: 0.1800, OBJ: 0.3706, TOTAL: 0.2181\n",
      "CE: 0.5116, OBJ: 0.3034, TOTAL: 0.4699\n",
      "CE: 0.2631, OBJ: 0.4662, TOTAL: 0.3037\n",
      "CE: 0.3749, OBJ: 0.4227, TOTAL: 0.3844\n",
      "CE: 0.2425, OBJ: 0.4601, TOTAL: 0.2861\n",
      "CE: 0.7465, OBJ: 0.3873, TOTAL: 0.6746\n",
      "CE: 0.6295, OBJ: 0.2161, TOTAL: 0.5468\n",
      "CE: 0.0933, OBJ: 0.2222, TOTAL: 0.1191\n",
      "CE: 0.2488, OBJ: 0.1926, TOTAL: 0.2376\n",
      "CE: 0.2242, OBJ: 0.2897, TOTAL: 0.2373\n",
      "CE: 0.1677, OBJ: 0.2571, TOTAL: 0.1856\n",
      "CE: 0.2745, OBJ: 0.3560, TOTAL: 0.2908\n",
      "CE: 0.6577, OBJ: 0.4107, TOTAL: 0.6083\n",
      "CE: 0.2903, OBJ: 0.2869, TOTAL: 0.2896\n",
      "CE: 0.5431, OBJ: 0.3580, TOTAL: 0.5061\n",
      "CE: 0.1245, OBJ: 0.3223, TOTAL: 0.1641\n",
      "CE: 0.1684, OBJ: 0.2128, TOTAL: 0.1773\n",
      "CE: 0.4113, OBJ: 0.3218, TOTAL: 0.3934\n",
      "CE: 0.2192, OBJ: 0.1497, TOTAL: 0.2053\n",
      "CE: 0.1398, OBJ: 0.3053, TOTAL: 0.1729\n",
      "CE: 0.4413, OBJ: 0.3066, TOTAL: 0.4144\n",
      "CE: 0.2007, OBJ: 0.2356, TOTAL: 0.2077\n",
      "CE: 0.2420, OBJ: 0.2557, TOTAL: 0.2447\n",
      "CE: 0.3628, OBJ: 0.3446, TOTAL: 0.3592\n",
      "CE: 0.3923, OBJ: 0.3727, TOTAL: 0.3884\n",
      "CE: 0.4362, OBJ: 0.2743, TOTAL: 0.4038\n",
      "CE: 0.2132, OBJ: 0.3448, TOTAL: 0.2395\n",
      "CE: 0.2947, OBJ: 0.3453, TOTAL: 0.3048\n",
      "CE: 0.2464, OBJ: 0.4871, TOTAL: 0.2946\n",
      "CE: 0.3314, OBJ: 0.2862, TOTAL: 0.3224\n",
      "CE: 0.1886, OBJ: 0.2368, TOTAL: 0.1982\n",
      "CE: 0.5587, OBJ: 0.1904, TOTAL: 0.4851\n",
      "CE: 0.2393, OBJ: 0.2700, TOTAL: 0.2454\n",
      "CE: 0.7030, OBJ: 0.2535, TOTAL: 0.6131\n",
      "CE: 0.1921, OBJ: 0.2490, TOTAL: 0.2035\n",
      "CE: 0.3406, OBJ: 0.3653, TOTAL: 0.3455\n",
      "CE: 0.3034, OBJ: 0.3143, TOTAL: 0.3056\n",
      "CE: 0.5777, OBJ: 0.3267, TOTAL: 0.5275\n",
      "CE: 0.2520, OBJ: 0.4110, TOTAL: 0.2838\n",
      "CE: 0.3770, OBJ: 0.3273, TOTAL: 0.3670\n",
      "CE: 0.4248, OBJ: 0.3268, TOTAL: 0.4052\n",
      "CE: 0.6998, OBJ: 0.4430, TOTAL: 0.6484\n",
      "CE: 0.3739, OBJ: 0.4021, TOTAL: 0.3796\n",
      "CE: 0.2206, OBJ: 0.2977, TOTAL: 0.2361\n",
      "CE: 0.2917, OBJ: 0.2583, TOTAL: 0.2850\n",
      "CE: 0.1614, OBJ: 0.1320, TOTAL: 0.1555\n",
      "CE: 0.2592, OBJ: 0.2367, TOTAL: 0.2547\n",
      "CE: 0.3367, OBJ: 0.3242, TOTAL: 0.3342\n",
      "CE: 0.5473, OBJ: 0.4186, TOTAL: 0.5216\n",
      "CE: 0.3829, OBJ: 0.2356, TOTAL: 0.3534\n",
      "CE: 0.2126, OBJ: 0.2112, TOTAL: 0.2123\n",
      "CE: 0.2780, OBJ: 0.2657, TOTAL: 0.2755\n",
      "CE: 0.8049, OBJ: 0.2757, TOTAL: 0.6990\n",
      "CE: 0.1876, OBJ: 0.3046, TOTAL: 0.2110\n",
      "CE: 0.6470, OBJ: 0.3857, TOTAL: 0.5947\n",
      "CE: 0.6175, OBJ: 0.2628, TOTAL: 0.5465\n",
      "CE: 0.2379, OBJ: 0.3935, TOTAL: 0.2691\n",
      "CE: 0.5235, OBJ: 0.4981, TOTAL: 0.5184\n",
      "CE: 0.4329, OBJ: 0.3400, TOTAL: 0.4143\n",
      "CE: 0.0618, OBJ: 0.2290, TOTAL: 0.0952\n",
      "CE: 0.4292, OBJ: 0.3391, TOTAL: 0.4112\n",
      "CE: 0.4823, OBJ: 0.3313, TOTAL: 0.4521\n",
      "CE: 0.3277, OBJ: 0.5962, TOTAL: 0.3814\n",
      "CE: 0.2638, OBJ: 0.3497, TOTAL: 0.2810\n",
      "CE: 0.6075, OBJ: 0.1935, TOTAL: 0.5247\n",
      "CE: 0.3304, OBJ: 0.1712, TOTAL: 0.2986\n",
      "CE: 0.3645, OBJ: 0.2402, TOTAL: 0.3396\n",
      "CE: 0.3299, OBJ: 0.3257, TOTAL: 0.3290\n",
      "CE: 0.1975, OBJ: 0.4074, TOTAL: 0.2395\n",
      "CE: 0.5405, OBJ: 0.4248, TOTAL: 0.5174\n",
      "CE: 0.2555, OBJ: 0.3884, TOTAL: 0.2821\n",
      "CE: 0.1803, OBJ: 0.3162, TOTAL: 0.2075\n",
      "CE: 0.2887, OBJ: 0.2658, TOTAL: 0.2841\n",
      "CE: 0.3061, OBJ: 0.3271, TOTAL: 0.3103\n",
      "CE: 0.1354, OBJ: 0.4020, TOTAL: 0.1887\n",
      "CE: 0.4491, OBJ: 0.3404, TOTAL: 0.4274\n",
      "CE: 0.5436, OBJ: 0.3523, TOTAL: 0.5053\n",
      "CE: 0.1001, OBJ: 0.1975, TOTAL: 0.1196\n",
      "CE: 0.1798, OBJ: 0.4430, TOTAL: 0.2324\n",
      "CE: 0.1604, OBJ: 0.4247, TOTAL: 0.2132\n",
      "CE: 0.2724, OBJ: 0.3456, TOTAL: 0.2870\n",
      "CE: 0.1534, OBJ: 0.2741, TOTAL: 0.1775\n",
      "CE: 0.2370, OBJ: 0.2515, TOTAL: 0.2399\n",
      "CE: 0.0869, OBJ: 0.1722, TOTAL: 0.1040\n",
      "CE: 0.2695, OBJ: 0.2821, TOTAL: 0.2720\n",
      "CE: 0.2902, OBJ: 0.3499, TOTAL: 0.3022\n",
      "CE: 0.3077, OBJ: 0.4897, TOTAL: 0.3441\n",
      "CE: 0.3659, OBJ: 0.3634, TOTAL: 0.3654\n",
      "CE: 0.3992, OBJ: 0.3488, TOTAL: 0.3891\n",
      "CE: 0.6243, OBJ: 0.3018, TOTAL: 0.5598\n",
      "CE: 0.3718, OBJ: 0.3377, TOTAL: 0.3650\n",
      "CE: 0.2790, OBJ: 0.3983, TOTAL: 0.3028\n",
      "CE: 0.2028, OBJ: 0.2676, TOTAL: 0.2158\n",
      "CE: 0.2474, OBJ: 0.3526, TOTAL: 0.2684\n",
      "CE: 0.2420, OBJ: 0.1598, TOTAL: 0.2255\n",
      "CE: 0.3013, OBJ: 0.3037, TOTAL: 0.3018\n",
      "CE: 0.1972, OBJ: 0.4055, TOTAL: 0.2388\n",
      "CE: 0.5818, OBJ: 0.3298, TOTAL: 0.5314\n",
      "CE: 0.3863, OBJ: 0.2861, TOTAL: 0.3663\n",
      "CE: 0.3334, OBJ: 0.3390, TOTAL: 0.3345\n",
      "CE: 0.3409, OBJ: 0.2806, TOTAL: 0.3289\n",
      "CE: 0.4433, OBJ: 0.2522, TOTAL: 0.4051\n",
      "CE: 0.2951, OBJ: 0.2622, TOTAL: 0.2885\n",
      "CE: 0.1576, OBJ: 0.3428, TOTAL: 0.1946\n",
      "CE: 0.6732, OBJ: 0.2987, TOTAL: 0.5983\n",
      "CE: 0.2024, OBJ: 0.2867, TOTAL: 0.2193\n",
      "CE: 0.3285, OBJ: 0.3097, TOTAL: 0.3248\n",
      "CE: 0.1337, OBJ: 0.3210, TOTAL: 0.1712\n",
      "CE: 0.3109, OBJ: 0.2822, TOTAL: 0.3051\n",
      "CE: 0.4990, OBJ: 0.2726, TOTAL: 0.4537\n",
      "CE: 0.4749, OBJ: 0.2537, TOTAL: 0.4307\n",
      "CE: 0.4434, OBJ: 0.2369, TOTAL: 0.4021\n",
      "CE: 0.2463, OBJ: 0.2231, TOTAL: 0.2417\n",
      "CE: 0.6091, OBJ: 0.2207, TOTAL: 0.5314\n",
      "CE: 0.5353, OBJ: 0.3432, TOTAL: 0.4968\n",
      "CE: 0.1587, OBJ: 0.1881, TOTAL: 0.1646\n",
      "CE: 0.4591, OBJ: 0.3717, TOTAL: 0.4416\n",
      "CE: 0.1869, OBJ: 0.3542, TOTAL: 0.2203\n",
      "CE: 0.3275, OBJ: 0.5033, TOTAL: 0.3627\n",
      "CE: 0.3840, OBJ: 0.3318, TOTAL: 0.3736\n",
      "CE: 0.2378, OBJ: 0.3726, TOTAL: 0.2648\n",
      "CE: 0.1609, OBJ: 0.3841, TOTAL: 0.2056\n",
      "CE: 0.2020, OBJ: 0.2318, TOTAL: 0.2080\n",
      "CE: 0.2325, OBJ: 0.3432, TOTAL: 0.2547\n",
      "CE: 0.1250, OBJ: 0.3021, TOTAL: 0.1604\n",
      "CE: 0.2277, OBJ: 0.2529, TOTAL: 0.2328\n",
      "CE: 0.1369, OBJ: 0.2025, TOTAL: 0.1500\n",
      "CE: 0.0955, OBJ: 0.3127, TOTAL: 0.1390\n",
      "CE: 0.3890, OBJ: 0.4628, TOTAL: 0.4038\n",
      "CE: 0.2096, OBJ: 0.5046, TOTAL: 0.2686\n",
      "CE: 0.1847, OBJ: 0.2651, TOTAL: 0.2008\n",
      "CE: 0.6663, OBJ: 0.3905, TOTAL: 0.6111\n",
      "CE: 0.1470, OBJ: 0.4176, TOTAL: 0.2011\n",
      "CE: 0.4476, OBJ: 0.5418, TOTAL: 0.4664\n",
      "CE: 0.4177, OBJ: 0.3371, TOTAL: 0.4016\n",
      "CE: 0.3187, OBJ: 0.3210, TOTAL: 0.3191\n",
      "CE: 0.1480, OBJ: 0.3512, TOTAL: 0.1887\n",
      "CE: 0.0961, OBJ: 0.2765, TOTAL: 0.1322\n",
      "CE: 0.3655, OBJ: 0.5771, TOTAL: 0.4078\n",
      "CE: 0.1187, OBJ: 0.2672, TOTAL: 0.1484\n",
      "CE: 0.2242, OBJ: 0.3365, TOTAL: 0.2467\n",
      "CE: 0.3474, OBJ: 0.4878, TOTAL: 0.3755\n",
      "CE: 0.2240, OBJ: 0.2462, TOTAL: 0.2285\n",
      "CE: 0.5815, OBJ: 0.2757, TOTAL: 0.5203\n",
      "CE: 0.1174, OBJ: 0.3206, TOTAL: 0.1581\n",
      "CE: 0.4298, OBJ: 0.2957, TOTAL: 0.4030\n",
      "CE: 0.1420, OBJ: 0.3296, TOTAL: 0.1795\n",
      "CE: 0.5388, OBJ: 0.2625, TOTAL: 0.4835\n",
      "CE: 0.2310, OBJ: 0.3391, TOTAL: 0.2526\n",
      "CE: 0.4464, OBJ: 0.2321, TOTAL: 0.4035\n",
      "CE: 0.1691, OBJ: 0.2565, TOTAL: 0.1866\n",
      "CE: 0.2805, OBJ: 0.2062, TOTAL: 0.2657\n",
      "CE: 0.5483, OBJ: 0.3297, TOTAL: 0.5046\n",
      "CE: 0.2389, OBJ: 0.3769, TOTAL: 0.2665\n",
      "CE: 0.0699, OBJ: 0.3770, TOTAL: 0.1313\n",
      "CE: 0.5123, OBJ: 0.3277, TOTAL: 0.4753\n",
      "CE: 0.2782, OBJ: 0.2941, TOTAL: 0.2814\n",
      "CE: 0.2954, OBJ: 0.2038, TOTAL: 0.2771\n",
      "CE: 0.1188, OBJ: 0.2736, TOTAL: 0.1498\n",
      "CE: 0.2315, OBJ: 0.2293, TOTAL: 0.2311\n",
      "CE: 0.4284, OBJ: 0.2441, TOTAL: 0.3916\n",
      "CE: 0.0931, OBJ: 0.3181, TOTAL: 0.1381\n",
      "CE: 0.2204, OBJ: 0.3176, TOTAL: 0.2398\n",
      "CE: 0.3707, OBJ: 0.3733, TOTAL: 0.3712\n",
      "CE: 0.1387, OBJ: 0.1953, TOTAL: 0.1500\n",
      "CE: 0.4639, OBJ: 0.4498, TOTAL: 0.4611\n",
      "CE: 0.5012, OBJ: 0.2225, TOTAL: 0.4454\n",
      "CE: 0.2721, OBJ: 0.2956, TOTAL: 0.2768\n",
      "CE: 0.2568, OBJ: 0.3209, TOTAL: 0.2697\n",
      "CE: 0.2561, OBJ: 0.2700, TOTAL: 0.2589\n",
      "CE: 0.2484, OBJ: 0.1649, TOTAL: 0.2317\n",
      "CE: 0.5106, OBJ: 0.4526, TOTAL: 0.4990\n",
      "CE: 0.3572, OBJ: 0.3692, TOTAL: 0.3596\n",
      "CE: 0.3446, OBJ: 0.2919, TOTAL: 0.3340\n",
      "CE: 0.4754, OBJ: 0.3363, TOTAL: 0.4476\n",
      "CE: 0.1522, OBJ: 0.3190, TOTAL: 0.1856\n",
      "CE: 0.1698, OBJ: 0.4008, TOTAL: 0.2160\n",
      "CE: 0.3417, OBJ: 0.3170, TOTAL: 0.3367\n",
      "CE: 0.1504, OBJ: 0.3215, TOTAL: 0.1846\n",
      "CE: 0.2932, OBJ: 0.3370, TOTAL: 0.3020\n",
      "CE: 0.3707, OBJ: 0.2177, TOTAL: 0.3401\n",
      "CE: 0.3837, OBJ: 0.2408, TOTAL: 0.3551\n",
      "CE: 0.3950, OBJ: 0.2889, TOTAL: 0.3738\n",
      "CE: 0.2766, OBJ: 0.3057, TOTAL: 0.2824\n",
      "CE: 0.1568, OBJ: 0.3891, TOTAL: 0.2033\n",
      "CE: 0.2458, OBJ: 0.3561, TOTAL: 0.2678\n",
      "CE: 0.4581, OBJ: 0.3236, TOTAL: 0.4312\n",
      "CE: 0.4774, OBJ: 0.3552, TOTAL: 0.4530\n",
      "CE: 0.3118, OBJ: 0.3828, TOTAL: 0.3260\n",
      "CE: 0.2054, OBJ: 0.2159, TOTAL: 0.2075\n",
      "CE: 0.5920, OBJ: 0.3914, TOTAL: 0.5519\n",
      "CE: 0.1532, OBJ: 0.2420, TOTAL: 0.1710\n",
      "CE: 0.5872, OBJ: 0.2747, TOTAL: 0.5247\n",
      "CE: 0.1153, OBJ: 0.2615, TOTAL: 0.1446\n",
      "CE: 0.4524, OBJ: 0.2959, TOTAL: 0.4211\n",
      "CE: 0.3240, OBJ: 0.2392, TOTAL: 0.3070\n",
      "CE: 0.1956, OBJ: 0.3181, TOTAL: 0.2201\n",
      "CE: 0.2391, OBJ: 0.3560, TOTAL: 0.2625\n",
      "CE: 0.6107, OBJ: 0.3436, TOTAL: 0.5573\n",
      "CE: 0.2659, OBJ: 0.3683, TOTAL: 0.2864\n",
      "CE: 0.4280, OBJ: 0.4333, TOTAL: 0.4291\n",
      "CE: 0.5385, OBJ: 0.5300, TOTAL: 0.5368\n",
      "CE: 0.4137, OBJ: 0.2459, TOTAL: 0.3802\n",
      "CE: 0.5668, OBJ: 0.4639, TOTAL: 0.5462\n",
      "CE: 0.7090, OBJ: 0.4238, TOTAL: 0.6520\n",
      "CE: 0.8496, OBJ: 0.4122, TOTAL: 0.7621\n",
      "CE: 0.2342, OBJ: 0.3197, TOTAL: 0.2513\n",
      "CE: 0.1685, OBJ: 0.2665, TOTAL: 0.1881\n",
      "CE: 0.5078, OBJ: 0.2704, TOTAL: 0.4603\n",
      "CE: 0.1852, OBJ: 0.3367, TOTAL: 0.2155\n",
      "CE: 0.3794, OBJ: 0.3663, TOTAL: 0.3768\n",
      "CE: 0.1790, OBJ: 0.1875, TOTAL: 0.1807\n",
      "CE: 0.3889, OBJ: 0.3377, TOTAL: 0.3787\n",
      "CE: 0.3807, OBJ: 0.3081, TOTAL: 0.3662\n",
      "CE: 0.2434, OBJ: 0.3409, TOTAL: 0.2629\n",
      "CE: 0.3889, OBJ: 0.3917, TOTAL: 0.3894\n",
      "CE: 0.0842, OBJ: 0.3960, TOTAL: 0.1466\n",
      "CE: 0.7485, OBJ: 0.3459, TOTAL: 0.6680\n",
      "CE: 0.2116, OBJ: 0.2651, TOTAL: 0.2223\n",
      "CE: 0.6656, OBJ: 0.4206, TOTAL: 0.6166\n",
      "CE: 0.5451, OBJ: 0.2493, TOTAL: 0.4859\n",
      "CE: 0.5081, OBJ: 0.3755, TOTAL: 0.4815\n",
      "CE: 0.2307, OBJ: 0.3524, TOTAL: 0.2551\n",
      "CE: 0.4322, OBJ: 0.4700, TOTAL: 0.4398\n",
      "CE: 0.4817, OBJ: 0.2841, TOTAL: 0.4421\n",
      "CE: 0.3635, OBJ: 0.2984, TOTAL: 0.3505\n",
      "CE: 0.0503, OBJ: 0.2074, TOTAL: 0.0818\n",
      "CE: 0.4975, OBJ: 0.2294, TOTAL: 0.4439\n",
      "CE: 0.2302, OBJ: 0.3461, TOTAL: 0.2534\n",
      "CE: 0.4382, OBJ: 0.3007, TOTAL: 0.4107\n",
      "CE: 0.1985, OBJ: 0.3410, TOTAL: 0.2270\n",
      "CE: 0.1648, OBJ: 0.3043, TOTAL: 0.1927\n",
      "CE: 0.2783, OBJ: 0.3745, TOTAL: 0.2975\n",
      "CE: 0.3213, OBJ: 0.2486, TOTAL: 0.3068\n",
      "CE: 0.0784, OBJ: 0.2022, TOTAL: 0.1031\n",
      "CE: 0.4677, OBJ: 0.2666, TOTAL: 0.4275\n",
      "CE: 0.2613, OBJ: 0.2598, TOTAL: 0.2610\n",
      "CE: 0.2143, OBJ: 0.1796, TOTAL: 0.2074\n",
      "CE: 0.2033, OBJ: 0.2994, TOTAL: 0.2226\n",
      "CE: 0.3050, OBJ: 0.2853, TOTAL: 0.3011\n",
      "CE: 0.1939, OBJ: 0.3276, TOTAL: 0.2207\n",
      "CE: 0.3849, OBJ: 0.2897, TOTAL: 0.3658\n",
      "CE: 0.7696, OBJ: 0.5268, TOTAL: 0.7211\n",
      "CE: 0.3383, OBJ: 0.3286, TOTAL: 0.3364\n",
      "CE: 0.2450, OBJ: 0.4332, TOTAL: 0.2827\n",
      "CE: 0.2466, OBJ: 0.2477, TOTAL: 0.2468\n",
      "CE: 0.5481, OBJ: 0.4031, TOTAL: 0.5191\n",
      "CE: 0.1032, OBJ: 0.3360, TOTAL: 0.1497\n",
      "CE: 0.1189, OBJ: 0.2900, TOTAL: 0.1531\n",
      "CE: 0.2011, OBJ: 0.3133, TOTAL: 0.2235\n",
      "CE: 0.1795, OBJ: 0.2343, TOTAL: 0.1905\n",
      "CE: 0.5421, OBJ: 0.5053, TOTAL: 0.5348\n",
      "CE: 0.4697, OBJ: 0.2663, TOTAL: 0.4290\n",
      "CE: 0.6333, OBJ: 0.5178, TOTAL: 0.6102\n",
      "CE: 0.3376, OBJ: 0.2621, TOTAL: 0.3225\n",
      "CE: 0.2715, OBJ: 0.3822, TOTAL: 0.2936\n",
      "CE: 0.1753, OBJ: 0.3143, TOTAL: 0.2031\n",
      "CE: 0.2579, OBJ: 0.4252, TOTAL: 0.2914\n",
      "CE: 0.2147, OBJ: 0.2313, TOTAL: 0.2180\n",
      "CE: 0.4384, OBJ: 0.3164, TOTAL: 0.4140\n",
      "CE: 0.5971, OBJ: 0.3098, TOTAL: 0.5396\n",
      "CE: 0.5401, OBJ: 0.3666, TOTAL: 0.5054\n",
      "CE: 0.2825, OBJ: 0.2595, TOTAL: 0.2779\n",
      "CE: 0.4787, OBJ: 0.3661, TOTAL: 0.4562\n",
      "CE: 0.5381, OBJ: 0.3488, TOTAL: 0.5002\n",
      "CE: 0.3922, OBJ: 0.3812, TOTAL: 0.3900\n",
      "CE: 0.3239, OBJ: 0.3276, TOTAL: 0.3247\n",
      "CE: 0.1476, OBJ: 0.2618, TOTAL: 0.1705\n",
      "CE: 0.3136, OBJ: 0.2622, TOTAL: 0.3034\n",
      "CE: 0.3350, OBJ: 0.3908, TOTAL: 0.3462\n",
      "CE: 0.3321, OBJ: 0.3349, TOTAL: 0.3327\n",
      "CE: 0.0959, OBJ: 0.3939, TOTAL: 0.1555\n",
      "CE: 0.1726, OBJ: 0.3869, TOTAL: 0.2155\n",
      "CE: 0.2100, OBJ: 0.2828, TOTAL: 0.2245\n",
      "CE: 0.1745, OBJ: 0.2860, TOTAL: 0.1968\n",
      "CE: 0.6016, OBJ: 0.2904, TOTAL: 0.5393\n",
      "CE: 0.2198, OBJ: 0.3396, TOTAL: 0.2438\n",
      "CE: 0.1354, OBJ: 0.4857, TOTAL: 0.2054\n",
      "CE: 0.2809, OBJ: 0.3044, TOTAL: 0.2856\n",
      "CE: 0.2724, OBJ: 0.3499, TOTAL: 0.2879\n",
      "CE: 0.5835, OBJ: 0.3478, TOTAL: 0.5364\n",
      "CE: 0.3804, OBJ: 0.3489, TOTAL: 0.3741\n",
      "CE: 0.4058, OBJ: 0.2645, TOTAL: 0.3775\n",
      "CE: 0.1668, OBJ: 0.2722, TOTAL: 0.1879\n",
      "CE: 0.2257, OBJ: 0.3007, TOTAL: 0.2407\n",
      "CE: 0.6935, OBJ: 0.3483, TOTAL: 0.6245\n",
      "CE: 0.1079, OBJ: 0.2422, TOTAL: 0.1347\n",
      "CE: 0.1395, OBJ: 0.3028, TOTAL: 0.1722\n",
      "CE: 0.2742, OBJ: 0.4548, TOTAL: 0.3103\n",
      "CE: 0.4527, OBJ: 0.3346, TOTAL: 0.4291\n",
      "CE: 0.5114, OBJ: 0.4632, TOTAL: 0.5017\n",
      "CE: 0.2441, OBJ: 0.3698, TOTAL: 0.2693\n",
      "CE: 0.1792, OBJ: 0.2696, TOTAL: 0.1973\n",
      "CE: 0.2339, OBJ: 0.1670, TOTAL: 0.2205\n",
      "CE: 0.4472, OBJ: 0.2225, TOTAL: 0.4023\n",
      "CE: 0.1786, OBJ: 0.3945, TOTAL: 0.2218\n",
      "CE: 0.2802, OBJ: 0.3656, TOTAL: 0.2973\n",
      "CE: 0.2943, OBJ: 0.4536, TOTAL: 0.3262\n",
      "CE: 0.3319, OBJ: 0.2758, TOTAL: 0.3207\n",
      "CE: 0.2683, OBJ: 0.2447, TOTAL: 0.2636\n",
      "CE: 0.5310, OBJ: 0.2591, TOTAL: 0.4766\n",
      "CE: 0.2414, OBJ: 0.2602, TOTAL: 0.2452\n",
      "CE: 0.4505, OBJ: 0.4386, TOTAL: 0.4481\n",
      "CE: 0.4972, OBJ: 0.3968, TOTAL: 0.4771\n",
      "CE: 0.2530, OBJ: 0.2790, TOTAL: 0.2582\n",
      "CE: 0.5174, OBJ: 0.3695, TOTAL: 0.4878\n",
      "CE: 0.6780, OBJ: 0.4794, TOTAL: 0.6383\n",
      "CE: 0.5461, OBJ: 0.3793, TOTAL: 0.5128\n",
      "CE: 0.3738, OBJ: 0.1497, TOTAL: 0.3289\n",
      "CE: 0.2112, OBJ: 0.4344, TOTAL: 0.2558\n",
      "CE: 0.2713, OBJ: 0.3440, TOTAL: 0.2859\n",
      "CE: 0.5075, OBJ: 0.3605, TOTAL: 0.4781\n",
      "CE: 0.1725, OBJ: 0.3425, TOTAL: 0.2065\n",
      "CE: 0.1900, OBJ: 0.1814, TOTAL: 0.1883\n",
      "CE: 0.5927, OBJ: 0.4901, TOTAL: 0.5722\n",
      "CE: 0.1752, OBJ: 0.1601, TOTAL: 0.1722\n",
      "CE: 0.4723, OBJ: 0.3713, TOTAL: 0.4521\n",
      "CE: 0.4204, OBJ: 0.3285, TOTAL: 0.4021\n",
      "CE: 0.2719, OBJ: 0.2549, TOTAL: 0.2685\n",
      "CE: 0.3629, OBJ: 0.2510, TOTAL: 0.3405\n",
      "CE: 0.2179, OBJ: 0.3266, TOTAL: 0.2396\n",
      "CE: 0.2142, OBJ: 0.4118, TOTAL: 0.2537\n",
      "CE: 0.6461, OBJ: 0.2465, TOTAL: 0.5662\n",
      "CE: 0.6589, OBJ: 0.2532, TOTAL: 0.5777\n",
      "CE: 0.1821, OBJ: 0.2669, TOTAL: 0.1990\n",
      "CE: 0.3493, OBJ: 0.3877, TOTAL: 0.3570\n",
      "CE: 0.0778, OBJ: 0.3267, TOTAL: 0.1276\n",
      "CE: 0.1874, OBJ: 0.2969, TOTAL: 0.2093\n",
      "CE: 0.1430, OBJ: 0.1278, TOTAL: 0.1399\n",
      "CE: 0.1008, OBJ: 0.3033, TOTAL: 0.1413\n",
      "CE: 0.2525, OBJ: 0.3043, TOTAL: 0.2629\n",
      "CE: 0.2117, OBJ: 0.3937, TOTAL: 0.2481\n",
      "CE: 0.5073, OBJ: 0.2773, TOTAL: 0.4613\n",
      "CE: 0.4634, OBJ: 0.2979, TOTAL: 0.4303\n",
      "CE: 0.4440, OBJ: 0.4121, TOTAL: 0.4376\n",
      "CE: 0.1921, OBJ: 0.2875, TOTAL: 0.2112\n",
      "CE: 0.3634, OBJ: 0.2554, TOTAL: 0.3418\n",
      "CE: 0.5381, OBJ: 0.4477, TOTAL: 0.5200\n",
      "CE: 0.5726, OBJ: 0.3776, TOTAL: 0.5336\n",
      "CE: 0.3458, OBJ: 0.5023, TOTAL: 0.3771\n",
      "CE: 0.3247, OBJ: 0.3793, TOTAL: 0.3356\n",
      "CE: 0.4102, OBJ: 0.5499, TOTAL: 0.4381\n",
      "CE: 0.1745, OBJ: 0.3766, TOTAL: 0.2149\n",
      "CE: 0.3055, OBJ: 0.4018, TOTAL: 0.3248\n",
      "CE: 0.3529, OBJ: 0.3419, TOTAL: 0.3507\n",
      "CE: 0.3177, OBJ: 0.3188, TOTAL: 0.3179\n",
      "CE: 0.3769, OBJ: 0.2622, TOTAL: 0.3539\n",
      "CE: 0.1129, OBJ: 0.3387, TOTAL: 0.1581\n",
      "CE: 0.0714, OBJ: 0.2508, TOTAL: 0.1073\n",
      "CE: 0.3562, OBJ: 0.3756, TOTAL: 0.3601\n",
      "CE: 0.2934, OBJ: 0.3165, TOTAL: 0.2980\n",
      "CE: 0.2335, OBJ: 0.2880, TOTAL: 0.2444\n",
      "CE: 0.3807, OBJ: 0.3242, TOTAL: 0.3694\n",
      "CE: 0.1374, OBJ: 0.3809, TOTAL: 0.1861\n",
      "CE: 0.2965, OBJ: 0.2596, TOTAL: 0.2891\n",
      "CE: 0.1059, OBJ: 0.3350, TOTAL: 0.1517\n",
      "CE: 0.3497, OBJ: 0.2938, TOTAL: 0.3385\n",
      "CE: 0.2665, OBJ: 0.2697, TOTAL: 0.2671\n",
      "CE: 0.2219, OBJ: 0.2715, TOTAL: 0.2318\n",
      "CE: 0.2889, OBJ: 0.2583, TOTAL: 0.2828\n",
      "CE: 0.4763, OBJ: 0.3605, TOTAL: 0.4531\n",
      "CE: 0.5032, OBJ: 0.4619, TOTAL: 0.4949\n",
      "CE: 0.1577, OBJ: 0.2344, TOTAL: 0.1731\n",
      "CE: 0.3164, OBJ: 0.2600, TOTAL: 0.3051\n",
      "CE: 0.1966, OBJ: 0.3644, TOTAL: 0.2302\n",
      "CE: 0.3299, OBJ: 0.3541, TOTAL: 0.3347\n",
      "CE: 0.4121, OBJ: 0.4820, TOTAL: 0.4261\n",
      "CE: 0.2653, OBJ: 0.3782, TOTAL: 0.2879\n",
      "CE: 0.2460, OBJ: 0.2960, TOTAL: 0.2560\n",
      "CE: 0.1259, OBJ: 0.4027, TOTAL: 0.1813\n",
      "CE: 0.3145, OBJ: 0.3532, TOTAL: 0.3222\n",
      "CE: 0.2783, OBJ: 0.1660, TOTAL: 0.2558\n",
      "CE: 0.1425, OBJ: 0.2730, TOTAL: 0.1686\n",
      "CE: 0.3093, OBJ: 0.3866, TOTAL: 0.3247\n",
      "CE: 0.1755, OBJ: 0.3680, TOTAL: 0.2140\n",
      "CE: 0.4440, OBJ: 0.2616, TOTAL: 0.4076\n",
      "CE: 0.1595, OBJ: 0.2202, TOTAL: 0.1716\n",
      "CE: 0.4983, OBJ: 0.3335, TOTAL: 0.4654\n",
      "CE: 0.1409, OBJ: 0.2679, TOTAL: 0.1663\n",
      "CE: 0.2061, OBJ: 0.2657, TOTAL: 0.2180\n",
      "CE: 0.6431, OBJ: 0.3852, TOTAL: 0.5915\n",
      "CE: 0.3098, OBJ: 0.2471, TOTAL: 0.2973\n",
      "CE: 0.6942, OBJ: 0.4438, TOTAL: 0.6441\n",
      "CE: 0.3246, OBJ: 0.4138, TOTAL: 0.3424\n",
      "CE: 0.4543, OBJ: 0.3666, TOTAL: 0.4368\n",
      "CE: 0.3883, OBJ: 0.3659, TOTAL: 0.3838\n",
      "CE: 0.3508, OBJ: 0.3732, TOTAL: 0.3553\n",
      "CE: 0.2115, OBJ: 0.3049, TOTAL: 0.2302\n",
      "CE: 0.1399, OBJ: 0.4195, TOTAL: 0.1958\n",
      "CE: 0.1948, OBJ: 0.2796, TOTAL: 0.2117\n",
      "CE: 0.1748, OBJ: 0.3180, TOTAL: 0.2034\n",
      "CE: 0.4331, OBJ: 0.3510, TOTAL: 0.4167\n",
      "CE: 0.2611, OBJ: 0.3346, TOTAL: 0.2758\n",
      "CE: 0.0398, OBJ: 0.2488, TOTAL: 0.0816\n",
      "CE: 0.3884, OBJ: 0.3793, TOTAL: 0.3866\n",
      "CE: 0.2912, OBJ: 0.3323, TOTAL: 0.2995\n",
      "CE: 0.4655, OBJ: 0.2148, TOTAL: 0.4154\n",
      "CE: 0.1815, OBJ: 0.3861, TOTAL: 0.2224\n",
      "CE: 0.3116, OBJ: 0.2963, TOTAL: 0.3085\n",
      "CE: 0.5079, OBJ: 0.3525, TOTAL: 0.4769\n",
      "CE: 0.2216, OBJ: 0.3494, TOTAL: 0.2472\n",
      "CE: 0.3063, OBJ: 0.4070, TOTAL: 0.3265\n",
      "CE: 0.5472, OBJ: 0.3118, TOTAL: 0.5001\n",
      "CE: 0.4089, OBJ: 0.2367, TOTAL: 0.3744\n",
      "CE: 0.4097, OBJ: 0.3078, TOTAL: 0.3893\n",
      "CE: 0.2848, OBJ: 0.1975, TOTAL: 0.2673\n",
      "CE: 0.3429, OBJ: 0.3126, TOTAL: 0.3368\n",
      "CE: 0.2022, OBJ: 0.3474, TOTAL: 0.2313\n",
      "CE: 0.3123, OBJ: 0.1641, TOTAL: 0.2826\n",
      "CE: 0.1115, OBJ: 0.2370, TOTAL: 0.1366\n",
      "CE: 0.5296, OBJ: 0.3007, TOTAL: 0.4838\n",
      "CE: 0.1182, OBJ: 0.1747, TOTAL: 0.1295\n",
      "CE: 0.2957, OBJ: 0.3453, TOTAL: 0.3056\n",
      "CE: 0.2573, OBJ: 0.3169, TOTAL: 0.2692\n",
      "CE: 0.3626, OBJ: 0.2575, TOTAL: 0.3416\n",
      "CE: 0.3275, OBJ: 0.1953, TOTAL: 0.3011\n",
      "CE: 0.2602, OBJ: 0.3789, TOTAL: 0.2839\n",
      "CE: 0.1699, OBJ: 0.3160, TOTAL: 0.1991\n",
      "CE: 0.1991, OBJ: 0.3327, TOTAL: 0.2258\n",
      "CE: 0.2702, OBJ: 0.3228, TOTAL: 0.2807\n",
      "CE: 0.1289, OBJ: 0.2812, TOTAL: 0.1594\n",
      "CE: 0.1624, OBJ: 0.2201, TOTAL: 0.1739\n",
      "CE: 0.3206, OBJ: 0.3629, TOTAL: 0.3290\n",
      "CE: 0.2949, OBJ: 0.4167, TOTAL: 0.3193\n",
      "CE: 0.3674, OBJ: 0.3773, TOTAL: 0.3694\n",
      "CE: 0.2121, OBJ: 0.2749, TOTAL: 0.2246\n",
      "CE: 0.5719, OBJ: 0.2057, TOTAL: 0.4987\n",
      "CE: 0.2664, OBJ: 0.2438, TOTAL: 0.2619\n",
      "CE: 0.2196, OBJ: 0.2890, TOTAL: 0.2335\n",
      "CE: 0.2740, OBJ: 0.3460, TOTAL: 0.2884\n",
      "CE: 0.5267, OBJ: 0.4798, TOTAL: 0.5173\n",
      "CE: 0.2752, OBJ: 0.2985, TOTAL: 0.2799\n",
      "CE: 0.1876, OBJ: 0.2952, TOTAL: 0.2091\n",
      "CE: 0.3330, OBJ: 0.3678, TOTAL: 0.3400\n",
      "CE: 0.2051, OBJ: 0.3191, TOTAL: 0.2279\n",
      "CE: 0.2657, OBJ: 0.3283, TOTAL: 0.2782\n",
      "CE: 0.2935, OBJ: 0.3546, TOTAL: 0.3057\n",
      "CE: 0.2146, OBJ: 0.3825, TOTAL: 0.2482\n",
      "CE: 0.3840, OBJ: 0.3731, TOTAL: 0.3818\n",
      "CE: 0.0600, OBJ: 0.1712, TOTAL: 0.0823\n",
      "CE: 0.1402, OBJ: 0.2460, TOTAL: 0.1613\n",
      "CE: 0.4285, OBJ: 0.4237, TOTAL: 0.4276\n",
      "CE: 0.6543, OBJ: 0.2468, TOTAL: 0.5728\n",
      "CE: 0.3350, OBJ: 0.2891, TOTAL: 0.3258\n",
      "CE: 0.2168, OBJ: 0.2308, TOTAL: 0.2196\n",
      "CE: 0.4085, OBJ: 0.3594, TOTAL: 0.3987\n",
      "CE: 0.3360, OBJ: 0.3582, TOTAL: 0.3405\n",
      "CE: 0.3858, OBJ: 0.2446, TOTAL: 0.3575\n",
      "CE: 0.2826, OBJ: 0.3188, TOTAL: 0.2898\n",
      "CE: 0.2488, OBJ: 0.3011, TOTAL: 0.2593\n",
      "CE: 0.4888, OBJ: 0.3378, TOTAL: 0.4586\n",
      "CE: 0.2397, OBJ: 0.3159, TOTAL: 0.2549\n",
      "CE: 0.2055, OBJ: 0.4199, TOTAL: 0.2484\n",
      "CE: 0.4745, OBJ: 0.4744, TOTAL: 0.4745\n",
      "CE: 0.2346, OBJ: 0.5191, TOTAL: 0.2915\n",
      "CE: 0.2128, OBJ: 0.2078, TOTAL: 0.2118\n",
      "CE: 0.4069, OBJ: 0.3705, TOTAL: 0.3996\n",
      "CE: 0.2869, OBJ: 0.3400, TOTAL: 0.2975\n",
      "CE: 0.2525, OBJ: 0.5516, TOTAL: 0.3123\n",
      "CE: 0.4458, OBJ: 0.3585, TOTAL: 0.4283\n",
      "CE: 0.4993, OBJ: 0.2741, TOTAL: 0.4543\n",
      "CE: 0.8791, OBJ: 0.3267, TOTAL: 0.7686\n",
      "CE: 0.1603, OBJ: 0.3694, TOTAL: 0.2021\n",
      "CE: 0.1321, OBJ: 0.1982, TOTAL: 0.1453\n",
      "CE: 0.1258, OBJ: 0.3583, TOTAL: 0.1723\n",
      "CE: 0.1568, OBJ: 0.1951, TOTAL: 0.1645\n",
      "CE: 0.3882, OBJ: 0.2593, TOTAL: 0.3624\n",
      "CE: 0.0736, OBJ: 0.2214, TOTAL: 0.1031\n",
      "CE: 0.3163, OBJ: 0.3503, TOTAL: 0.3231\n",
      "CE: 0.2728, OBJ: 0.3658, TOTAL: 0.2914\n",
      "CE: 0.2020, OBJ: 0.2611, TOTAL: 0.2138\n",
      "CE: 0.3148, OBJ: 0.3145, TOTAL: 0.3147\n",
      "CE: 0.1881, OBJ: 0.2305, TOTAL: 0.1966\n",
      "CE: 0.2487, OBJ: 0.1980, TOTAL: 0.2386\n",
      "CE: 0.3422, OBJ: 0.4157, TOTAL: 0.3569\n",
      "CE: 0.3115, OBJ: 0.2776, TOTAL: 0.3047\n",
      "CE: 0.1258, OBJ: 0.2195, TOTAL: 0.1446\n",
      "CE: 0.1918, OBJ: 0.2915, TOTAL: 0.2117\n",
      "CE: 0.3446, OBJ: 0.2553, TOTAL: 0.3267\n",
      "CE: 0.6346, OBJ: 0.2950, TOTAL: 0.5667\n",
      "CE: 0.2436, OBJ: 0.3646, TOTAL: 0.2678\n",
      "CE: 0.1883, OBJ: 0.2736, TOTAL: 0.2054\n",
      "CE: 0.2215, OBJ: 0.3928, TOTAL: 0.2558\n",
      "CE: 0.6182, OBJ: 0.3674, TOTAL: 0.5681\n",
      "CE: 0.2741, OBJ: 0.3300, TOTAL: 0.2853\n",
      "CE: 0.4147, OBJ: 0.3414, TOTAL: 0.4000\n",
      "CE: 0.4616, OBJ: 0.3067, TOTAL: 0.4306\n",
      "CE: 0.4011, OBJ: 0.2241, TOTAL: 0.3657\n",
      "CE: 0.4054, OBJ: 0.2768, TOTAL: 0.3796\n",
      "CE: 0.2119, OBJ: 0.2246, TOTAL: 0.2145\n",
      "CE: 0.3287, OBJ: 0.2391, TOTAL: 0.3108\n",
      "CE: 0.3153, OBJ: 0.2767, TOTAL: 0.3076\n",
      "CE: 0.2984, OBJ: 0.3491, TOTAL: 0.3085\n",
      "CE: 0.4030, OBJ: 0.3950, TOTAL: 0.4014\n",
      "CE: 0.4916, OBJ: 0.2204, TOTAL: 0.4374\n",
      "CE: 0.0769, OBJ: 0.4039, TOTAL: 0.1423\n",
      "CE: 0.3618, OBJ: 0.2174, TOTAL: 0.3329\n",
      "CE: 0.1702, OBJ: 0.4649, TOTAL: 0.2291\n",
      "CE: 0.3665, OBJ: 0.3634, TOTAL: 0.3659\n",
      "CE: 0.4910, OBJ: 0.4777, TOTAL: 0.4883\n",
      "CE: 0.1652, OBJ: 0.3114, TOTAL: 0.1945\n",
      "CE: 0.4239, OBJ: 0.2548, TOTAL: 0.3901\n",
      "CE: 0.3748, OBJ: 0.3547, TOTAL: 0.3708\n",
      "CE: 0.4207, OBJ: 0.3420, TOTAL: 0.4049\n",
      "CE: 0.1800, OBJ: 0.2938, TOTAL: 0.2028\n",
      "CE: 0.4927, OBJ: 0.2521, TOTAL: 0.4446\n",
      "CE: 0.2902, OBJ: 0.3484, TOTAL: 0.3019\n",
      "CE: 0.1876, OBJ: 0.4785, TOTAL: 0.2458\n",
      "CE: 0.1041, OBJ: 0.1752, TOTAL: 0.1183\n",
      "CE: 0.2338, OBJ: 0.3607, TOTAL: 0.2592\n",
      "CE: 0.3768, OBJ: 0.3669, TOTAL: 0.3748\n",
      "CE: 0.3615, OBJ: 0.4059, TOTAL: 0.3704\n",
      "CE: 0.2976, OBJ: 0.2764, TOTAL: 0.2934\n",
      "CE: 0.2625, OBJ: 0.3235, TOTAL: 0.2747\n",
      "CE: 0.0648, OBJ: 0.2517, TOTAL: 0.1022\n",
      "CE: 0.1018, OBJ: 0.4362, TOTAL: 0.1687\n",
      "CE: 0.3347, OBJ: 0.3282, TOTAL: 0.3334\n",
      "CE: 0.4151, OBJ: 0.4352, TOTAL: 0.4191\n",
      "CE: 0.0764, OBJ: 0.3941, TOTAL: 0.1399\n",
      "CE: 0.1489, OBJ: 0.3447, TOTAL: 0.1880\n",
      "CE: 0.1350, OBJ: 0.4006, TOTAL: 0.1881\n",
      "CE: 0.1209, OBJ: 0.4882, TOTAL: 0.1944\n",
      "CE: 0.0564, OBJ: 0.3087, TOTAL: 0.1069\n",
      "CE: 0.2180, OBJ: 0.3591, TOTAL: 0.2462\n",
      "CE: 0.3050, OBJ: 0.3232, TOTAL: 0.3087\n",
      "CE: 0.3277, OBJ: 0.2356, TOTAL: 0.3093\n",
      "CE: 0.2774, OBJ: 0.2743, TOTAL: 0.2768\n",
      "CE: 0.4152, OBJ: 0.3366, TOTAL: 0.3995\n",
      "CE: 0.2493, OBJ: 0.3453, TOTAL: 0.2685\n",
      "CE: 0.2328, OBJ: 0.1899, TOTAL: 0.2242\n",
      "CE: 0.6059, OBJ: 0.1943, TOTAL: 0.5236\n",
      "CE: 0.4880, OBJ: 0.4370, TOTAL: 0.4778\n",
      "CE: 0.0416, OBJ: 0.2656, TOTAL: 0.0864\n",
      "CE: 0.4151, OBJ: 0.3375, TOTAL: 0.3996\n",
      "CE: 0.2214, OBJ: 0.2989, TOTAL: 0.2369\n",
      "CE: 0.2380, OBJ: 0.2152, TOTAL: 0.2334\n",
      "CE: 0.2489, OBJ: 0.3502, TOTAL: 0.2692\n",
      "CE: 0.1131, OBJ: 0.2940, TOTAL: 0.1493\n",
      "CE: 0.6854, OBJ: 0.3985, TOTAL: 0.6280\n",
      "CE: 0.4048, OBJ: 0.3220, TOTAL: 0.3883\n",
      "CE: 0.4607, OBJ: 0.3017, TOTAL: 0.4289\n",
      "CE: 0.4791, OBJ: 0.3582, TOTAL: 0.4549\n",
      "CE: 0.1818, OBJ: 0.3213, TOTAL: 0.2097\n",
      "CE: 0.1872, OBJ: 0.2010, TOTAL: 0.1900\n",
      "CE: 0.7677, OBJ: 0.3395, TOTAL: 0.6820\n",
      "CE: 0.1162, OBJ: 0.2446, TOTAL: 0.1419\n",
      "CE: 0.4676, OBJ: 0.3446, TOTAL: 0.4430\n",
      "CE: 0.1411, OBJ: 0.3324, TOTAL: 0.1793\n",
      "CE: 0.2503, OBJ: 0.3953, TOTAL: 0.2793\n",
      "CE: 0.3794, OBJ: 0.3504, TOTAL: 0.3736\n",
      "CE: 0.3796, OBJ: 0.2983, TOTAL: 0.3634\n",
      "CE: 0.2506, OBJ: 0.2785, TOTAL: 0.2562\n",
      "CE: 0.2690, OBJ: 0.3688, TOTAL: 0.2889\n",
      "CE: 0.3937, OBJ: 0.4007, TOTAL: 0.3951\n",
      "CE: 0.5368, OBJ: 0.2636, TOTAL: 0.4822\n",
      "CE: 0.1830, OBJ: 0.3529, TOTAL: 0.2170\n",
      "CE: 0.4693, OBJ: 0.3083, TOTAL: 0.4371\n",
      "CE: 0.5172, OBJ: 0.3891, TOTAL: 0.4916\n",
      "CE: 0.5706, OBJ: 0.2662, TOTAL: 0.5097\n",
      "CE: 0.2272, OBJ: 0.2855, TOTAL: 0.2389\n",
      "CE: 0.1457, OBJ: 0.5096, TOTAL: 0.2184\n",
      "CE: 0.2036, OBJ: 0.4246, TOTAL: 0.2478\n",
      "CE: 0.2336, OBJ: 0.2865, TOTAL: 0.2442\n",
      "CE: 0.1901, OBJ: 0.2686, TOTAL: 0.2058\n",
      "CE: 0.4897, OBJ: 0.3960, TOTAL: 0.4710\n",
      "CE: 0.8158, OBJ: 0.4326, TOTAL: 0.7391\n",
      "CE: 0.7599, OBJ: 0.4341, TOTAL: 0.6948\n",
      "CE: 0.2365, OBJ: 0.2265, TOTAL: 0.2345\n",
      "CE: 0.4649, OBJ: 0.3905, TOTAL: 0.4500\n",
      "CE: 0.5002, OBJ: 0.3810, TOTAL: 0.4764\n",
      "CE: 0.1378, OBJ: 0.1795, TOTAL: 0.1461\n",
      "CE: 0.0500, OBJ: 0.2457, TOTAL: 0.0891\n",
      "CE: 0.1969, OBJ: 0.4016, TOTAL: 0.2379\n",
      "CE: 0.6337, OBJ: 0.2302, TOTAL: 0.5530\n",
      "CE: 0.1017, OBJ: 0.3551, TOTAL: 0.1524\n",
      "CE: 0.2924, OBJ: 0.3333, TOTAL: 0.3006\n",
      "CE: 0.1796, OBJ: 0.3576, TOTAL: 0.2152\n",
      "CE: 0.4879, OBJ: 0.2874, TOTAL: 0.4478\n",
      "CE: 0.1168, OBJ: 0.3704, TOTAL: 0.1676\n",
      "CE: 0.4907, OBJ: 0.2957, TOTAL: 0.4517\n",
      "CE: 0.5986, OBJ: 0.3168, TOTAL: 0.5422\n",
      "CE: 0.3148, OBJ: 0.2864, TOTAL: 0.3091\n",
      "CE: 0.3748, OBJ: 0.3998, TOTAL: 0.3798\n",
      "CE: 0.3433, OBJ: 0.3146, TOTAL: 0.3376\n",
      "CE: 0.4627, OBJ: 0.2979, TOTAL: 0.4298\n",
      "CE: 0.4301, OBJ: 0.3160, TOTAL: 0.4073\n",
      "CE: 0.7428, OBJ: 0.3483, TOTAL: 0.6639\n",
      "CE: 0.6182, OBJ: 0.3939, TOTAL: 0.5734\n",
      "CE: 0.1222, OBJ: 0.2984, TOTAL: 0.1574\n",
      "CE: 0.2453, OBJ: 0.3308, TOTAL: 0.2624\n",
      "CE: 0.4466, OBJ: 0.3480, TOTAL: 0.4269\n",
      "CE: 0.3987, OBJ: 0.3815, TOTAL: 0.3952\n",
      "CE: 0.9722, OBJ: 0.3457, TOTAL: 0.8469\n",
      "CE: 0.2451, OBJ: 0.3133, TOTAL: 0.2587\n",
      "CE: 0.1189, OBJ: 0.2428, TOTAL: 0.1436\n",
      "CE: 0.4217, OBJ: 0.4394, TOTAL: 0.4253\n",
      "CE: 0.3358, OBJ: 0.5020, TOTAL: 0.3690\n",
      "CE: 0.4916, OBJ: 0.3553, TOTAL: 0.4643\n",
      "CE: 0.4420, OBJ: 0.2025, TOTAL: 0.3941\n",
      "CE: 0.3087, OBJ: 0.3265, TOTAL: 0.3123\n",
      "CE: 0.2783, OBJ: 0.2505, TOTAL: 0.2727\n",
      "CE: 0.2761, OBJ: 0.2501, TOTAL: 0.2709\n",
      "CE: 0.2536, OBJ: 0.3264, TOTAL: 0.2682\n",
      "CE: 0.0809, OBJ: 0.2144, TOTAL: 0.1076\n",
      "CE: 0.4645, OBJ: 0.4276, TOTAL: 0.4571\n",
      "CE: 0.6328, OBJ: 0.4507, TOTAL: 0.5964\n",
      "CE: 0.6300, OBJ: 0.4784, TOTAL: 0.5997\n",
      "CE: 0.0947, OBJ: 0.3411, TOTAL: 0.1440\n",
      "CE: 0.2663, OBJ: 0.4895, TOTAL: 0.3110\n",
      "CE: 0.3563, OBJ: 0.3349, TOTAL: 0.3520\n",
      "CE: 0.3734, OBJ: 0.4370, TOTAL: 0.3861\n",
      "CE: 0.1508, OBJ: 0.2759, TOTAL: 0.1758\n",
      "CE: 0.4368, OBJ: 0.2500, TOTAL: 0.3994\n",
      "CE: 0.1265, OBJ: 0.2400, TOTAL: 0.1492\n",
      "CE: 0.3201, OBJ: 0.2799, TOTAL: 0.3121\n",
      "CE: 0.2887, OBJ: 0.3263, TOTAL: 0.2962\n",
      "CE: 0.2432, OBJ: 0.2486, TOTAL: 0.2443\n",
      "CE: 0.5538, OBJ: 0.3763, TOTAL: 0.5183\n",
      "CE: 0.3569, OBJ: 0.3870, TOTAL: 0.3629\n",
      "CE: 0.2565, OBJ: 0.3137, TOTAL: 0.2679\n",
      "CE: 0.1315, OBJ: 0.3418, TOTAL: 0.1736\n",
      "CE: 0.2678, OBJ: 0.3272, TOTAL: 0.2797\n",
      "CE: 0.2529, OBJ: 0.2374, TOTAL: 0.2498\n",
      "CE: 0.1310, OBJ: 0.3677, TOTAL: 0.1784\n",
      "CE: 0.3214, OBJ: 0.3672, TOTAL: 0.3306\n",
      "CE: 0.1978, OBJ: 0.3602, TOTAL: 0.2303\n",
      "CE: 0.0948, OBJ: 0.2124, TOTAL: 0.1183\n",
      "CE: 0.2990, OBJ: 0.3374, TOTAL: 0.3067\n",
      "CE: 0.9501, OBJ: 0.3631, TOTAL: 0.8327\n",
      "CE: 0.3077, OBJ: 0.3212, TOTAL: 0.3104\n",
      "CE: 0.4162, OBJ: 0.3504, TOTAL: 0.4031\n",
      "CE: 0.2734, OBJ: 0.1471, TOTAL: 0.2482\n",
      "CE: 0.2112, OBJ: 0.3933, TOTAL: 0.2476\n",
      "CE: 0.2430, OBJ: 0.1904, TOTAL: 0.2325\n",
      "CE: 0.5821, OBJ: 0.3311, TOTAL: 0.5319\n",
      "CE: 0.3708, OBJ: 0.2452, TOTAL: 0.3457\n",
      "CE: 0.1916, OBJ: 0.3992, TOTAL: 0.2331\n",
      "CE: 0.5468, OBJ: 0.1947, TOTAL: 0.4763\n",
      "CE: 0.1436, OBJ: 0.4664, TOTAL: 0.2082\n",
      "CE: 0.5695, OBJ: 0.2885, TOTAL: 0.5133\n",
      "CE: 0.2295, OBJ: 0.2271, TOTAL: 0.2290\n",
      "CE: 0.3511, OBJ: 0.3087, TOTAL: 0.3426\n",
      "CE: 0.5782, OBJ: 0.4460, TOTAL: 0.5517\n",
      "CE: 0.2022, OBJ: 0.4184, TOTAL: 0.2454\n",
      "CE: 0.1672, OBJ: 0.2681, TOTAL: 0.1874\n",
      "CE: 0.3621, OBJ: 0.2131, TOTAL: 0.3323\n",
      "CE: 0.1291, OBJ: 0.1977, TOTAL: 0.1428\n",
      "CE: 0.2752, OBJ: 0.2403, TOTAL: 0.2682\n",
      "CE: 0.3725, OBJ: 0.4012, TOTAL: 0.3782\n",
      "CE: 0.2566, OBJ: 0.3812, TOTAL: 0.2815\n",
      "CE: 0.2006, OBJ: 0.2774, TOTAL: 0.2159\n",
      "CE: 0.2828, OBJ: 0.3883, TOTAL: 0.3039\n",
      "CE: 0.0673, OBJ: 0.2492, TOTAL: 0.1037\n",
      "CE: 0.2836, OBJ: 0.3436, TOTAL: 0.2956\n",
      "CE: 0.3642, OBJ: 0.4146, TOTAL: 0.3743\n",
      "CE: 0.4054, OBJ: 0.2698, TOTAL: 0.3783\n",
      "CE: 0.1467, OBJ: 0.3367, TOTAL: 0.1847\n",
      "CE: 0.2806, OBJ: 0.2414, TOTAL: 0.2727\n",
      "CE: 0.1514, OBJ: 0.2903, TOTAL: 0.1792\n",
      "CE: 0.4394, OBJ: 0.3231, TOTAL: 0.4162\n",
      "CE: 0.1900, OBJ: 0.3074, TOTAL: 0.2135\n",
      "CE: 0.3012, OBJ: 0.2028, TOTAL: 0.2815\n",
      "CE: 0.2255, OBJ: 0.2606, TOTAL: 0.2325\n",
      "CE: 0.4249, OBJ: 0.1968, TOTAL: 0.3793\n",
      "CE: 0.2712, OBJ: 0.3966, TOTAL: 0.2963\n",
      "CE: 0.2674, OBJ: 0.3010, TOTAL: 0.2741\n",
      "CE: 0.4642, OBJ: 0.2791, TOTAL: 0.4272\n",
      "CE: 0.2894, OBJ: 0.4465, TOTAL: 0.3208\n",
      "CE: 0.1632, OBJ: 0.2817, TOTAL: 0.1869\n",
      "CE: 0.3270, OBJ: 0.3215, TOTAL: 0.3259\n",
      "CE: 0.4528, OBJ: 0.2700, TOTAL: 0.4162\n",
      "CE: 0.2246, OBJ: 0.2771, TOTAL: 0.2351\n",
      "CE: 0.1703, OBJ: 0.3075, TOTAL: 0.1978\n",
      "CE: 0.0602, OBJ: 0.2900, TOTAL: 0.1061\n",
      "CE: 0.2668, OBJ: 0.3696, TOTAL: 0.2874\n",
      "CE: 0.4506, OBJ: 0.3126, TOTAL: 0.4230\n",
      "CE: 0.4820, OBJ: 0.2558, TOTAL: 0.4368\n",
      "CE: 0.4823, OBJ: 0.3289, TOTAL: 0.4516\n",
      "CE: 0.1149, OBJ: 0.4056, TOTAL: 0.1730\n",
      "CE: 0.2058, OBJ: 0.2206, TOTAL: 0.2088\n",
      "CE: 0.1595, OBJ: 0.4279, TOTAL: 0.2132\n",
      "CE: 0.8120, OBJ: 0.5322, TOTAL: 0.7561\n",
      "CE: 0.2672, OBJ: 0.3835, TOTAL: 0.2905\n",
      "CE: 0.3090, OBJ: 0.4223, TOTAL: 0.3317\n",
      "CE: 0.2185, OBJ: 0.2415, TOTAL: 0.2231\n",
      "CE: 0.1858, OBJ: 0.3726, TOTAL: 0.2232\n",
      "CE: 0.1923, OBJ: 0.3172, TOTAL: 0.2172\n",
      "CE: 0.3810, OBJ: 0.2784, TOTAL: 0.3604\n",
      "CE: 0.0527, OBJ: 0.2800, TOTAL: 0.0982\n",
      "CE: 0.1559, OBJ: 0.3601, TOTAL: 0.1967\n",
      "CE: 0.9027, OBJ: 0.2264, TOTAL: 0.7674\n",
      "CE: 0.1361, OBJ: 0.2162, TOTAL: 0.1521\n",
      "CE: 0.6759, OBJ: 0.3388, TOTAL: 0.6085\n",
      "CE: 0.1182, OBJ: 0.3179, TOTAL: 0.1582\n",
      "CE: 0.4577, OBJ: 0.3576, TOTAL: 0.4377\n",
      "CE: 0.7911, OBJ: 0.2938, TOTAL: 0.6917\n",
      "CE: 0.3105, OBJ: 0.2694, TOTAL: 0.3023\n",
      "CE: 0.0788, OBJ: 0.4422, TOTAL: 0.1515\n",
      "CE: 0.1821, OBJ: 0.2745, TOTAL: 0.2006\n",
      "CE: 0.5173, OBJ: 0.4805, TOTAL: 0.5099\n",
      "CE: 0.4031, OBJ: 0.2602, TOTAL: 0.3745\n",
      "CE: 0.4525, OBJ: 0.3061, TOTAL: 0.4232\n",
      "CE: 0.1082, OBJ: 0.3325, TOTAL: 0.1531\n",
      "CE: 0.2554, OBJ: 0.3616, TOTAL: 0.2766\n",
      "CE: 0.5761, OBJ: 0.3052, TOTAL: 0.5220\n",
      "CE: 0.5765, OBJ: 0.2345, TOTAL: 0.5081\n",
      "CE: 0.1516, OBJ: 0.3966, TOTAL: 0.2006\n",
      "CE: 0.3809, OBJ: 0.3511, TOTAL: 0.3749\n",
      "CE: 0.3580, OBJ: 0.4116, TOTAL: 0.3687\n",
      "CE: 0.4714, OBJ: 0.2200, TOTAL: 0.4211\n",
      "CE: 0.1740, OBJ: 0.2982, TOTAL: 0.1989\n",
      "CE: 0.4027, OBJ: 0.6123, TOTAL: 0.4447\n",
      "CE: 0.4703, OBJ: 0.4059, TOTAL: 0.4574\n",
      "CE: 0.2675, OBJ: 0.2495, TOTAL: 0.2639\n",
      "CE: 0.2552, OBJ: 0.2418, TOTAL: 0.2525\n",
      "CE: 0.1753, OBJ: 0.3324, TOTAL: 0.2067\n",
      "CE: 0.3831, OBJ: 0.3529, TOTAL: 0.3771\n",
      "CE: 0.2605, OBJ: 0.2725, TOTAL: 0.2629\n",
      "CE: 0.4124, OBJ: 0.3527, TOTAL: 0.4005\n",
      "CE: 0.4663, OBJ: 0.3498, TOTAL: 0.4430\n",
      "CE: 0.4645, OBJ: 0.3496, TOTAL: 0.4415\n",
      "CE: 0.1413, OBJ: 0.3815, TOTAL: 0.1893\n",
      "CE: 0.7338, OBJ: 0.2641, TOTAL: 0.6398\n",
      "CE: 0.3143, OBJ: 0.4045, TOTAL: 0.3323\n",
      "CE: 0.3849, OBJ: 0.2914, TOTAL: 0.3662\n",
      "CE: 0.6698, OBJ: 0.2995, TOTAL: 0.5957\n",
      "CE: 0.4014, OBJ: 0.3711, TOTAL: 0.3954\n",
      "CE: 0.1316, OBJ: 0.2344, TOTAL: 0.1522\n",
      "CE: 0.3488, OBJ: 0.2992, TOTAL: 0.3389\n",
      "CE: 0.1344, OBJ: 0.2732, TOTAL: 0.1622\n",
      "CE: 0.5443, OBJ: 0.3620, TOTAL: 0.5078\n",
      "CE: 0.1028, OBJ: 0.2917, TOTAL: 0.1406\n",
      "CE: 0.3313, OBJ: 0.2942, TOTAL: 0.3239\n",
      "CE: 0.3126, OBJ: 0.4527, TOTAL: 0.3407\n",
      "CE: 0.2303, OBJ: 0.4581, TOTAL: 0.2759\n",
      "CE: 0.2801, OBJ: 0.2459, TOTAL: 0.2733\n",
      "CE: 0.4082, OBJ: 0.3506, TOTAL: 0.3967\n",
      "CE: 0.1428, OBJ: 0.2541, TOTAL: 0.1650\n",
      "CE: 0.8934, OBJ: 0.4379, TOTAL: 0.8023\n",
      "CE: 0.4338, OBJ: 0.2957, TOTAL: 0.4062\n",
      "CE: 0.1745, OBJ: 0.3507, TOTAL: 0.2097\n",
      "CE: 0.5001, OBJ: 0.3435, TOTAL: 0.4688\n",
      "CE: 0.3115, OBJ: 0.2354, TOTAL: 0.2963\n",
      "CE: 0.1568, OBJ: 0.2304, TOTAL: 0.1715\n",
      "CE: 0.6520, OBJ: 0.4199, TOTAL: 0.6056\n",
      "CE: 0.2938, OBJ: 0.3532, TOTAL: 0.3057\n",
      "CE: 0.1336, OBJ: 0.1956, TOTAL: 0.1460\n",
      "CE: 0.1565, OBJ: 0.2867, TOTAL: 0.1826\n",
      "CE: 0.1514, OBJ: 0.2670, TOTAL: 0.1745\n",
      "CE: 0.2177, OBJ: 0.3579, TOTAL: 0.2457\n",
      "CE: 0.4940, OBJ: 0.3449, TOTAL: 0.4642\n",
      "CE: 0.3723, OBJ: 0.3606, TOTAL: 0.3699\n",
      "CE: 0.3102, OBJ: 0.1957, TOTAL: 0.2873\n",
      "CE: 0.3014, OBJ: 0.3708, TOTAL: 0.3153\n",
      "CE: 0.1664, OBJ: 0.4156, TOTAL: 0.2162\n",
      "CE: 0.2286, OBJ: 0.2759, TOTAL: 0.2381\n",
      "CE: 0.3139, OBJ: 0.2832, TOTAL: 0.3078\n",
      "CE: 0.2280, OBJ: 0.2534, TOTAL: 0.2331\n",
      "CE: 0.2307, OBJ: 0.3183, TOTAL: 0.2482\n",
      "CE: 0.1291, OBJ: 0.2060, TOTAL: 0.1445\n",
      "CE: 0.3437, OBJ: 0.2846, TOTAL: 0.3319\n",
      "CE: 0.4568, OBJ: 0.3181, TOTAL: 0.4291\n",
      "CE: 0.5428, OBJ: 0.4861, TOTAL: 0.5315\n",
      "CE: 0.2639, OBJ: 0.2864, TOTAL: 0.2684\n",
      "CE: 0.1451, OBJ: 0.4049, TOTAL: 0.1971\n",
      "CE: 0.3143, OBJ: 0.2852, TOTAL: 0.3085\n",
      "CE: 0.2090, OBJ: 0.2415, TOTAL: 0.2155\n",
      "CE: 0.2819, OBJ: 0.3157, TOTAL: 0.2887\n",
      "CE: 0.2166, OBJ: 0.2134, TOTAL: 0.2159\n",
      "CE: 0.3451, OBJ: 0.4029, TOTAL: 0.3566\n",
      "CE: 0.3685, OBJ: 0.2030, TOTAL: 0.3354\n",
      "CE: 0.3747, OBJ: 0.3220, TOTAL: 0.3642\n",
      "CE: 0.5926, OBJ: 0.2866, TOTAL: 0.5314\n",
      "CE: 0.2153, OBJ: 0.3784, TOTAL: 0.2479\n",
      "CE: 0.5204, OBJ: 0.3947, TOTAL: 0.4952\n",
      "CE: 0.4983, OBJ: 0.1215, TOTAL: 0.4229\n",
      "CE: 0.3617, OBJ: 0.4412, TOTAL: 0.3776\n",
      "CE: 0.4201, OBJ: 0.4700, TOTAL: 0.4301\n",
      "CE: 0.2728, OBJ: 0.4178, TOTAL: 0.3018\n",
      "CE: 0.4111, OBJ: 0.2909, TOTAL: 0.3870\n",
      "CE: 0.2318, OBJ: 0.2669, TOTAL: 0.2388\n",
      "CE: 0.4325, OBJ: 0.4524, TOTAL: 0.4365\n",
      "CE: 0.2198, OBJ: 0.2990, TOTAL: 0.2357\n",
      "CE: 0.1400, OBJ: 0.3813, TOTAL: 0.1883\n",
      "CE: 0.3954, OBJ: 0.3070, TOTAL: 0.3778\n",
      "CE: 0.2670, OBJ: 0.3100, TOTAL: 0.2756\n",
      "CE: 0.3518, OBJ: 0.2025, TOTAL: 0.3219\n",
      "CE: 0.3495, OBJ: 0.1906, TOTAL: 0.3177\n",
      "CE: 0.5234, OBJ: 0.3626, TOTAL: 0.4913\n",
      "CE: 0.4948, OBJ: 0.4121, TOTAL: 0.4782\n",
      "CE: 0.2582, OBJ: 0.4147, TOTAL: 0.2895\n",
      "CE: 0.4067, OBJ: 0.2868, TOTAL: 0.3827\n",
      "CE: 0.1663, OBJ: 0.2253, TOTAL: 0.1781\n",
      "CE: 0.5335, OBJ: 0.2840, TOTAL: 0.4836\n",
      "CE: 0.2240, OBJ: 0.3748, TOTAL: 0.2541\n",
      "CE: 0.2942, OBJ: 0.4266, TOTAL: 0.3207\n",
      "CE: 0.2349, OBJ: 0.3118, TOTAL: 0.2503\n",
      "CE: 0.2917, OBJ: 0.2896, TOTAL: 0.2913\n",
      "CE: 0.2614, OBJ: 0.3910, TOTAL: 0.2873\n",
      "CE: 0.3820, OBJ: 0.5240, TOTAL: 0.4104\n",
      "CE: 0.7060, OBJ: 0.4684, TOTAL: 0.6585\n",
      "CE: 0.1785, OBJ: 0.3379, TOTAL: 0.2104\n",
      "CE: 0.3375, OBJ: 0.4045, TOTAL: 0.3509\n",
      "CE: 0.4616, OBJ: 0.3127, TOTAL: 0.4318\n",
      "CE: 0.4336, OBJ: 0.2025, TOTAL: 0.3874\n",
      "CE: 0.2376, OBJ: 0.2661, TOTAL: 0.2433\n",
      "CE: 0.1546, OBJ: 0.4194, TOTAL: 0.2076\n",
      "CE: 0.1241, OBJ: 0.3283, TOTAL: 0.1649\n",
      "CE: 0.6292, OBJ: 0.3033, TOTAL: 0.5641\n",
      "CE: 0.5084, OBJ: 0.2267, TOTAL: 0.4521\n",
      "CE: 0.1369, OBJ: 0.2249, TOTAL: 0.1545\n",
      "CE: 0.1914, OBJ: 0.3865, TOTAL: 0.2304\n",
      "CE: 0.4090, OBJ: 0.4534, TOTAL: 0.4179\n",
      "CE: 0.5535, OBJ: 0.3120, TOTAL: 0.5052\n",
      "CE: 0.2302, OBJ: 0.2155, TOTAL: 0.2273\n",
      "CE: 0.2834, OBJ: 0.2828, TOTAL: 0.2833\n",
      "CE: 0.3938, OBJ: 0.3229, TOTAL: 0.3796\n",
      "CE: 0.5791, OBJ: 0.2883, TOTAL: 0.5210\n",
      "CE: 0.1791, OBJ: 0.3492, TOTAL: 0.2131\n",
      "CE: 0.2459, OBJ: 0.2665, TOTAL: 0.2500\n",
      "CE: 0.1674, OBJ: 0.2772, TOTAL: 0.1894\n",
      "CE: 0.4652, OBJ: 0.1841, TOTAL: 0.4090\n",
      "CE: 0.1825, OBJ: 0.2614, TOTAL: 0.1982\n",
      "CE: 0.1901, OBJ: 0.2304, TOTAL: 0.1982\n",
      "CE: 0.4296, OBJ: 0.1603, TOTAL: 0.3758\n",
      "CE: 0.3191, OBJ: 0.2855, TOTAL: 0.3123\n",
      "CE: 0.3551, OBJ: 0.3257, TOTAL: 0.3492\n",
      "CE: 0.5866, OBJ: 0.3609, TOTAL: 0.5415\n",
      "CE: 0.3986, OBJ: 0.2333, TOTAL: 0.3655\n",
      "CE: 0.2346, OBJ: 0.4396, TOTAL: 0.2756\n",
      "CE: 0.1490, OBJ: 0.3386, TOTAL: 0.1869\n",
      "CE: 0.3133, OBJ: 0.4337, TOTAL: 0.3374\n",
      "CE: 0.2253, OBJ: 0.1354, TOTAL: 0.2073\n",
      "CE: 0.5263, OBJ: 0.3507, TOTAL: 0.4912\n",
      "CE: 0.3438, OBJ: 0.3062, TOTAL: 0.3363\n",
      "CE: 0.1731, OBJ: 0.3776, TOTAL: 0.2140\n",
      "CE: 0.2175, OBJ: 0.2949, TOTAL: 0.2330\n",
      "CE: 0.1045, OBJ: 0.4016, TOTAL: 0.1639\n",
      "CE: 0.2825, OBJ: 0.3558, TOTAL: 0.2972\n",
      "CE: 0.4845, OBJ: 0.3310, TOTAL: 0.4538\n",
      "CE: 0.2644, OBJ: 0.3668, TOTAL: 0.2849\n",
      "CE: 0.1506, OBJ: 0.3704, TOTAL: 0.1946\n",
      "CE: 0.0832, OBJ: 0.2194, TOTAL: 0.1104\n",
      "CE: 0.6527, OBJ: 0.3178, TOTAL: 0.5857\n",
      "CE: 0.4259, OBJ: 0.2167, TOTAL: 0.3841\n",
      "CE: 0.1797, OBJ: 0.3769, TOTAL: 0.2192\n",
      "CE: 0.3755, OBJ: 0.2347, TOTAL: 0.3473\n",
      "CE: 0.3439, OBJ: 0.2662, TOTAL: 0.3284\n",
      "CE: 0.2223, OBJ: 0.2208, TOTAL: 0.2220\n",
      "CE: 0.2407, OBJ: 0.2574, TOTAL: 0.2440\n",
      "CE: 0.2295, OBJ: 0.2335, TOTAL: 0.2303\n",
      "CE: 0.1912, OBJ: 0.2251, TOTAL: 0.1980\n",
      "CE: 0.6048, OBJ: 0.3912, TOTAL: 0.5621\n",
      "CE: 0.1156, OBJ: 0.3062, TOTAL: 0.1537\n",
      "CE: 0.4153, OBJ: 0.2008, TOTAL: 0.3724\n",
      "CE: 0.3413, OBJ: 0.3141, TOTAL: 0.3359\n",
      "CE: 0.2739, OBJ: 0.3899, TOTAL: 0.2971\n",
      "CE: 0.1533, OBJ: 0.4797, TOTAL: 0.2186\n",
      "CE: 0.3991, OBJ: 0.5497, TOTAL: 0.4292\n",
      "CE: 0.2334, OBJ: 0.4574, TOTAL: 0.2782\n",
      "CE: 0.2645, OBJ: 0.2856, TOTAL: 0.2688\n",
      "CE: 0.2323, OBJ: 0.2582, TOTAL: 0.2375\n",
      "CE: 0.2612, OBJ: 0.3606, TOTAL: 0.2811\n",
      "CE: 0.5578, OBJ: 0.3315, TOTAL: 0.5125\n",
      "CE: 0.4891, OBJ: 0.3814, TOTAL: 0.4675\n",
      "CE: 0.7720, OBJ: 0.3657, TOTAL: 0.6907\n",
      "CE: 0.2041, OBJ: 0.2829, TOTAL: 0.2199\n",
      "CE: 0.2929, OBJ: 0.3799, TOTAL: 0.3103\n",
      "CE: 0.1701, OBJ: 0.1764, TOTAL: 0.1713\n",
      "CE: 0.2668, OBJ: 0.2569, TOTAL: 0.2648\n",
      "CE: 0.5018, OBJ: 0.3308, TOTAL: 0.4676\n",
      "CE: 0.5870, OBJ: 0.3273, TOTAL: 0.5351\n",
      "CE: 0.0923, OBJ: 0.2958, TOTAL: 0.1330\n",
      "CE: 0.2646, OBJ: 0.2898, TOTAL: 0.2697\n",
      "CE: 0.2760, OBJ: 0.3118, TOTAL: 0.2832\n",
      "CE: 0.3349, OBJ: 0.3147, TOTAL: 0.3309\n",
      "CE: 0.3900, OBJ: 0.2998, TOTAL: 0.3719\n",
      "CE: 0.1588, OBJ: 0.3377, TOTAL: 0.1946\n",
      "CE: 0.3608, OBJ: 0.2629, TOTAL: 0.3413\n",
      "CE: 0.3948, OBJ: 0.3079, TOTAL: 0.3775\n",
      "CE: 0.2529, OBJ: 0.1931, TOTAL: 0.2410\n",
      "CE: 0.2622, OBJ: 0.2540, TOTAL: 0.2605\n",
      "CE: 0.3432, OBJ: 0.4144, TOTAL: 0.3574\n",
      "CE: 0.1143, OBJ: 0.3785, TOTAL: 0.1672\n",
      "CE: 0.1765, OBJ: 0.4135, TOTAL: 0.2239\n",
      "CE: 0.1910, OBJ: 0.3796, TOTAL: 0.2287\n",
      "CE: 0.6876, OBJ: 0.2977, TOTAL: 0.6096\n",
      "CE: 0.2429, OBJ: 0.3349, TOTAL: 0.2613\n",
      "CE: 0.4104, OBJ: 0.3477, TOTAL: 0.3979\n",
      "CE: 0.2166, OBJ: 0.1546, TOTAL: 0.2042\n",
      "CE: 0.6277, OBJ: 0.3580, TOTAL: 0.5738\n",
      "CE: 0.2165, OBJ: 0.2649, TOTAL: 0.2262\n",
      "CE: 0.8771, OBJ: 0.2503, TOTAL: 0.7518\n",
      "CE: 0.1107, OBJ: 0.2585, TOTAL: 0.1402\n",
      "CE: 0.5376, OBJ: 0.1366, TOTAL: 0.4574\n",
      "CE: 0.2637, OBJ: 0.2012, TOTAL: 0.2512\n",
      "CE: 0.1788, OBJ: 0.4666, TOTAL: 0.2364\n",
      "CE: 0.4468, OBJ: 0.3065, TOTAL: 0.4187\n",
      "CE: 0.3303, OBJ: 0.2274, TOTAL: 0.3097\n",
      "CE: 0.6296, OBJ: 0.3218, TOTAL: 0.5681\n",
      "CE: 0.9700, OBJ: 0.3008, TOTAL: 0.8362\n",
      "CE: 0.5618, OBJ: 0.3941, TOTAL: 0.5283\n",
      "CE: 0.4940, OBJ: 0.4099, TOTAL: 0.4772\n",
      "CE: 0.1150, OBJ: 0.3249, TOTAL: 0.1570\n",
      "CE: 0.4611, OBJ: 0.3713, TOTAL: 0.4431\n",
      "CE: 0.3542, OBJ: 0.2661, TOTAL: 0.3365\n",
      "CE: 0.2644, OBJ: 0.3473, TOTAL: 0.2809\n",
      "CE: 0.1729, OBJ: 0.2917, TOTAL: 0.1966\n",
      "CE: 0.1924, OBJ: 0.2306, TOTAL: 0.2000\n",
      "CE: 0.1663, OBJ: 0.1665, TOTAL: 0.1663\n",
      "CE: 0.1687, OBJ: 0.3946, TOTAL: 0.2139\n",
      "CE: 0.0970, OBJ: 0.4250, TOTAL: 0.1626\n",
      "CE: 0.1771, OBJ: 0.2872, TOTAL: 0.1991\n",
      "CE: 0.2584, OBJ: 0.3027, TOTAL: 0.2673\n",
      "CE: 0.4694, OBJ: 0.4085, TOTAL: 0.4572\n",
      "CE: 0.3052, OBJ: 0.3269, TOTAL: 0.3096\n",
      "CE: 0.2516, OBJ: 0.3526, TOTAL: 0.2718\n",
      "CE: 0.3253, OBJ: 0.2397, TOTAL: 0.3082\n",
      "CE: 0.4363, OBJ: 0.4061, TOTAL: 0.4303\n",
      "CE: 0.3052, OBJ: 0.2929, TOTAL: 0.3028\n",
      "CE: 0.3439, OBJ: 0.2586, TOTAL: 0.3268\n",
      "CE: 0.2002, OBJ: 0.1985, TOTAL: 0.1999\n",
      "CE: 0.5155, OBJ: 0.2089, TOTAL: 0.4542\n",
      "CE: 0.1678, OBJ: 0.2219, TOTAL: 0.1786\n",
      "CE: 0.4028, OBJ: 0.3539, TOTAL: 0.3930\n",
      "CE: 0.1720, OBJ: 0.2124, TOTAL: 0.1801\n",
      "CE: 0.6616, OBJ: 0.2926, TOTAL: 0.5878\n",
      "CE: 0.3778, OBJ: 0.3011, TOTAL: 0.3625\n",
      "CE: 0.6545, OBJ: 0.3934, TOTAL: 0.6023\n",
      "CE: 0.2629, OBJ: 0.2010, TOTAL: 0.2505\n",
      "CE: 0.2324, OBJ: 0.2835, TOTAL: 0.2426\n",
      "CE: 0.4226, OBJ: 0.3750, TOTAL: 0.4131\n",
      "CE: 0.4000, OBJ: 0.3224, TOTAL: 0.3845\n",
      "CE: 0.3719, OBJ: 0.2611, TOTAL: 0.3497\n",
      "CE: 0.1849, OBJ: 0.3411, TOTAL: 0.2161\n",
      "CE: 0.0798, OBJ: 0.2516, TOTAL: 0.1141\n",
      "CE: 0.2358, OBJ: 0.1869, TOTAL: 0.2261\n",
      "CE: 0.4038, OBJ: 0.3460, TOTAL: 0.3922\n",
      "CE: 0.2218, OBJ: 0.3992, TOTAL: 0.2573\n",
      "CE: 0.3765, OBJ: 0.3182, TOTAL: 0.3648\n",
      "CE: 0.6004, OBJ: 0.3061, TOTAL: 0.5415\n",
      "CE: 0.3778, OBJ: 0.5119, TOTAL: 0.4046\n",
      "CE: 0.5047, OBJ: 0.2863, TOTAL: 0.4610\n",
      "CE: 0.3441, OBJ: 0.2577, TOTAL: 0.3268\n",
      "CE: 0.5842, OBJ: 0.4146, TOTAL: 0.5503\n",
      "CE: 0.2895, OBJ: 0.2583, TOTAL: 0.2833\n",
      "CE: 0.3155, OBJ: 0.3293, TOTAL: 0.3183\n",
      "CE: 0.2943, OBJ: 0.3331, TOTAL: 0.3020\n",
      "CE: 0.4996, OBJ: 0.3083, TOTAL: 0.4613\n",
      "CE: 0.4356, OBJ: 0.2521, TOTAL: 0.3989\n",
      "CE: 0.6099, OBJ: 0.4548, TOTAL: 0.5789\n",
      "CE: 0.1712, OBJ: 0.3151, TOTAL: 0.2000\n",
      "CE: 0.2683, OBJ: 0.4195, TOTAL: 0.2986\n",
      "CE: 0.3529, OBJ: 0.3354, TOTAL: 0.3494\n",
      "CE: 0.4823, OBJ: 0.3296, TOTAL: 0.4518\n",
      "CE: 0.2475, OBJ: 0.2280, TOTAL: 0.2436\n",
      "CE: 0.2351, OBJ: 0.4946, TOTAL: 0.2870\n",
      "CE: 0.2183, OBJ: 0.2382, TOTAL: 0.2223\n",
      "CE: 0.5008, OBJ: 0.4905, TOTAL: 0.4987\n",
      "CE: 0.2485, OBJ: 0.2379, TOTAL: 0.2463\n",
      "CE: 0.1348, OBJ: 0.2505, TOTAL: 0.1580\n",
      "CE: 0.3415, OBJ: 0.2351, TOTAL: 0.3202\n",
      "CE: 0.4258, OBJ: 0.2865, TOTAL: 0.3979\n",
      "CE: 0.3384, OBJ: 0.4483, TOTAL: 0.3604\n",
      "CE: 0.2298, OBJ: 0.4331, TOTAL: 0.2705\n",
      "CE: 0.3718, OBJ: 0.2844, TOTAL: 0.3543\n",
      "CE: 0.1076, OBJ: 0.3112, TOTAL: 0.1483\n",
      "CE: 0.0828, OBJ: 0.3649, TOTAL: 0.1392\n",
      "CE: 0.6809, OBJ: 0.4110, TOTAL: 0.6269\n",
      "CE: 0.2005, OBJ: 0.4569, TOTAL: 0.2518\n",
      "CE: 0.2158, OBJ: 0.2806, TOTAL: 0.2288\n",
      "CE: 0.2329, OBJ: 0.2205, TOTAL: 0.2304\n",
      "CE: 0.2007, OBJ: 0.3114, TOTAL: 0.2228\n",
      "CE: 0.1084, OBJ: 0.2534, TOTAL: 0.1374\n",
      "CE: 0.3960, OBJ: 0.3991, TOTAL: 0.3966\n",
      "CE: 0.4149, OBJ: 0.2775, TOTAL: 0.3874\n",
      "CE: 0.3684, OBJ: 0.2592, TOTAL: 0.3466\n",
      "CE: 0.3606, OBJ: 0.2976, TOTAL: 0.3480\n",
      "CE: 0.6593, OBJ: 0.4204, TOTAL: 0.6115\n",
      "CE: 0.1619, OBJ: 0.2574, TOTAL: 0.1810\n",
      "CE: 0.4319, OBJ: 0.3552, TOTAL: 0.4165\n",
      "CE: 0.2245, OBJ: 0.2557, TOTAL: 0.2307\n",
      "CE: 0.3421, OBJ: 0.2309, TOTAL: 0.3198\n",
      "CE: 0.2522, OBJ: 0.2626, TOTAL: 0.2543\n",
      "CE: 0.2564, OBJ: 0.2210, TOTAL: 0.2493\n",
      "CE: 0.9047, OBJ: 0.2899, TOTAL: 0.7817\n",
      "CE: 0.2792, OBJ: 0.3901, TOTAL: 0.3014\n",
      "CE: 0.4784, OBJ: 0.4131, TOTAL: 0.4654\n",
      "CE: 0.5143, OBJ: 0.3220, TOTAL: 0.4758\n",
      "CE: 0.3994, OBJ: 0.3577, TOTAL: 0.3911\n",
      "CE: 0.1620, OBJ: 0.3621, TOTAL: 0.2020\n",
      "CE: 0.3862, OBJ: 0.3088, TOTAL: 0.3707\n",
      "CE: 0.2301, OBJ: 0.2197, TOTAL: 0.2280\n",
      "CE: 0.7910, OBJ: 0.5002, TOTAL: 0.7329\n",
      "CE: 0.2691, OBJ: 0.3340, TOTAL: 0.2821\n",
      "CE: 0.5636, OBJ: 0.4239, TOTAL: 0.5357\n",
      "CE: 0.6946, OBJ: 0.3581, TOTAL: 0.6273\n",
      "CE: 0.3876, OBJ: 0.4549, TOTAL: 0.4011\n",
      "CE: 0.4211, OBJ: 0.4483, TOTAL: 0.4265\n",
      "CE: 0.2102, OBJ: 0.3427, TOTAL: 0.2367\n",
      "CE: 0.1765, OBJ: 0.3769, TOTAL: 0.2166\n",
      "CE: 0.1786, OBJ: 0.1751, TOTAL: 0.1779\n",
      "CE: 0.1739, OBJ: 0.3087, TOTAL: 0.2008\n",
      "CE: 0.8166, OBJ: 0.2211, TOTAL: 0.6975\n",
      "CE: 0.1186, OBJ: 0.1539, TOTAL: 0.1256\n",
      "CE: 0.3568, OBJ: 0.2633, TOTAL: 0.3381\n",
      "CE: 0.4510, OBJ: 0.3123, TOTAL: 0.4233\n",
      "CE: 0.3132, OBJ: 0.2686, TOTAL: 0.3043\n",
      "CE: 0.2179, OBJ: 0.3276, TOTAL: 0.2398\n",
      "CE: 0.1531, OBJ: 0.2685, TOTAL: 0.1762\n",
      "CE: 0.1402, OBJ: 0.3286, TOTAL: 0.1779\n",
      "CE: 0.1997, OBJ: 0.3526, TOTAL: 0.2303\n",
      "CE: 0.4592, OBJ: 0.3378, TOTAL: 0.4349\n",
      "CE: 0.5953, OBJ: 0.4216, TOTAL: 0.5606\n",
      "CE: 0.1281, OBJ: 0.1996, TOTAL: 0.1424\n",
      "CE: 0.1074, OBJ: 0.1231, TOTAL: 0.1105\n",
      "CE: 0.1635, OBJ: 0.3564, TOTAL: 0.2021\n",
      "CE: 0.2357, OBJ: 0.5027, TOTAL: 0.2891\n",
      "CE: 0.4260, OBJ: 0.4919, TOTAL: 0.4392\n",
      "CE: 0.3608, OBJ: 0.3043, TOTAL: 0.3495\n",
      "CE: 0.2870, OBJ: 0.4185, TOTAL: 0.3133\n",
      "CE: 0.3252, OBJ: 0.4744, TOTAL: 0.3551\n",
      "CE: 0.1503, OBJ: 0.3557, TOTAL: 0.1914\n",
      "CE: 0.4128, OBJ: 0.3062, TOTAL: 0.3915\n",
      "CE: 0.3147, OBJ: 0.2435, TOTAL: 0.3005\n",
      "CE: 0.0755, OBJ: 0.3328, TOTAL: 0.1270\n",
      "CE: 0.7906, OBJ: 0.2904, TOTAL: 0.6906\n",
      "CE: 0.5402, OBJ: 0.4177, TOTAL: 0.5157\n",
      "CE: 0.3120, OBJ: 0.3079, TOTAL: 0.3112\n",
      "CE: 0.3943, OBJ: 0.1957, TOTAL: 0.3546\n",
      "CE: 0.2202, OBJ: 0.3915, TOTAL: 0.2544\n",
      "CE: 0.3951, OBJ: 0.3976, TOTAL: 0.3956\n",
      "CE: 0.2765, OBJ: 0.2709, TOTAL: 0.2754\n",
      "CE: 0.1896, OBJ: 0.2550, TOTAL: 0.2027\n",
      "CE: 0.3680, OBJ: 0.5262, TOTAL: 0.3997\n",
      "CE: 0.3326, OBJ: 0.2596, TOTAL: 0.3180\n",
      "CE: 0.3354, OBJ: 0.3295, TOTAL: 0.3342\n",
      "CE: 0.3268, OBJ: 0.2533, TOTAL: 0.3121\n",
      "CE: 0.6395, OBJ: 0.4335, TOTAL: 0.5983\n",
      "CE: 0.2967, OBJ: 0.3007, TOTAL: 0.2975\n",
      "CE: 0.1861, OBJ: 0.2420, TOTAL: 0.1973\n",
      "CE: 0.4618, OBJ: 0.3452, TOTAL: 0.4385\n",
      "CE: 0.2018, OBJ: 0.3584, TOTAL: 0.2331\n",
      "CE: 0.8464, OBJ: 0.4146, TOTAL: 0.7600\n",
      "CE: 0.2139, OBJ: 0.2526, TOTAL: 0.2217\n",
      "CE: 0.3590, OBJ: 0.3695, TOTAL: 0.3611\n",
      "CE: 0.1890, OBJ: 0.2407, TOTAL: 0.1993\n",
      "CE: 0.3504, OBJ: 0.2617, TOTAL: 0.3327\n",
      "CE: 0.2670, OBJ: 0.2540, TOTAL: 0.2644\n",
      "CE: 0.2889, OBJ: 0.2162, TOTAL: 0.2744\n",
      "CE: 0.2120, OBJ: 0.3840, TOTAL: 0.2464\n",
      "CE: 0.5869, OBJ: 0.4466, TOTAL: 0.5589\n",
      "CE: 0.1225, OBJ: 0.4614, TOTAL: 0.1903\n",
      "CE: 0.1012, OBJ: 0.2822, TOTAL: 0.1374\n",
      "CE: 0.3365, OBJ: 0.2966, TOTAL: 0.3285\n",
      "CE: 0.2139, OBJ: 0.3655, TOTAL: 0.2442\n",
      "CE: 0.1817, OBJ: 0.3153, TOTAL: 0.2084\n",
      "CE: 0.3302, OBJ: 0.3044, TOTAL: 0.3251\n",
      "CE: 0.2692, OBJ: 0.4220, TOTAL: 0.2998\n",
      "CE: 0.1619, OBJ: 0.3589, TOTAL: 0.2013\n",
      "CE: 0.2142, OBJ: 0.3156, TOTAL: 0.2345\n",
      "CE: 0.3207, OBJ: 0.2722, TOTAL: 0.3110\n",
      "CE: 0.2753, OBJ: 0.3336, TOTAL: 0.2869\n",
      "CE: 0.3519, OBJ: 0.2945, TOTAL: 0.3404\n",
      "CE: 0.3144, OBJ: 0.2119, TOTAL: 0.2939\n",
      "CE: 0.3761, OBJ: 0.3025, TOTAL: 0.3614\n",
      "CE: 0.1948, OBJ: 0.3488, TOTAL: 0.2256\n",
      "CE: 0.2655, OBJ: 0.3403, TOTAL: 0.2804\n",
      "CE: 0.3509, OBJ: 0.4380, TOTAL: 0.3683\n",
      "CE: 0.3089, OBJ: 0.4341, TOTAL: 0.3340\n",
      "CE: 0.4392, OBJ: 0.3502, TOTAL: 0.4214\n",
      "CE: 0.5548, OBJ: 0.2762, TOTAL: 0.4991\n",
      "CE: 0.6180, OBJ: 0.2843, TOTAL: 0.5512\n",
      "CE: 0.3368, OBJ: 0.4454, TOTAL: 0.3585\n",
      "CE: 0.1831, OBJ: 0.3517, TOTAL: 0.2168\n",
      "CE: 0.0553, OBJ: 0.3045, TOTAL: 0.1051\n",
      "CE: 0.1670, OBJ: 0.3709, TOTAL: 0.2078\n",
      "CE: 0.3264, OBJ: 0.2612, TOTAL: 0.3133\n",
      "CE: 0.2243, OBJ: 0.3060, TOTAL: 0.2406\n",
      "CE: 0.1770, OBJ: 0.2782, TOTAL: 0.1972\n",
      "CE: 0.0484, OBJ: 0.3018, TOTAL: 0.0991\n",
      "CE: 0.1136, OBJ: 0.2748, TOTAL: 0.1459\n",
      "CE: 0.4483, OBJ: 0.4393, TOTAL: 0.4465\n",
      "CE: 0.3651, OBJ: 0.4031, TOTAL: 0.3727\n",
      "CE: 0.3920, OBJ: 0.2844, TOTAL: 0.3704\n",
      "CE: 0.1650, OBJ: 0.3095, TOTAL: 0.1939\n",
      "CE: 0.3146, OBJ: 0.3629, TOTAL: 0.3243\n",
      "CE: 0.3379, OBJ: 0.3981, TOTAL: 0.3499\n",
      "CE: 0.4172, OBJ: 0.2290, TOTAL: 0.3796\n",
      "CE: 0.4580, OBJ: 0.3145, TOTAL: 0.4293\n",
      "CE: 0.2719, OBJ: 0.3231, TOTAL: 0.2822\n",
      "CE: 0.3344, OBJ: 0.3294, TOTAL: 0.3334\n",
      "CE: 0.1492, OBJ: 0.3649, TOTAL: 0.1924\n",
      "CE: 0.6207, OBJ: 0.3504, TOTAL: 0.5666\n",
      "CE: 0.5930, OBJ: 0.3994, TOTAL: 0.5543\n",
      "CE: 0.5972, OBJ: 0.3978, TOTAL: 0.5574\n",
      "CE: 0.6313, OBJ: 0.4010, TOTAL: 0.5853\n",
      "CE: 0.1412, OBJ: 0.3171, TOTAL: 0.1764\n",
      "CE: 0.6567, OBJ: 0.2652, TOTAL: 0.5784\n",
      "CE: 0.4062, OBJ: 0.2175, TOTAL: 0.3685\n",
      "CE: 0.2957, OBJ: 0.3982, TOTAL: 0.3162\n",
      "CE: 0.8268, OBJ: 0.3950, TOTAL: 0.7404\n",
      "CE: 0.7174, OBJ: 0.4774, TOTAL: 0.6694\n",
      "CE: 0.2071, OBJ: 0.2683, TOTAL: 0.2194\n",
      "CE: 0.4521, OBJ: 0.2210, TOTAL: 0.4059\n",
      "CE: 0.4529, OBJ: 0.3066, TOTAL: 0.4237\n",
      "CE: 0.3110, OBJ: 0.2926, TOTAL: 0.3073\n",
      "CE: 0.8660, OBJ: 0.3583, TOTAL: 0.7645\n",
      "CE: 0.3492, OBJ: 0.3209, TOTAL: 0.3435\n",
      "CE: 0.2415, OBJ: 0.2405, TOTAL: 0.2413\n",
      "CE: 0.4166, OBJ: 0.2967, TOTAL: 0.3926\n",
      "CE: 0.6851, OBJ: 0.2442, TOTAL: 0.5969\n",
      "CE: 0.3604, OBJ: 0.3003, TOTAL: 0.3484\n",
      "CE: 0.0901, OBJ: 0.2863, TOTAL: 0.1293\n",
      "CE: 0.1558, OBJ: 0.2302, TOTAL: 0.1707\n",
      "CE: 0.0785, OBJ: 0.3266, TOTAL: 0.1281\n",
      "CE: 0.4275, OBJ: 0.3449, TOTAL: 0.4109\n",
      "CE: 0.2859, OBJ: 0.2193, TOTAL: 0.2726\n",
      "CE: 0.5458, OBJ: 0.2857, TOTAL: 0.4938\n",
      "CE: 0.5952, OBJ: 0.3308, TOTAL: 0.5423\n",
      "CE: 0.2279, OBJ: 0.1792, TOTAL: 0.2182\n",
      "CE: 0.2978, OBJ: 0.3580, TOTAL: 0.3098\n",
      "CE: 0.2823, OBJ: 0.2939, TOTAL: 0.2846\n",
      "CE: 0.2326, OBJ: 0.3399, TOTAL: 0.2541\n",
      "CE: 0.5384, OBJ: 0.2073, TOTAL: 0.4722\n",
      "CE: 0.2672, OBJ: 0.2486, TOTAL: 0.2635\n",
      "CE: 0.1991, OBJ: 0.2405, TOTAL: 0.2074\n",
      "CE: 0.1416, OBJ: 0.3083, TOTAL: 0.1749\n",
      "CE: 0.2633, OBJ: 0.4140, TOTAL: 0.2934\n",
      "CE: 0.3116, OBJ: 0.4169, TOTAL: 0.3327\n",
      "CE: 0.9513, OBJ: 0.3505, TOTAL: 0.8311\n",
      "CE: 0.4392, OBJ: 0.2166, TOTAL: 0.3947\n",
      "CE: 0.0934, OBJ: 0.3849, TOTAL: 0.1517\n",
      "CE: 0.2526, OBJ: 0.3283, TOTAL: 0.2677\n",
      "CE: 0.3773, OBJ: 0.4139, TOTAL: 0.3846\n",
      "CE: 0.7275, OBJ: 0.4277, TOTAL: 0.6675\n",
      "CE: 0.1878, OBJ: 0.4598, TOTAL: 0.2422\n",
      "CE: 0.6037, OBJ: 0.1692, TOTAL: 0.5168\n",
      "CE: 0.4546, OBJ: 0.4773, TOTAL: 0.4591\n",
      "CE: 0.2239, OBJ: 0.2914, TOTAL: 0.2374\n",
      "CE: 0.6898, OBJ: 0.4235, TOTAL: 0.6365\n",
      "CE: 0.3871, OBJ: 0.3004, TOTAL: 0.3697\n",
      "CE: 0.2638, OBJ: 0.3009, TOTAL: 0.2712\n",
      "CE: 0.4211, OBJ: 0.3529, TOTAL: 0.4074\n",
      "CE: 0.2319, OBJ: 0.2628, TOTAL: 0.2381\n",
      "CE: 0.3026, OBJ: 0.2288, TOTAL: 0.2879\n",
      "CE: 0.1129, OBJ: 0.3690, TOTAL: 0.1641\n",
      "CE: 0.3108, OBJ: 0.3324, TOTAL: 0.3151\n",
      "CE: 0.3234, OBJ: 0.3886, TOTAL: 0.3365\n",
      "CE: 0.3062, OBJ: 0.3867, TOTAL: 0.3223\n",
      "CE: 0.3952, OBJ: 0.3323, TOTAL: 0.3826\n",
      "CE: 0.1530, OBJ: 0.1973, TOTAL: 0.1619\n",
      "CE: 0.1028, OBJ: 0.2779, TOTAL: 0.1378\n",
      "CE: 0.1193, OBJ: 0.2583, TOTAL: 0.1471\n",
      "CE: 0.2742, OBJ: 0.2694, TOTAL: 0.2733\n",
      "CE: 0.1386, OBJ: 0.3254, TOTAL: 0.1760\n",
      "CE: 0.2186, OBJ: 0.2585, TOTAL: 0.2266\n",
      "CE: 0.0625, OBJ: 0.1361, TOTAL: 0.0772\n",
      "CE: 0.3446, OBJ: 0.2793, TOTAL: 0.3316\n",
      "CE: 0.3674, OBJ: 0.4348, TOTAL: 0.3809\n",
      "CE: 0.5674, OBJ: 0.1669, TOTAL: 0.4873\n",
      "CE: 0.3485, OBJ: 0.2059, TOTAL: 0.3200\n",
      "CE: 0.2323, OBJ: 0.3229, TOTAL: 0.2504\n",
      "CE: 0.4154, OBJ: 0.2557, TOTAL: 0.3834\n",
      "CE: 0.1218, OBJ: 0.3343, TOTAL: 0.1643\n",
      "CE: 0.3404, OBJ: 0.4110, TOTAL: 0.3545\n",
      "CE: 0.1054, OBJ: 0.2543, TOTAL: 0.1351\n",
      "CE: 0.2136, OBJ: 0.3329, TOTAL: 0.2375\n",
      "CE: 0.4453, OBJ: 0.2497, TOTAL: 0.4062\n",
      "CE: 0.7109, OBJ: 0.3549, TOTAL: 0.6397\n",
      "CE: 0.2011, OBJ: 0.2513, TOTAL: 0.2111\n",
      "CE: 0.2270, OBJ: 0.2965, TOTAL: 0.2409\n",
      "CE: 0.3240, OBJ: 0.3077, TOTAL: 0.3208\n",
      "CE: 0.7282, OBJ: 0.3857, TOTAL: 0.6597\n",
      "CE: 0.4611, OBJ: 0.4142, TOTAL: 0.4518\n",
      "CE: 0.4876, OBJ: 0.3406, TOTAL: 0.4582\n",
      "CE: 0.4356, OBJ: 0.3121, TOTAL: 0.4109\n",
      "CE: 0.1447, OBJ: 0.3297, TOTAL: 0.1817\n",
      "CE: 0.7736, OBJ: 0.5150, TOTAL: 0.7218\n",
      "CE: 0.7747, OBJ: 0.4037, TOTAL: 0.7005\n",
      "CE: 0.7494, OBJ: 0.3688, TOTAL: 0.6733\n",
      "CE: 0.4206, OBJ: 0.3213, TOTAL: 0.4008\n",
      "CE: 0.5098, OBJ: 0.3355, TOTAL: 0.4749\n",
      "CE: 0.3800, OBJ: 0.3847, TOTAL: 0.3810\n",
      "CE: 0.2261, OBJ: 0.2987, TOTAL: 0.2406\n",
      "CE: 0.1053, OBJ: 0.2702, TOTAL: 0.1383\n",
      "CE: 0.4532, OBJ: 0.3922, TOTAL: 0.4410\n",
      "CE: 0.6134, OBJ: 0.2454, TOTAL: 0.5398\n",
      "CE: 0.3046, OBJ: 0.3358, TOTAL: 0.3108\n",
      "CE: 0.3224, OBJ: 0.3506, TOTAL: 0.3281\n",
      "CE: 0.1309, OBJ: 0.2805, TOTAL: 0.1608\n",
      "CE: 0.2337, OBJ: 0.3128, TOTAL: 0.2495\n",
      "CE: 0.2818, OBJ: 0.3289, TOTAL: 0.2912\n",
      "CE: 0.1477, OBJ: 0.3152, TOTAL: 0.1812\n",
      "CE: 0.4700, OBJ: 0.3453, TOTAL: 0.4451\n",
      "CE: 0.1272, OBJ: 0.2300, TOTAL: 0.1477\n",
      "CE: 0.2629, OBJ: 0.2727, TOTAL: 0.2648\n",
      "CE: 0.3042, OBJ: 0.3023, TOTAL: 0.3038\n",
      "CE: 0.2052, OBJ: 0.3320, TOTAL: 0.2306\n",
      "CE: 0.8482, OBJ: 0.3407, TOTAL: 0.7467\n",
      "CE: 0.6953, OBJ: 0.2799, TOTAL: 0.6122\n",
      "CE: 0.1583, OBJ: 0.1749, TOTAL: 0.1616\n",
      "CE: 0.5904, OBJ: 0.2447, TOTAL: 0.5213\n",
      "CE: 0.2142, OBJ: 0.3237, TOTAL: 0.2361\n",
      "CE: 0.1385, OBJ: 0.3250, TOTAL: 0.1758\n",
      "CE: 0.2572, OBJ: 0.3747, TOTAL: 0.2807\n",
      "CE: 0.4438, OBJ: 0.2841, TOTAL: 0.4119\n",
      "CE: 0.3534, OBJ: 0.3638, TOTAL: 0.3555\n",
      "CE: 0.2476, OBJ: 0.2907, TOTAL: 0.2562\n",
      "CE: 0.6709, OBJ: 0.4146, TOTAL: 0.6196\n",
      "CE: 0.4025, OBJ: 0.3443, TOTAL: 0.3908\n",
      "CE: 0.4903, OBJ: 0.4609, TOTAL: 0.4844\n",
      "CE: 0.2029, OBJ: 0.4357, TOTAL: 0.2495\n",
      "CE: 0.3119, OBJ: 0.3238, TOTAL: 0.3143\n",
      "CE: 0.2293, OBJ: 0.3328, TOTAL: 0.2500\n",
      "CE: 0.2490, OBJ: 0.2915, TOTAL: 0.2575\n",
      "CE: 0.4520, OBJ: 0.2816, TOTAL: 0.4179\n",
      "CE: 0.3974, OBJ: 0.2746, TOTAL: 0.3728\n",
      "CE: 0.1564, OBJ: 0.2504, TOTAL: 0.1752\n",
      "CE: 0.4268, OBJ: 0.1510, TOTAL: 0.3716\n",
      "CE: 0.2362, OBJ: 0.3946, TOTAL: 0.2679\n",
      "CE: 0.3399, OBJ: 0.2985, TOTAL: 0.3316\n",
      "CE: 0.1049, OBJ: 0.2295, TOTAL: 0.1298\n",
      "CE: 0.2995, OBJ: 0.3618, TOTAL: 0.3119\n",
      "CE: 0.3444, OBJ: 0.4148, TOTAL: 0.3585\n",
      "CE: 0.1382, OBJ: 0.3730, TOTAL: 0.1851\n",
      "CE: 0.1885, OBJ: 0.2677, TOTAL: 0.2043\n",
      "CE: 0.2149, OBJ: 0.2474, TOTAL: 0.2214\n",
      "CE: 0.1732, OBJ: 0.3685, TOTAL: 0.2123\n",
      "CE: 0.3701, OBJ: 0.2793, TOTAL: 0.3520\n",
      "CE: 0.3684, OBJ: 0.2484, TOTAL: 0.3444\n",
      "CE: 0.1668, OBJ: 0.2635, TOTAL: 0.1861\n",
      "CE: 0.5836, OBJ: 0.3313, TOTAL: 0.5331\n",
      "CE: 0.4336, OBJ: 0.5573, TOTAL: 0.4583\n",
      "CE: 0.3211, OBJ: 0.2957, TOTAL: 0.3160\n",
      "CE: 0.2944, OBJ: 0.2005, TOTAL: 0.2756\n",
      "CE: 0.7490, OBJ: 0.4318, TOTAL: 0.6856\n",
      "CE: 0.4131, OBJ: 0.3170, TOTAL: 0.3939\n",
      "CE: 0.3727, OBJ: 0.2714, TOTAL: 0.3524\n",
      "CE: 0.7187, OBJ: 0.2429, TOTAL: 0.6236\n",
      "CE: 0.4341, OBJ: 0.4252, TOTAL: 0.4323\n",
      "CE: 0.1448, OBJ: 0.4125, TOTAL: 0.1983\n",
      "CE: 0.3141, OBJ: 0.4213, TOTAL: 0.3355\n",
      "CE: 0.4657, OBJ: 0.3672, TOTAL: 0.4460\n",
      "CE: 0.2409, OBJ: 0.4012, TOTAL: 0.2730\n",
      "CE: 0.3013, OBJ: 0.3994, TOTAL: 0.3209\n",
      "CE: 0.4474, OBJ: 0.3439, TOTAL: 0.4267\n",
      "CE: 0.3822, OBJ: 0.3751, TOTAL: 0.3808\n",
      "CE: 0.1590, OBJ: 0.2674, TOTAL: 0.1807\n",
      "CE: 0.2402, OBJ: 0.2936, TOTAL: 0.2508\n",
      "CE: 0.3335, OBJ: 0.1946, TOTAL: 0.3057\n",
      "CE: 0.5253, OBJ: 0.3474, TOTAL: 0.4897\n",
      "CE: 0.2939, OBJ: 0.4249, TOTAL: 0.3201\n",
      "CE: 0.3610, OBJ: 0.4156, TOTAL: 0.3719\n",
      "CE: 0.3839, OBJ: 0.3528, TOTAL: 0.3777\n",
      "CE: 0.3017, OBJ: 0.3309, TOTAL: 0.3076\n",
      "CE: 0.2523, OBJ: 0.4257, TOTAL: 0.2870\n",
      "CE: 0.3572, OBJ: 0.4059, TOTAL: 0.3670\n",
      "CE: 0.2890, OBJ: 0.3479, TOTAL: 0.3008\n",
      "CE: 0.2720, OBJ: 0.2333, TOTAL: 0.2642\n",
      "CE: 0.2601, OBJ: 0.1923, TOTAL: 0.2465\n",
      "CE: 0.3331, OBJ: 0.1522, TOTAL: 0.2969\n",
      "CE: 0.3615, OBJ: 0.2568, TOTAL: 0.3406\n",
      "CE: 0.3477, OBJ: 0.3697, TOTAL: 0.3521\n",
      "CE: 0.2111, OBJ: 0.3640, TOTAL: 0.2417\n",
      "CE: 0.3106, OBJ: 0.4582, TOTAL: 0.3401\n",
      "CE: 0.4148, OBJ: 0.2874, TOTAL: 0.3893\n",
      "CE: 0.1469, OBJ: 0.2969, TOTAL: 0.1769\n",
      "CE: 0.3904, OBJ: 0.2365, TOTAL: 0.3596\n",
      "CE: 0.5295, OBJ: 0.3544, TOTAL: 0.4944\n",
      "CE: 0.2657, OBJ: 0.2949, TOTAL: 0.2715\n",
      "CE: 0.2783, OBJ: 0.3098, TOTAL: 0.2846\n",
      "CE: 0.2030, OBJ: 0.4027, TOTAL: 0.2430\n",
      "CE: 0.1515, OBJ: 0.2692, TOTAL: 0.1750\n",
      "CE: 0.8422, OBJ: 0.3876, TOTAL: 0.7513\n",
      "CE: 0.5976, OBJ: 0.4060, TOTAL: 0.5593\n",
      "CE: 0.4534, OBJ: 0.3300, TOTAL: 0.4287\n",
      "CE: 0.2804, OBJ: 0.2213, TOTAL: 0.2685\n",
      "CE: 0.2272, OBJ: 0.4151, TOTAL: 0.2648\n",
      "CE: 0.2572, OBJ: 0.3136, TOTAL: 0.2685\n",
      "CE: 0.2959, OBJ: 0.3968, TOTAL: 0.3161\n",
      "CE: 0.2648, OBJ: 0.2430, TOTAL: 0.2604\n",
      "CE: 0.2908, OBJ: 0.3918, TOTAL: 0.3110\n",
      "CE: 0.4693, OBJ: 0.2553, TOTAL: 0.4265\n",
      "CE: 0.2186, OBJ: 0.2031, TOTAL: 0.2155\n",
      "CE: 0.2670, OBJ: 0.2677, TOTAL: 0.2671\n",
      "CE: 0.4104, OBJ: 0.3497, TOTAL: 0.3983\n",
      "CE: 0.5473, OBJ: 0.3571, TOTAL: 0.5092\n",
      "CE: 0.3781, OBJ: 0.2889, TOTAL: 0.3602\n",
      "CE: 0.3691, OBJ: 0.2777, TOTAL: 0.3508\n",
      "CE: 0.3631, OBJ: 0.3298, TOTAL: 0.3564\n",
      "CE: 0.3592, OBJ: 0.3274, TOTAL: 0.3529\n",
      "CE: 0.1719, OBJ: 0.4381, TOTAL: 0.2251\n",
      "CE: 0.2559, OBJ: 0.2603, TOTAL: 0.2568\n",
      "CE: 0.1112, OBJ: 0.2286, TOTAL: 0.1347\n",
      "CE: 0.2128, OBJ: 0.3470, TOTAL: 0.2396\n",
      "CE: 0.3386, OBJ: 0.3386, TOTAL: 0.3386\n",
      "CE: 0.1910, OBJ: 0.4504, TOTAL: 0.2429\n",
      "CE: 0.6820, OBJ: 0.4023, TOTAL: 0.6261\n",
      "CE: 0.6455, OBJ: 0.4227, TOTAL: 0.6009\n",
      "CE: 0.4339, OBJ: 0.3686, TOTAL: 0.4208\n",
      "CE: 0.6111, OBJ: 0.3885, TOTAL: 0.5666\n",
      "CE: 0.4160, OBJ: 0.4625, TOTAL: 0.4253\n",
      "CE: 0.1070, OBJ: 0.1936, TOTAL: 0.1243\n",
      "CE: 0.2662, OBJ: 0.3401, TOTAL: 0.2810\n",
      "CE: 0.1021, OBJ: 0.3605, TOTAL: 0.1538\n",
      "CE: 0.3948, OBJ: 0.2708, TOTAL: 0.3700\n",
      "CE: 0.4055, OBJ: 0.3330, TOTAL: 0.3910\n",
      "CE: 0.2179, OBJ: 0.3151, TOTAL: 0.2373\n",
      "CE: 0.2090, OBJ: 0.3944, TOTAL: 0.2461\n",
      "CE: 0.1088, OBJ: 0.2770, TOTAL: 0.1424\n",
      "CE: 0.2564, OBJ: 0.2920, TOTAL: 0.2635\n",
      "CE: 0.3810, OBJ: 0.4084, TOTAL: 0.3865\n",
      "CE: 0.4926, OBJ: 0.2002, TOTAL: 0.4341\n",
      "CE: 0.3334, OBJ: 0.3703, TOTAL: 0.3408\n",
      "CE: 0.3324, OBJ: 0.2666, TOTAL: 0.3192\n",
      "CE: 0.0705, OBJ: 0.2501, TOTAL: 0.1065\n",
      "CE: 0.2446, OBJ: 0.3160, TOTAL: 0.2589\n",
      "CE: 0.5002, OBJ: 0.3573, TOTAL: 0.4716\n",
      "CE: 0.2980, OBJ: 0.4125, TOTAL: 0.3209\n",
      "CE: 0.1709, OBJ: 0.3681, TOTAL: 0.2104\n",
      "CE: 0.3941, OBJ: 0.3022, TOTAL: 0.3757\n",
      "CE: 0.5831, OBJ: 0.2829, TOTAL: 0.5231\n",
      "CE: 0.2844, OBJ: 0.1900, TOTAL: 0.2655\n",
      "CE: 0.4768, OBJ: 0.3401, TOTAL: 0.4495\n",
      "CE: 0.2419, OBJ: 0.3159, TOTAL: 0.2567\n",
      "CE: 0.2461, OBJ: 0.4310, TOTAL: 0.2831\n",
      "CE: 0.3258, OBJ: 0.2098, TOTAL: 0.3026\n",
      "CE: 0.5137, OBJ: 0.3054, TOTAL: 0.4721\n",
      "CE: 0.2157, OBJ: 0.2886, TOTAL: 0.2303\n",
      "CE: 0.5255, OBJ: 0.4573, TOTAL: 0.5118\n",
      "CE: 0.2264, OBJ: 0.4553, TOTAL: 0.2722\n",
      "CE: 0.1195, OBJ: 0.2541, TOTAL: 0.1464\n",
      "CE: 0.1416, OBJ: 0.2777, TOTAL: 0.1688\n",
      "CE: 0.2941, OBJ: 0.2582, TOTAL: 0.2870\n",
      "CE: 0.2565, OBJ: 0.3970, TOTAL: 0.2846\n",
      "CE: 0.1467, OBJ: 0.3424, TOTAL: 0.1858\n",
      "CE: 0.7669, OBJ: 0.4369, TOTAL: 0.7009\n",
      "CE: 0.1322, OBJ: 0.2291, TOTAL: 0.1516\n",
      "CE: 0.1795, OBJ: 0.2393, TOTAL: 0.1915\n",
      "CE: 0.2564, OBJ: 0.3309, TOTAL: 0.2713\n",
      "CE: 0.1836, OBJ: 0.3434, TOTAL: 0.2156\n",
      "CE: 0.3872, OBJ: 0.3865, TOTAL: 0.3870\n",
      "CE: 0.0903, OBJ: 0.4002, TOTAL: 0.1523\n",
      "CE: 0.3630, OBJ: 0.3753, TOTAL: 0.3655\n",
      "CE: 0.2667, OBJ: 0.2952, TOTAL: 0.2724\n",
      "CE: 0.2418, OBJ: 0.3887, TOTAL: 0.2712\n",
      "CE: 0.3228, OBJ: 0.3390, TOTAL: 0.3260\n",
      "CE: 0.4924, OBJ: 0.3811, TOTAL: 0.4701\n",
      "CE: 0.5658, OBJ: 0.4285, TOTAL: 0.5384\n",
      "CE: 0.2861, OBJ: 0.2333, TOTAL: 0.2756\n",
      "CE: 0.2606, OBJ: 0.2146, TOTAL: 0.2514\n",
      "CE: 0.3658, OBJ: 0.2332, TOTAL: 0.3393\n",
      "CE: 0.3855, OBJ: 0.3942, TOTAL: 0.3872\n",
      "CE: 0.3160, OBJ: 0.4232, TOTAL: 0.3374\n",
      "CE: 0.5644, OBJ: 0.2923, TOTAL: 0.5099\n",
      "CE: 0.1356, OBJ: 0.2152, TOTAL: 0.1516\n",
      "CE: 0.1284, OBJ: 0.3957, TOTAL: 0.1819\n",
      "CE: 0.2049, OBJ: 0.2559, TOTAL: 0.2151\n",
      "CE: 0.3465, OBJ: 0.2561, TOTAL: 0.3285\n",
      "CE: 0.1508, OBJ: 0.1772, TOTAL: 0.1561\n",
      "CE: 0.4675, OBJ: 0.3568, TOTAL: 0.4453\n",
      "CE: 0.3355, OBJ: 0.3203, TOTAL: 0.3325\n",
      "CE: 0.3208, OBJ: 0.1500, TOTAL: 0.2867\n",
      "CE: 0.5570, OBJ: 0.3213, TOTAL: 0.5099\n",
      "CE: 0.2329, OBJ: 0.2004, TOTAL: 0.2264\n",
      "CE: 0.3789, OBJ: 0.3021, TOTAL: 0.3636\n",
      "CE: 0.3953, OBJ: 0.3752, TOTAL: 0.3913\n",
      "CE: 0.2271, OBJ: 0.3254, TOTAL: 0.2468\n",
      "CE: 0.3113, OBJ: 0.3929, TOTAL: 0.3277\n",
      "CE: 0.6346, OBJ: 0.3917, TOTAL: 0.5860\n",
      "CE: 0.3477, OBJ: 0.2519, TOTAL: 0.3285\n",
      "CE: 0.2354, OBJ: 0.2958, TOTAL: 0.2475\n",
      "CE: 0.2035, OBJ: 0.4286, TOTAL: 0.2485\n",
      "CE: 0.3321, OBJ: 0.2277, TOTAL: 0.3112\n",
      "CE: 0.2313, OBJ: 0.3448, TOTAL: 0.2540\n",
      "CE: 0.2997, OBJ: 0.3243, TOTAL: 0.3046\n",
      "CE: 0.1783, OBJ: 0.4437, TOTAL: 0.2314\n",
      "CE: 0.3285, OBJ: 0.4244, TOTAL: 0.3477\n",
      "CE: 0.3510, OBJ: 0.4253, TOTAL: 0.3659\n",
      "CE: 0.1147, OBJ: 0.2832, TOTAL: 0.1484\n",
      "CE: 0.4314, OBJ: 0.4142, TOTAL: 0.4280\n",
      "CE: 0.4525, OBJ: 0.2648, TOTAL: 0.4150\n",
      "CE: 0.3509, OBJ: 0.4973, TOTAL: 0.3802\n",
      "CE: 0.5221, OBJ: 0.2854, TOTAL: 0.4747\n",
      "CE: 0.5824, OBJ: 0.2424, TOTAL: 0.5144\n",
      "CE: 0.2936, OBJ: 0.4535, TOTAL: 0.3256\n",
      "CE: 0.2080, OBJ: 0.3045, TOTAL: 0.2273\n",
      "CE: 0.3133, OBJ: 0.2856, TOTAL: 0.3078\n",
      "CE: 0.2986, OBJ: 0.2832, TOTAL: 0.2955\n",
      "CE: 0.1004, OBJ: 0.2639, TOTAL: 0.1331\n",
      "CE: 0.1702, OBJ: 0.5040, TOTAL: 0.2370\n",
      "CE: 0.2276, OBJ: 0.2043, TOTAL: 0.2229\n",
      "CE: 0.2169, OBJ: 0.2848, TOTAL: 0.2305\n",
      "CE: 0.2070, OBJ: 0.3319, TOTAL: 0.2320\n",
      "CE: 0.2503, OBJ: 0.4009, TOTAL: 0.2805\n",
      "CE: 0.4908, OBJ: 0.2755, TOTAL: 0.4478\n",
      "CE: 0.3347, OBJ: 0.4398, TOTAL: 0.3557\n",
      "CE: 0.3621, OBJ: 0.2561, TOTAL: 0.3409\n",
      "CE: 0.3977, OBJ: 0.3478, TOTAL: 0.3877\n",
      "CE: 0.3293, OBJ: 0.2052, TOTAL: 0.3045\n",
      "CE: 0.3809, OBJ: 0.4353, TOTAL: 0.3918\n",
      "CE: 0.3689, OBJ: 0.2757, TOTAL: 0.3503\n",
      "CE: 0.2703, OBJ: 0.2972, TOTAL: 0.2757\n",
      "CE: 0.5128, OBJ: 0.4501, TOTAL: 0.5003\n",
      "CE: 0.2055, OBJ: 0.2527, TOTAL: 0.2150\n",
      "CE: 0.2619, OBJ: 0.3144, TOTAL: 0.2724\n",
      "CE: 0.6227, OBJ: 0.2640, TOTAL: 0.5509\n",
      "CE: 0.3521, OBJ: 0.3798, TOTAL: 0.3576\n",
      "CE: 0.5367, OBJ: 0.2279, TOTAL: 0.4749\n",
      "CE: 0.3929, OBJ: 0.4259, TOTAL: 0.3995\n",
      "CE: 0.4849, OBJ: 0.1904, TOTAL: 0.4260\n",
      "CE: 0.2060, OBJ: 0.3360, TOTAL: 0.2320\n",
      "CE: 0.1519, OBJ: 0.2476, TOTAL: 0.1711\n",
      "CE: 0.2917, OBJ: 0.3961, TOTAL: 0.3125\n",
      "CE: 0.2892, OBJ: 0.2323, TOTAL: 0.2778\n",
      "CE: 0.3652, OBJ: 0.3470, TOTAL: 0.3615\n",
      "CE: 0.1708, OBJ: 0.4198, TOTAL: 0.2206\n",
      "CE: 0.1436, OBJ: 0.2471, TOTAL: 0.1643\n",
      "CE: 0.3335, OBJ: 0.3191, TOTAL: 0.3307\n",
      "CE: 0.1457, OBJ: 0.3123, TOTAL: 0.1790\n",
      "CE: 0.6329, OBJ: 0.2453, TOTAL: 0.5553\n",
      "CE: 0.1393, OBJ: 0.1439, TOTAL: 0.1403\n",
      "CE: 0.3195, OBJ: 0.4268, TOTAL: 0.3410\n",
      "CE: 0.1240, OBJ: 0.2618, TOTAL: 0.1516\n",
      "CE: 0.2673, OBJ: 0.1899, TOTAL: 0.2518\n",
      "CE: 0.1808, OBJ: 0.2642, TOTAL: 0.1975\n",
      "CE: 0.4758, OBJ: 0.2890, TOTAL: 0.4384\n",
      "CE: 0.3459, OBJ: 0.3203, TOTAL: 0.3408\n",
      "CE: 0.4049, OBJ: 0.2858, TOTAL: 0.3811\n",
      "CE: 0.1641, OBJ: 0.1742, TOTAL: 0.1662\n",
      "CE: 0.8158, OBJ: 0.3636, TOTAL: 0.7253\n",
      "CE: 0.4483, OBJ: 0.3589, TOTAL: 0.4304\n",
      "CE: 0.2758, OBJ: 0.1807, TOTAL: 0.2568\n",
      "CE: 0.2145, OBJ: 0.2743, TOTAL: 0.2264\n",
      "CE: 0.1398, OBJ: 0.3491, TOTAL: 0.1817\n",
      "CE: 0.0938, OBJ: 0.2776, TOTAL: 0.1306\n",
      "CE: 0.2426, OBJ: 0.3032, TOTAL: 0.2547\n",
      "CE: 0.2641, OBJ: 0.3137, TOTAL: 0.2740\n",
      "CE: 0.4492, OBJ: 0.3472, TOTAL: 0.4288\n",
      "CE: 0.3240, OBJ: 0.4680, TOTAL: 0.3528\n",
      "CE: 0.3538, OBJ: 0.4072, TOTAL: 0.3645\n",
      "CE: 0.7181, OBJ: 0.4034, TOTAL: 0.6551\n",
      "CE: 0.1236, OBJ: 0.2340, TOTAL: 0.1457\n",
      "CE: 0.1043, OBJ: 0.3117, TOTAL: 0.1458\n",
      "CE: 0.1345, OBJ: 0.2134, TOTAL: 0.1503\n",
      "CE: 0.3904, OBJ: 0.1992, TOTAL: 0.3522\n",
      "CE: 0.3984, OBJ: 0.4484, TOTAL: 0.4084\n",
      "CE: 0.7121, OBJ: 0.4433, TOTAL: 0.6583\n",
      "CE: 0.3741, OBJ: 0.2935, TOTAL: 0.3580\n",
      "CE: 0.5913, OBJ: 0.2692, TOTAL: 0.5269\n",
      "CE: 0.4111, OBJ: 0.3260, TOTAL: 0.3941\n",
      "CE: 0.4051, OBJ: 0.3929, TOTAL: 0.4027\n",
      "CE: 0.1297, OBJ: 0.2831, TOTAL: 0.1604\n",
      "CE: 0.2289, OBJ: 0.2674, TOTAL: 0.2366\n",
      "CE: 0.5697, OBJ: 0.2589, TOTAL: 0.5075\n",
      "CE: 0.6490, OBJ: 0.4794, TOTAL: 0.6151\n",
      "CE: 0.2347, OBJ: 0.3521, TOTAL: 0.2582\n",
      "CE: 0.1674, OBJ: 0.1739, TOTAL: 0.1687\n",
      "CE: 0.2692, OBJ: 0.3264, TOTAL: 0.2807\n",
      "CE: 0.2369, OBJ: 0.2380, TOTAL: 0.2371\n",
      "CE: 0.4388, OBJ: 0.2236, TOTAL: 0.3958\n",
      "CE: 0.5848, OBJ: 0.2913, TOTAL: 0.5261\n",
      "CE: 0.1281, OBJ: 0.1963, TOTAL: 0.1418\n",
      "CE: 0.1100, OBJ: 0.3772, TOTAL: 0.1634\n",
      "CE: 0.3330, OBJ: 0.3681, TOTAL: 0.3400\n",
      "CE: 0.1820, OBJ: 0.3575, TOTAL: 0.2171\n",
      "CE: 0.2256, OBJ: 0.4295, TOTAL: 0.2664\n",
      "CE: 0.2748, OBJ: 0.4377, TOTAL: 0.3074\n",
      "CE: 0.2418, OBJ: 0.3891, TOTAL: 0.2712\n",
      "CE: 0.5358, OBJ: 0.4147, TOTAL: 0.5116\n",
      "CE: 0.5490, OBJ: 0.2588, TOTAL: 0.4910\n",
      "CE: 0.2026, OBJ: 0.4117, TOTAL: 0.2444\n",
      "CE: 0.3810, OBJ: 0.2845, TOTAL: 0.3617\n",
      "CE: 0.5588, OBJ: 0.3600, TOTAL: 0.5190\n",
      "CE: 0.0963, OBJ: 0.2966, TOTAL: 0.1363\n",
      "CE: 0.3989, OBJ: 0.1998, TOTAL: 0.3591\n",
      "CE: 0.4841, OBJ: 0.3219, TOTAL: 0.4516\n",
      "CE: 0.5488, OBJ: 0.5912, TOTAL: 0.5573\n",
      "CE: 0.2518, OBJ: 0.3082, TOTAL: 0.2631\n",
      "CE: 0.5357, OBJ: 0.3855, TOTAL: 0.5056\n",
      "CE: 0.5717, OBJ: 0.3080, TOTAL: 0.5189\n",
      "CE: 0.7275, OBJ: 0.4380, TOTAL: 0.6696\n",
      "CE: 0.4603, OBJ: 0.3548, TOTAL: 0.4392\n",
      "CE: 0.4135, OBJ: 0.3477, TOTAL: 0.4003\n",
      "CE: 0.5084, OBJ: 0.3508, TOTAL: 0.4769\n",
      "CE: 0.4553, OBJ: 0.4701, TOTAL: 0.4582\n",
      "CE: 0.2437, OBJ: 0.1999, TOTAL: 0.2349\n",
      "CE: 0.2325, OBJ: 0.2820, TOTAL: 0.2424\n",
      "CE: 0.3464, OBJ: 0.4021, TOTAL: 0.3575\n",
      "CE: 0.2067, OBJ: 0.3924, TOTAL: 0.2439\n",
      "CE: 0.1926, OBJ: 0.2835, TOTAL: 0.2108\n",
      "CE: 0.2605, OBJ: 0.1817, TOTAL: 0.2447\n",
      "CE: 0.2031, OBJ: 0.4232, TOTAL: 0.2471\n",
      "CE: 0.3365, OBJ: 0.3261, TOTAL: 0.3344\n",
      "CE: 0.6856, OBJ: 0.2127, TOTAL: 0.5910\n",
      "CE: 0.3504, OBJ: 0.3005, TOTAL: 0.3404\n",
      "CE: 0.3771, OBJ: 0.4261, TOTAL: 0.3869\n",
      "CE: 0.1750, OBJ: 0.3594, TOTAL: 0.2119\n",
      "CE: 0.2293, OBJ: 0.2608, TOTAL: 0.2356\n",
      "CE: 0.2936, OBJ: 0.3263, TOTAL: 0.3002\n",
      "CE: 0.2153, OBJ: 0.3255, TOTAL: 0.2374\n",
      "CE: 0.4677, OBJ: 0.3144, TOTAL: 0.4370\n",
      "CE: 0.2989, OBJ: 0.3707, TOTAL: 0.3133\n",
      "CE: 0.7873, OBJ: 0.3976, TOTAL: 0.7094\n",
      "CE: 0.1700, OBJ: 0.3034, TOTAL: 0.1967\n",
      "CE: 0.5617, OBJ: 0.4257, TOTAL: 0.5345\n",
      "CE: 0.4275, OBJ: 0.3407, TOTAL: 0.4101\n",
      "CE: 0.2397, OBJ: 0.2632, TOTAL: 0.2444\n",
      "CE: 0.3153, OBJ: 0.3159, TOTAL: 0.3154\n",
      "CE: 0.1851, OBJ: 0.2153, TOTAL: 0.1911\n",
      "CE: 0.4275, OBJ: 0.2462, TOTAL: 0.3912\n",
      "CE: 0.5383, OBJ: 0.3946, TOTAL: 0.5096\n",
      "CE: 0.5059, OBJ: 0.3041, TOTAL: 0.4656\n",
      "CE: 0.1050, OBJ: 0.2959, TOTAL: 0.1432\n",
      "CE: 0.4826, OBJ: 0.3962, TOTAL: 0.4653\n",
      "CE: 0.2834, OBJ: 0.3506, TOTAL: 0.2968\n",
      "CE: 0.1452, OBJ: 0.3498, TOTAL: 0.1861\n",
      "CE: 0.2571, OBJ: 0.4423, TOTAL: 0.2942\n",
      "CE: 0.1702, OBJ: 0.2995, TOTAL: 0.1960\n",
      "CE: 0.3007, OBJ: 0.2636, TOTAL: 0.2933\n",
      "CE: 0.1628, OBJ: 0.1776, TOTAL: 0.1657\n",
      "CE: 0.0650, OBJ: 0.1935, TOTAL: 0.0907\n",
      "CE: 0.3656, OBJ: 0.3619, TOTAL: 0.3648\n",
      "CE: 0.1351, OBJ: 0.3429, TOTAL: 0.1766\n",
      "CE: 0.1093, OBJ: 0.4891, TOTAL: 0.1852\n",
      "CE: 0.2942, OBJ: 0.4428, TOTAL: 0.3239\n",
      "CE: 0.2437, OBJ: 0.3650, TOTAL: 0.2680\n",
      "CE: 0.4937, OBJ: 0.3102, TOTAL: 0.4570\n",
      "CE: 0.1093, OBJ: 0.3904, TOTAL: 0.1655\n",
      "CE: 0.3466, OBJ: 0.3306, TOTAL: 0.3434\n",
      "CE: 0.2950, OBJ: 0.2636, TOTAL: 0.2887\n",
      "CE: 0.2944, OBJ: 0.2273, TOTAL: 0.2810\n",
      "CE: 0.2428, OBJ: 0.3988, TOTAL: 0.2740\n",
      "CE: 0.6070, OBJ: 0.2936, TOTAL: 0.5443\n",
      "CE: 0.2383, OBJ: 0.3229, TOTAL: 0.2552\n",
      "CE: 0.1057, OBJ: 0.1780, TOTAL: 0.1202\n",
      "CE: 0.4198, OBJ: 0.1961, TOTAL: 0.3750\n",
      "CE: 0.2354, OBJ: 0.2786, TOTAL: 0.2441\n",
      "CE: 0.2601, OBJ: 0.3522, TOTAL: 0.2785\n",
      "CE: 0.5926, OBJ: 0.2125, TOTAL: 0.5166\n",
      "CE: 0.6383, OBJ: 0.4757, TOTAL: 0.6058\n",
      "CE: 0.2299, OBJ: 0.3668, TOTAL: 0.2573\n",
      "CE: 0.2035, OBJ: 0.4026, TOTAL: 0.2433\n",
      "CE: 0.3027, OBJ: 0.2043, TOTAL: 0.2830\n",
      "CE: 0.2942, OBJ: 0.4056, TOTAL: 0.3165\n",
      "CE: 0.3462, OBJ: 0.3635, TOTAL: 0.3496\n",
      "CE: 0.1392, OBJ: 0.2238, TOTAL: 0.1561\n",
      "CE: 0.1222, OBJ: 0.3545, TOTAL: 0.1687\n",
      "CE: 0.4795, OBJ: 0.5014, TOTAL: 0.4838\n",
      "CE: 0.5480, OBJ: 0.4479, TOTAL: 0.5280\n",
      "CE: 0.0957, OBJ: 0.3419, TOTAL: 0.1449\n",
      "CE: 0.0960, OBJ: 0.4044, TOTAL: 0.1577\n",
      "CE: 0.2436, OBJ: 0.3494, TOTAL: 0.2647\n",
      "CE: 0.1798, OBJ: 0.3412, TOTAL: 0.2121\n",
      "CE: 0.3615, OBJ: 0.3714, TOTAL: 0.3635\n",
      "CE: 0.5850, OBJ: 0.3917, TOTAL: 0.5463\n",
      "CE: 0.2084, OBJ: 0.3290, TOTAL: 0.2325\n",
      "CE: 0.2827, OBJ: 0.2590, TOTAL: 0.2780\n",
      "CE: 0.3233, OBJ: 0.3452, TOTAL: 0.3277\n",
      "CE: 0.3754, OBJ: 0.4840, TOTAL: 0.3971\n",
      "CE: 0.8547, OBJ: 0.3417, TOTAL: 0.7521\n",
      "CE: 0.1574, OBJ: 0.3269, TOTAL: 0.1913\n",
      "CE: 0.4618, OBJ: 0.3349, TOTAL: 0.4364\n",
      "CE: 0.4779, OBJ: 0.2640, TOTAL: 0.4351\n",
      "CE: 0.4655, OBJ: 0.2139, TOTAL: 0.4152\n",
      "CE: 0.3331, OBJ: 0.2763, TOTAL: 0.3217\n",
      "CE: 0.3303, OBJ: 0.2848, TOTAL: 0.3212\n",
      "CE: 0.1641, OBJ: 0.2975, TOTAL: 0.1907\n",
      "CE: 0.3924, OBJ: 0.2971, TOTAL: 0.3734\n",
      "CE: 0.2233, OBJ: 0.2273, TOTAL: 0.2241\n",
      "CE: 0.1449, OBJ: 0.1563, TOTAL: 0.1472\n",
      "CE: 0.4038, OBJ: 0.3148, TOTAL: 0.3860\n",
      "CE: 0.6307, OBJ: 0.3844, TOTAL: 0.5814\n",
      "CE: 0.5103, OBJ: 0.2917, TOTAL: 0.4665\n",
      "CE: 0.2050, OBJ: 0.3212, TOTAL: 0.2282\n",
      "CE: 0.1875, OBJ: 0.2044, TOTAL: 0.1909\n",
      "CE: 0.2749, OBJ: 0.2763, TOTAL: 0.2752\n",
      "CE: 0.3855, OBJ: 0.2227, TOTAL: 0.3529\n",
      "CE: 0.2776, OBJ: 0.3061, TOTAL: 0.2833\n",
      "CE: 0.2531, OBJ: 0.2321, TOTAL: 0.2489\n",
      "CE: 0.0344, OBJ: 0.2823, TOTAL: 0.0840\n",
      "CE: 0.1891, OBJ: 0.3500, TOTAL: 0.2212\n",
      "CE: 0.2232, OBJ: 0.3881, TOTAL: 0.2562\n",
      "CE: 0.2056, OBJ: 0.4963, TOTAL: 0.2637\n",
      "CE: 0.4326, OBJ: 0.2733, TOTAL: 0.4007\n",
      "CE: 0.1230, OBJ: 0.3947, TOTAL: 0.1774\n",
      "CE: 0.1600, OBJ: 0.4430, TOTAL: 0.2166\n",
      "CE: 0.0912, OBJ: 0.2163, TOTAL: 0.1162\n",
      "CE: 0.3075, OBJ: 0.4167, TOTAL: 0.3293\n",
      "CE: 0.1716, OBJ: 0.2650, TOTAL: 0.1903\n",
      "CE: 0.5608, OBJ: 0.3450, TOTAL: 0.5176\n",
      "CE: 0.1515, OBJ: 0.3282, TOTAL: 0.1868\n",
      "CE: 0.1981, OBJ: 0.3176, TOTAL: 0.2220\n",
      "CE: 0.4187, OBJ: 0.2344, TOTAL: 0.3818\n",
      "CE: 0.3696, OBJ: 0.4338, TOTAL: 0.3825\n",
      "CE: 0.1297, OBJ: 0.3104, TOTAL: 0.1658\n",
      "CE: 0.5291, OBJ: 0.4247, TOTAL: 0.5082\n",
      "CE: 0.4229, OBJ: 0.4493, TOTAL: 0.4282\n",
      "CE: 0.0764, OBJ: 0.3037, TOTAL: 0.1218\n",
      "CE: 0.2201, OBJ: 0.4244, TOTAL: 0.2610\n",
      "CE: 0.1923, OBJ: 0.2699, TOTAL: 0.2078\n",
      "CE: 0.4239, OBJ: 0.3168, TOTAL: 0.4025\n",
      "CE: 0.3540, OBJ: 0.3756, TOTAL: 0.3583\n",
      "CE: 0.2140, OBJ: 0.2497, TOTAL: 0.2211\n",
      "CE: 0.1514, OBJ: 0.3442, TOTAL: 0.1900\n",
      "CE: 0.1943, OBJ: 0.3710, TOTAL: 0.2296\n",
      "CE: 0.1806, OBJ: 0.3985, TOTAL: 0.2242\n",
      "CE: 0.4817, OBJ: 0.3281, TOTAL: 0.4510\n",
      "CE: 0.3108, OBJ: 0.3299, TOTAL: 0.3146\n",
      "CE: 0.2485, OBJ: 0.3295, TOTAL: 0.2647\n",
      "CE: 0.1921, OBJ: 0.3599, TOTAL: 0.2256\n",
      "CE: 0.3252, OBJ: 0.3046, TOTAL: 0.3211\n",
      "CE: 0.3932, OBJ: 0.2135, TOTAL: 0.3572\n",
      "CE: 0.5503, OBJ: 0.5306, TOTAL: 0.5463\n",
      "CE: 0.2721, OBJ: 0.3678, TOTAL: 0.2912\n",
      "CE: 0.4238, OBJ: 0.5373, TOTAL: 0.4465\n",
      "CE: 0.2444, OBJ: 0.2430, TOTAL: 0.2441\n",
      "CE: 0.1259, OBJ: 0.2833, TOTAL: 0.1573\n",
      "CE: 0.5005, OBJ: 0.2861, TOTAL: 0.4576\n",
      "CE: 0.0948, OBJ: 0.2533, TOTAL: 0.1265\n",
      "CE: 0.8470, OBJ: 0.4132, TOTAL: 0.7602\n",
      "CE: 0.1823, OBJ: 0.1677, TOTAL: 0.1794\n",
      "CE: 0.3873, OBJ: 0.2685, TOTAL: 0.3635\n",
      "CE: 0.2871, OBJ: 0.3196, TOTAL: 0.2936\n",
      "CE: 0.4930, OBJ: 0.2283, TOTAL: 0.4400\n",
      "CE: 0.1753, OBJ: 0.2323, TOTAL: 0.1867\n",
      "CE: 0.5735, OBJ: 0.3499, TOTAL: 0.5288\n",
      "CE: 0.1241, OBJ: 0.2989, TOTAL: 0.1591\n",
      "CE: 0.1405, OBJ: 0.2971, TOTAL: 0.1718\n",
      "CE: 0.2427, OBJ: 0.1557, TOTAL: 0.2253\n",
      "CE: 0.1120, OBJ: 0.3476, TOTAL: 0.1591\n",
      "CE: 0.2656, OBJ: 0.4034, TOTAL: 0.2931\n",
      "CE: 0.3187, OBJ: 0.2851, TOTAL: 0.3120\n",
      "CE: 0.4594, OBJ: 0.3920, TOTAL: 0.4459\n",
      "CE: 0.4377, OBJ: 0.4460, TOTAL: 0.4394\n",
      "CE: 0.2413, OBJ: 0.2640, TOTAL: 0.2458\n",
      "CE: 0.3995, OBJ: 0.3477, TOTAL: 0.3891\n",
      "CE: 0.3096, OBJ: 0.2335, TOTAL: 0.2944\n",
      "CE: 0.2709, OBJ: 0.4464, TOTAL: 0.3060\n",
      "CE: 0.6463, OBJ: 0.2951, TOTAL: 0.5760\n",
      "CE: 0.1644, OBJ: 0.2897, TOTAL: 0.1895\n",
      "CE: 0.4153, OBJ: 0.3934, TOTAL: 0.4109\n",
      "CE: 0.1220, OBJ: 0.2018, TOTAL: 0.1380\n",
      "CE: 0.6390, OBJ: 0.4100, TOTAL: 0.5932\n",
      "CE: 0.3442, OBJ: 0.2615, TOTAL: 0.3277\n",
      "CE: 0.2030, OBJ: 0.3475, TOTAL: 0.2319\n",
      "CE: 0.3007, OBJ: 0.1936, TOTAL: 0.2793\n",
      "CE: 0.2408, OBJ: 0.2108, TOTAL: 0.2348\n",
      "CE: 0.2165, OBJ: 0.3501, TOTAL: 0.2432\n",
      "CE: 0.1451, OBJ: 0.2984, TOTAL: 0.1758\n",
      "CE: 0.5976, OBJ: 0.3098, TOTAL: 0.5400\n",
      "CE: 0.2243, OBJ: 0.2900, TOTAL: 0.2374\n",
      "CE: 0.2296, OBJ: 0.3888, TOTAL: 0.2615\n",
      "CE: 0.1437, OBJ: 0.2742, TOTAL: 0.1698\n",
      "CE: 0.3645, OBJ: 0.2587, TOTAL: 0.3433\n",
      "CE: 0.4914, OBJ: 0.2655, TOTAL: 0.4462\n",
      "CE: 0.3311, OBJ: 0.2721, TOTAL: 0.3193\n",
      "CE: 0.1988, OBJ: 0.2718, TOTAL: 0.2134\n",
      "CE: 0.0339, OBJ: 0.2842, TOTAL: 0.0840\n",
      "CE: 0.1027, OBJ: 0.2279, TOTAL: 0.1277\n",
      "CE: 0.0584, OBJ: 0.2235, TOTAL: 0.0914\n",
      "CE: 0.1607, OBJ: 0.3339, TOTAL: 0.1953\n",
      "CE: 0.3099, OBJ: 0.2419, TOTAL: 0.2963\n",
      "CE: 0.3224, OBJ: 0.2562, TOTAL: 0.3092\n",
      "CE: 0.2441, OBJ: 0.3242, TOTAL: 0.2602\n",
      "CE: 0.4214, OBJ: 0.4778, TOTAL: 0.4327\n",
      "CE: 0.6735, OBJ: 0.3392, TOTAL: 0.6066\n",
      "CE: 0.4566, OBJ: 0.2527, TOTAL: 0.4158\n",
      "CE: 0.2942, OBJ: 0.5377, TOTAL: 0.3429\n",
      "CE: 0.2913, OBJ: 0.2870, TOTAL: 0.2904\n",
      "CE: 0.2883, OBJ: 0.2038, TOTAL: 0.2714\n",
      "CE: 0.3741, OBJ: 0.3091, TOTAL: 0.3611\n",
      "CE: 0.3908, OBJ: 0.2320, TOTAL: 0.3590\n",
      "CE: 0.4516, OBJ: 0.3423, TOTAL: 0.4298\n",
      "CE: 0.1098, OBJ: 0.2706, TOTAL: 0.1420\n",
      "CE: 0.4683, OBJ: 0.3295, TOTAL: 0.4405\n",
      "CE: 0.7159, OBJ: 0.3307, TOTAL: 0.6388\n",
      "CE: 0.1011, OBJ: 0.2608, TOTAL: 0.1330\n",
      "CE: 0.0901, OBJ: 0.2125, TOTAL: 0.1146\n",
      "CE: 0.2714, OBJ: 0.3096, TOTAL: 0.2791\n",
      "CE: 0.5573, OBJ: 0.3152, TOTAL: 0.5089\n",
      "CE: 0.1985, OBJ: 0.3254, TOTAL: 0.2238\n",
      "CE: 0.2442, OBJ: 0.3190, TOTAL: 0.2591\n",
      "CE: 0.0895, OBJ: 0.2054, TOTAL: 0.1127\n",
      "CE: 0.2409, OBJ: 0.3203, TOTAL: 0.2568\n",
      "CE: 0.6557, OBJ: 0.3661, TOTAL: 0.5978\n",
      "CE: 0.4515, OBJ: 0.3169, TOTAL: 0.4246\n",
      "CE: 0.4511, OBJ: 0.2557, TOTAL: 0.4120\n",
      "CE: 0.4074, OBJ: 0.4086, TOTAL: 0.4077\n",
      "CE: 0.2204, OBJ: 0.3402, TOTAL: 0.2444\n",
      "CE: 0.3316, OBJ: 0.2956, TOTAL: 0.3244\n",
      "CE: 0.3201, OBJ: 0.4819, TOTAL: 0.3525\n",
      "CE: 0.4175, OBJ: 0.2616, TOTAL: 0.3863\n",
      "CE: 0.0480, OBJ: 0.2090, TOTAL: 0.0802\n",
      "CE: 0.7812, OBJ: 0.4594, TOTAL: 0.7168\n",
      "CE: 0.1789, OBJ: 0.2954, TOTAL: 0.2022\n",
      "CE: 0.0648, OBJ: 0.1838, TOTAL: 0.0886\n",
      "CE: 0.1704, OBJ: 0.2325, TOTAL: 0.1828\n",
      "CE: 0.2000, OBJ: 0.2668, TOTAL: 0.2134\n",
      "CE: 0.4361, OBJ: 0.2222, TOTAL: 0.3933\n",
      "CE: 0.1510, OBJ: 0.2263, TOTAL: 0.1660\n",
      "CE: 0.2199, OBJ: 0.2566, TOTAL: 0.2272\n",
      "CE: 0.3640, OBJ: 0.3348, TOTAL: 0.3582\n",
      "CE: 0.1248, OBJ: 0.2728, TOTAL: 0.1544\n",
      "CE: 0.3595, OBJ: 0.4102, TOTAL: 0.3697\n",
      "CE: 0.2878, OBJ: 0.3654, TOTAL: 0.3033\n",
      "CE: 0.5021, OBJ: 0.3724, TOTAL: 0.4761\n",
      "CE: 0.2991, OBJ: 0.3887, TOTAL: 0.3170\n",
      "CE: 0.3534, OBJ: 0.4167, TOTAL: 0.3661\n",
      "CE: 0.5083, OBJ: 0.3210, TOTAL: 0.4708\n",
      "CE: 0.5172, OBJ: 0.2499, TOTAL: 0.4637\n",
      "CE: 0.2248, OBJ: 0.2753, TOTAL: 0.2349\n",
      "CE: 0.2447, OBJ: 0.2735, TOTAL: 0.2505\n",
      "CE: 0.2462, OBJ: 0.3348, TOTAL: 0.2640\n",
      "CE: 0.3367, OBJ: 0.2639, TOTAL: 0.3221\n",
      "CE: 0.1860, OBJ: 0.2822, TOTAL: 0.2052\n",
      "CE: 0.4291, OBJ: 0.3206, TOTAL: 0.4074\n",
      "CE: 0.3355, OBJ: 0.2370, TOTAL: 0.3158\n",
      "CE: 0.0910, OBJ: 0.3100, TOTAL: 0.1348\n",
      "CE: 0.6256, OBJ: 0.3687, TOTAL: 0.5743\n",
      "CE: 0.4827, OBJ: 0.4100, TOTAL: 0.4681\n",
      "CE: 0.3447, OBJ: 0.4240, TOTAL: 0.3606\n",
      "CE: 0.2066, OBJ: 0.3123, TOTAL: 0.2277\n",
      "CE: 0.2018, OBJ: 0.3118, TOTAL: 0.2238\n",
      "CE: 0.5393, OBJ: 0.3727, TOTAL: 0.5060\n",
      "CE: 0.1806, OBJ: 0.2628, TOTAL: 0.1971\n",
      "CE: 0.1537, OBJ: 0.2947, TOTAL: 0.1819\n",
      "CE: 0.2892, OBJ: 0.3566, TOTAL: 0.3027\n",
      "CE: 0.2870, OBJ: 0.3680, TOTAL: 0.3032\n",
      "CE: 0.6852, OBJ: 0.3255, TOTAL: 0.6132\n",
      "CE: 0.3694, OBJ: 0.2880, TOTAL: 0.3531\n",
      "CE: 0.2049, OBJ: 0.2818, TOTAL: 0.2203\n",
      "CE: 0.4733, OBJ: 0.2723, TOTAL: 0.4331\n",
      "CE: 0.5875, OBJ: 0.1951, TOTAL: 0.5090\n",
      "CE: 0.4158, OBJ: 0.3407, TOTAL: 0.4008\n",
      "CE: 0.1053, OBJ: 0.2096, TOTAL: 0.1262\n",
      "CE: 0.1010, OBJ: 0.3330, TOTAL: 0.1474\n",
      "CE: 0.2920, OBJ: 0.1563, TOTAL: 0.2649\n",
      "CE: 0.4691, OBJ: 0.2692, TOTAL: 0.4292\n",
      "CE: 0.4610, OBJ: 0.3111, TOTAL: 0.4310\n",
      "CE: 0.2287, OBJ: 0.2640, TOTAL: 0.2358\n",
      "CE: 0.4253, OBJ: 0.4456, TOTAL: 0.4293\n",
      "CE: 0.1090, OBJ: 0.3547, TOTAL: 0.1582\n",
      "CE: 0.3452, OBJ: 0.3053, TOTAL: 0.3372\n",
      "CE: 0.2334, OBJ: 0.2830, TOTAL: 0.2433\n",
      "CE: 0.3747, OBJ: 0.2252, TOTAL: 0.3448\n",
      "CE: 0.7562, OBJ: 0.4268, TOTAL: 0.6903\n",
      "CE: 0.3394, OBJ: 0.3083, TOTAL: 0.3332\n",
      "CE: 0.3904, OBJ: 0.4069, TOTAL: 0.3937\n",
      "CE: 0.3071, OBJ: 0.3810, TOTAL: 0.3219\n",
      "CE: 0.2784, OBJ: 0.2627, TOTAL: 0.2752\n",
      "CE: 0.1896, OBJ: 0.2990, TOTAL: 0.2115\n",
      "CE: 0.6028, OBJ: 0.2596, TOTAL: 0.5342\n",
      "CE: 0.1598, OBJ: 0.3492, TOTAL: 0.1977\n",
      "CE: 0.2843, OBJ: 0.2367, TOTAL: 0.2748\n",
      "CE: 0.5946, OBJ: 0.1921, TOTAL: 0.5141\n",
      "CE: 0.2801, OBJ: 0.3502, TOTAL: 0.2941\n",
      "CE: 0.4645, OBJ: 0.3323, TOTAL: 0.4380\n",
      "CE: 0.3120, OBJ: 0.3512, TOTAL: 0.3198\n",
      "CE: 0.4627, OBJ: 0.3119, TOTAL: 0.4325\n",
      "CE: 0.6406, OBJ: 0.4481, TOTAL: 0.6021\n",
      "CE: 0.2711, OBJ: 0.2581, TOTAL: 0.2685\n",
      "CE: 0.1960, OBJ: 0.2651, TOTAL: 0.2098\n",
      "CE: 0.2565, OBJ: 0.3659, TOTAL: 0.2783\n",
      "CE: 0.0789, OBJ: 0.2492, TOTAL: 0.1130\n",
      "CE: 0.3825, OBJ: 0.3898, TOTAL: 0.3839\n",
      "CE: 0.2149, OBJ: 0.2433, TOTAL: 0.2206\n",
      "CE: 0.2252, OBJ: 0.3919, TOTAL: 0.2585\n",
      "CE: 0.3253, OBJ: 0.3536, TOTAL: 0.3310\n",
      "CE: 0.5382, OBJ: 0.3329, TOTAL: 0.4972\n",
      "CE: 0.5624, OBJ: 0.3618, TOTAL: 0.5223\n",
      "CE: 0.3379, OBJ: 0.4289, TOTAL: 0.3561\n",
      "CE: 0.1021, OBJ: 0.1820, TOTAL: 0.1181\n",
      "CE: 0.1869, OBJ: 0.4271, TOTAL: 0.2349\n",
      "CE: 0.7101, OBJ: 0.3102, TOTAL: 0.6301\n",
      "CE: 0.2743, OBJ: 0.3985, TOTAL: 0.2992\n",
      "CE: 0.2431, OBJ: 0.3235, TOTAL: 0.2592\n",
      "CE: 0.1705, OBJ: 0.3218, TOTAL: 0.2007\n",
      "CE: 0.0953, OBJ: 0.2879, TOTAL: 0.1338\n",
      "CE: 0.2130, OBJ: 0.3604, TOTAL: 0.2425\n",
      "CE: 0.1568, OBJ: 0.3136, TOTAL: 0.1882\n",
      "CE: 0.8070, OBJ: 0.3820, TOTAL: 0.7220\n",
      "CE: 0.4133, OBJ: 0.3301, TOTAL: 0.3967\n",
      "CE: 0.1340, OBJ: 0.1170, TOTAL: 0.1306\n",
      "CE: 0.1956, OBJ: 0.2602, TOTAL: 0.2085\n",
      "CE: 0.2905, OBJ: 0.4015, TOTAL: 0.3127\n",
      "CE: 0.3234, OBJ: 0.4487, TOTAL: 0.3485\n",
      "CE: 0.2590, OBJ: 0.2718, TOTAL: 0.2616\n",
      "CE: 0.3281, OBJ: 0.2559, TOTAL: 0.3137\n",
      "CE: 0.1858, OBJ: 0.3024, TOTAL: 0.2091\n",
      "CE: 0.4893, OBJ: 0.2913, TOTAL: 0.4497\n",
      "CE: 0.9552, OBJ: 0.3183, TOTAL: 0.8278\n",
      "CE: 0.4196, OBJ: 0.3355, TOTAL: 0.4028\n",
      "CE: 0.3278, OBJ: 0.3546, TOTAL: 0.3331\n",
      "CE: 0.3120, OBJ: 0.3292, TOTAL: 0.3154\n",
      "CE: 0.1829, OBJ: 0.4368, TOTAL: 0.2337\n",
      "CE: 0.0711, OBJ: 0.2727, TOTAL: 0.1114\n",
      "CE: 0.4661, OBJ: 0.4626, TOTAL: 0.4654\n",
      "CE: 0.4461, OBJ: 0.2345, TOTAL: 0.4038\n",
      "CE: 0.1608, OBJ: 0.3214, TOTAL: 0.1929\n",
      "CE: 0.1016, OBJ: 0.1883, TOTAL: 0.1189\n",
      "CE: 0.2856, OBJ: 0.4182, TOTAL: 0.3121\n",
      "CE: 0.1926, OBJ: 0.2844, TOTAL: 0.2110\n",
      "CE: 0.8754, OBJ: 0.2640, TOTAL: 0.7531\n",
      "CE: 0.4637, OBJ: 0.4109, TOTAL: 0.4532\n",
      "CE: 0.1792, OBJ: 0.4434, TOTAL: 0.2321\n",
      "CE: 0.0697, OBJ: 0.2577, TOTAL: 0.1073\n",
      "CE: 0.1262, OBJ: 0.3505, TOTAL: 0.1711\n",
      "CE: 0.4438, OBJ: 0.3508, TOTAL: 0.4252\n",
      "CE: 0.3509, OBJ: 0.3438, TOTAL: 0.3495\n",
      "CE: 0.5189, OBJ: 0.2411, TOTAL: 0.4633\n",
      "CE: 0.3282, OBJ: 0.2488, TOTAL: 0.3123\n",
      "CE: 0.3632, OBJ: 0.3494, TOTAL: 0.3605\n",
      "CE: 0.1965, OBJ: 0.4442, TOTAL: 0.2460\n",
      "CE: 0.3168, OBJ: 0.3356, TOTAL: 0.3205\n",
      "CE: 0.2703, OBJ: 0.3024, TOTAL: 0.2767\n",
      "CE: 0.3596, OBJ: 0.1804, TOTAL: 0.3237\n",
      "CE: 0.1401, OBJ: 0.2437, TOTAL: 0.1608\n",
      "CE: 0.4386, OBJ: 0.3008, TOTAL: 0.4110\n",
      "CE: 0.2894, OBJ: 0.2889, TOTAL: 0.2893\n",
      "CE: 0.3350, OBJ: 0.2364, TOTAL: 0.3153\n",
      "CE: 0.3149, OBJ: 0.3018, TOTAL: 0.3123\n",
      "CE: 0.2240, OBJ: 0.3526, TOTAL: 0.2497\n",
      "CE: 0.2024, OBJ: 0.3773, TOTAL: 0.2373\n",
      "CE: 0.5896, OBJ: 0.2576, TOTAL: 0.5232\n",
      "CE: 0.3626, OBJ: 0.2617, TOTAL: 0.3425\n",
      "CE: 0.1764, OBJ: 0.3670, TOTAL: 0.2145\n",
      "CE: 0.2537, OBJ: 0.3228, TOTAL: 0.2675\n",
      "CE: 0.4033, OBJ: 0.3081, TOTAL: 0.3842\n",
      "CE: 0.2418, OBJ: 0.3426, TOTAL: 0.2620\n",
      "CE: 0.0549, OBJ: 0.2651, TOTAL: 0.0969\n",
      "CE: 0.3445, OBJ: 0.3298, TOTAL: 0.3416\n",
      "CE: 0.2456, OBJ: 0.4654, TOTAL: 0.2896\n",
      "CE: 0.3127, OBJ: 0.2214, TOTAL: 0.2945\n",
      "CE: 0.2555, OBJ: 0.2829, TOTAL: 0.2610\n",
      "CE: 0.5056, OBJ: 0.3291, TOTAL: 0.4703\n",
      "CE: 0.4317, OBJ: 0.3738, TOTAL: 0.4201\n",
      "CE: 0.1767, OBJ: 0.4008, TOTAL: 0.2216\n",
      "CE: 0.2551, OBJ: 0.3091, TOTAL: 0.2659\n",
      "CE: 0.2609, OBJ: 0.2319, TOTAL: 0.2551\n",
      "CE: 0.1793, OBJ: 0.1924, TOTAL: 0.1820\n",
      "CE: 0.3943, OBJ: 0.3160, TOTAL: 0.3786\n",
      "CE: 0.1818, OBJ: 0.3347, TOTAL: 0.2123\n",
      "CE: 0.0612, OBJ: 0.2080, TOTAL: 0.0906\n",
      "CE: 0.2713, OBJ: 0.4425, TOTAL: 0.3055\n",
      "CE: 0.2250, OBJ: 0.2852, TOTAL: 0.2370\n",
      "CE: 0.3814, OBJ: 0.2937, TOTAL: 0.3638\n",
      "CE: 0.4466, OBJ: 0.4019, TOTAL: 0.4377\n",
      "CE: 0.1699, OBJ: 0.3368, TOTAL: 0.2033\n",
      "CE: 0.1868, OBJ: 0.3780, TOTAL: 0.2250\n",
      "CE: 0.3150, OBJ: 0.2520, TOTAL: 0.3024\n",
      "CE: 0.3280, OBJ: 0.4020, TOTAL: 0.3428\n",
      "CE: 0.4245, OBJ: 0.3441, TOTAL: 0.4085\n",
      "CE: 0.3743, OBJ: 0.3297, TOTAL: 0.3654\n",
      "CE: 0.1854, OBJ: 0.2693, TOTAL: 0.2021\n",
      "CE: 0.1337, OBJ: 0.3807, TOTAL: 0.1831\n",
      "CE: 0.3968, OBJ: 0.2290, TOTAL: 0.3632\n",
      "CE: 0.2607, OBJ: 0.2264, TOTAL: 0.2538\n",
      "CE: 0.2900, OBJ: 0.3353, TOTAL: 0.2990\n",
      "CE: 0.0900, OBJ: 0.3244, TOTAL: 0.1369\n",
      "CE: 0.5867, OBJ: 0.5083, TOTAL: 0.5710\n",
      "CE: 0.1167, OBJ: 0.1881, TOTAL: 0.1310\n",
      "CE: 0.2869, OBJ: 0.4402, TOTAL: 0.3175\n",
      "CE: 0.0943, OBJ: 0.3273, TOTAL: 0.1409\n",
      "CE: 0.0951, OBJ: 0.2610, TOTAL: 0.1283\n",
      "CE: 0.0746, OBJ: 0.2287, TOTAL: 0.1054\n",
      "CE: 0.5233, OBJ: 0.3765, TOTAL: 0.4940\n",
      "CE: 0.5128, OBJ: 0.3180, TOTAL: 0.4739\n",
      "CE: 0.6337, OBJ: 0.3410, TOTAL: 0.5752\n",
      "CE: 0.1179, OBJ: 0.2468, TOTAL: 0.1437\n",
      "CE: 0.3516, OBJ: 0.2797, TOTAL: 0.3372\n",
      "CE: 0.4723, OBJ: 0.5245, TOTAL: 0.4827\n",
      "CE: 0.2617, OBJ: 0.5047, TOTAL: 0.3103\n",
      "CE: 0.3116, OBJ: 0.3747, TOTAL: 0.3242\n",
      "CE: 0.2952, OBJ: 0.2363, TOTAL: 0.2834\n",
      "CE: 0.3487, OBJ: 0.3676, TOTAL: 0.3525\n",
      "CE: 0.4658, OBJ: 0.2161, TOTAL: 0.4159\n",
      "CE: 0.4254, OBJ: 0.3294, TOTAL: 0.4062\n",
      "CE: 0.1544, OBJ: 0.2242, TOTAL: 0.1684\n",
      "CE: 0.2202, OBJ: 0.1533, TOTAL: 0.2068\n",
      "CE: 0.6092, OBJ: 0.3848, TOTAL: 0.5643\n",
      "CE: 0.3385, OBJ: 0.3731, TOTAL: 0.3455\n",
      "CE: 0.2442, OBJ: 0.4079, TOTAL: 0.2769\n",
      "CE: 0.1538, OBJ: 0.3158, TOTAL: 0.1862\n",
      "CE: 0.2503, OBJ: 0.4510, TOTAL: 0.2905\n",
      "CE: 0.2057, OBJ: 0.2345, TOTAL: 0.2115\n",
      "CE: 0.1885, OBJ: 0.2790, TOTAL: 0.2066\n",
      "CE: 0.3121, OBJ: 0.3782, TOTAL: 0.3253\n",
      "CE: 0.0783, OBJ: 0.1909, TOTAL: 0.1008\n",
      "CE: 0.2096, OBJ: 0.3941, TOTAL: 0.2465\n",
      "CE: 0.4198, OBJ: 0.2750, TOTAL: 0.3908\n",
      "CE: 0.2070, OBJ: 0.4245, TOTAL: 0.2505\n",
      "CE: 0.2542, OBJ: 0.2700, TOTAL: 0.2573\n",
      "CE: 0.1149, OBJ: 0.2926, TOTAL: 0.1504\n",
      "CE: 0.2619, OBJ: 0.4026, TOTAL: 0.2900\n",
      "CE: 0.2621, OBJ: 0.3099, TOTAL: 0.2716\n",
      "CE: 0.6423, OBJ: 0.3543, TOTAL: 0.5847\n",
      "CE: 0.2282, OBJ: 0.3744, TOTAL: 0.2575\n",
      "CE: 0.1896, OBJ: 0.2719, TOTAL: 0.2060\n",
      "CE: 0.2010, OBJ: 0.3507, TOTAL: 0.2309\n",
      "CE: 0.4279, OBJ: 0.3612, TOTAL: 0.4146\n",
      "CE: 0.3470, OBJ: 0.2844, TOTAL: 0.3345\n",
      "CE: 0.2250, OBJ: 0.3326, TOTAL: 0.2465\n",
      "CE: 0.1803, OBJ: 0.2369, TOTAL: 0.1916\n",
      "CE: 0.2808, OBJ: 0.2634, TOTAL: 0.2773\n",
      "CE: 0.1402, OBJ: 0.3808, TOTAL: 0.1883\n",
      "CE: 0.4534, OBJ: 0.2974, TOTAL: 0.4222\n",
      "CE: 0.7026, OBJ: 0.4170, TOTAL: 0.6455\n",
      "CE: 0.2820, OBJ: 0.2772, TOTAL: 0.2810\n",
      "CE: 0.1818, OBJ: 0.2719, TOTAL: 0.1999\n",
      "CE: 0.1409, OBJ: 0.3135, TOTAL: 0.1754\n",
      "CE: 0.2752, OBJ: 0.4627, TOTAL: 0.3127\n",
      "CE: 0.4449, OBJ: 0.3458, TOTAL: 0.4251\n",
      "CE: 0.1709, OBJ: 0.3757, TOTAL: 0.2119\n",
      "CE: 0.2344, OBJ: 0.5178, TOTAL: 0.2911\n",
      "CE: 0.2898, OBJ: 0.2886, TOTAL: 0.2896\n",
      "CE: 0.3193, OBJ: 0.4280, TOTAL: 0.3410\n",
      "CE: 0.6513, OBJ: 0.4032, TOTAL: 0.6017\n",
      "CE: 0.2585, OBJ: 0.1749, TOTAL: 0.2418\n",
      "CE: 0.1759, OBJ: 0.3322, TOTAL: 0.2072\n",
      "CE: 0.2256, OBJ: 0.3305, TOTAL: 0.2466\n",
      "CE: 0.2203, OBJ: 0.3364, TOTAL: 0.2435\n",
      "CE: 0.5342, OBJ: 0.1973, TOTAL: 0.4668\n",
      "CE: 0.2915, OBJ: 0.2359, TOTAL: 0.2804\n",
      "CE: 0.1533, OBJ: 0.3775, TOTAL: 0.1981\n",
      "CE: 0.2013, OBJ: 0.3627, TOTAL: 0.2336\n",
      "CE: 0.2884, OBJ: 0.3787, TOTAL: 0.3065\n",
      "CE: 0.2888, OBJ: 0.2296, TOTAL: 0.2769\n",
      "CE: 0.2678, OBJ: 0.3332, TOTAL: 0.2809\n",
      "CE: 0.3774, OBJ: 0.3575, TOTAL: 0.3734\n",
      "CE: 0.2300, OBJ: 0.4196, TOTAL: 0.2679\n",
      "CE: 0.1745, OBJ: 0.3457, TOTAL: 0.2087\n",
      "CE: 0.5169, OBJ: 0.2556, TOTAL: 0.4646\n",
      "CE: 0.1660, OBJ: 0.2743, TOTAL: 0.1876\n",
      "CE: 0.3427, OBJ: 0.3929, TOTAL: 0.3527\n",
      "CE: 0.0526, OBJ: 0.2543, TOTAL: 0.0930\n",
      "CE: 0.2497, OBJ: 0.3535, TOTAL: 0.2705\n",
      "CE: 0.2364, OBJ: 0.2999, TOTAL: 0.2491\n",
      "CE: 0.1871, OBJ: 0.2286, TOTAL: 0.1954\n",
      "CE: 0.4963, OBJ: 0.3094, TOTAL: 0.4589\n",
      "CE: 0.2365, OBJ: 0.2740, TOTAL: 0.2440\n",
      "CE: 0.1395, OBJ: 0.1366, TOTAL: 0.1389\n",
      "CE: 0.4328, OBJ: 0.3958, TOTAL: 0.4254\n",
      "CE: 0.3920, OBJ: 0.3195, TOTAL: 0.3775\n",
      "CE: 0.1744, OBJ: 0.2919, TOTAL: 0.1979\n",
      "CE: 0.3442, OBJ: 0.4275, TOTAL: 0.3609\n",
      "CE: 0.4685, OBJ: 0.3467, TOTAL: 0.4441\n",
      "CE: 0.3203, OBJ: 0.3775, TOTAL: 0.3317\n",
      "CE: 0.1101, OBJ: 0.2810, TOTAL: 0.1443\n",
      "CE: 0.0643, OBJ: 0.1871, TOTAL: 0.0889\n",
      "CE: 0.2563, OBJ: 0.3133, TOTAL: 0.2677\n",
      "CE: 0.0769, OBJ: 0.1916, TOTAL: 0.0998\n",
      "CE: 0.4061, OBJ: 0.3069, TOTAL: 0.3862\n",
      "CE: 0.4763, OBJ: 0.3117, TOTAL: 0.4434\n",
      "CE: 0.3141, OBJ: 0.3367, TOTAL: 0.3187\n",
      "CE: 0.0965, OBJ: 0.3253, TOTAL: 0.1422\n",
      "CE: 0.2051, OBJ: 0.4317, TOTAL: 0.2504\n",
      "CE: 0.4019, OBJ: 0.5138, TOTAL: 0.4243\n",
      "CE: 0.3942, OBJ: 0.2281, TOTAL: 0.3610\n",
      "CE: 0.0740, OBJ: 0.2651, TOTAL: 0.1122\n",
      "CE: 0.3969, OBJ: 0.3247, TOTAL: 0.3825\n",
      "CE: 0.4178, OBJ: 0.3299, TOTAL: 0.4002\n",
      "CE: 0.5272, OBJ: 0.3154, TOTAL: 0.4848\n",
      "CE: 0.1695, OBJ: 0.2727, TOTAL: 0.1901\n",
      "CE: 0.9011, OBJ: 0.3621, TOTAL: 0.7933\n",
      "CE: 0.0911, OBJ: 0.2866, TOTAL: 0.1302\n",
      "CE: 0.1896, OBJ: 0.1836, TOTAL: 0.1884\n",
      "CE: 0.3861, OBJ: 0.4076, TOTAL: 0.3904\n",
      "CE: 0.2068, OBJ: 0.3339, TOTAL: 0.2322\n",
      "CE: 0.2764, OBJ: 0.3999, TOTAL: 0.3011\n",
      "CE: 0.6403, OBJ: 0.3818, TOTAL: 0.5886\n",
      "CE: 0.9958, OBJ: 0.2929, TOTAL: 0.8552\n",
      "CE: 0.2307, OBJ: 0.4028, TOTAL: 0.2651\n",
      "CE: 0.1766, OBJ: 0.3535, TOTAL: 0.2119\n",
      "CE: 0.3390, OBJ: 0.2211, TOTAL: 0.3154\n",
      "CE: 0.2492, OBJ: 0.3884, TOTAL: 0.2771\n",
      "CE: 0.3029, OBJ: 0.2973, TOTAL: 0.3018\n",
      "CE: 0.2725, OBJ: 0.3679, TOTAL: 0.2916\n",
      "CE: 0.3777, OBJ: 0.3035, TOTAL: 0.3629\n",
      "CE: 0.1821, OBJ: 0.2857, TOTAL: 0.2028\n",
      "CE: 0.2672, OBJ: 0.3694, TOTAL: 0.2877\n",
      "CE: 0.6814, OBJ: 0.3431, TOTAL: 0.6138\n",
      "CE: 0.3135, OBJ: 0.2628, TOTAL: 0.3033\n",
      "CE: 0.2836, OBJ: 0.3975, TOTAL: 0.3064\n",
      "CE: 0.3936, OBJ: 0.1934, TOTAL: 0.3535\n",
      "CE: 0.3039, OBJ: 0.1781, TOTAL: 0.2788\n",
      "CE: 0.3848, OBJ: 0.4468, TOTAL: 0.3972\n",
      "CE: 0.2209, OBJ: 0.3583, TOTAL: 0.2484\n",
      "CE: 0.2165, OBJ: 0.1923, TOTAL: 0.2116\n",
      "CE: 0.1201, OBJ: 0.2871, TOTAL: 0.1535\n",
      "CE: 0.4495, OBJ: 0.2227, TOTAL: 0.4041\n",
      "CE: 0.2880, OBJ: 0.3731, TOTAL: 0.3050\n",
      "CE: 0.2778, OBJ: 0.2639, TOTAL: 0.2751\n",
      "CE: 0.4624, OBJ: 0.3522, TOTAL: 0.4404\n",
      "CE: 0.2999, OBJ: 0.4126, TOTAL: 0.3224\n",
      "CE: 0.2797, OBJ: 0.2994, TOTAL: 0.2837\n",
      "CE: 0.3790, OBJ: 0.4139, TOTAL: 0.3860\n",
      "CE: 0.1109, OBJ: 0.2748, TOTAL: 0.1437\n",
      "CE: 0.3907, OBJ: 0.2985, TOTAL: 0.3723\n",
      "CE: 0.4210, OBJ: 0.2898, TOTAL: 0.3947\n",
      "CE: 0.1904, OBJ: 0.2867, TOTAL: 0.2097\n",
      "CE: 0.3291, OBJ: 0.4493, TOTAL: 0.3531\n",
      "CE: 0.2442, OBJ: 0.2666, TOTAL: 0.2487\n",
      "CE: 0.1931, OBJ: 0.4428, TOTAL: 0.2430\n",
      "CE: 0.2466, OBJ: 0.2138, TOTAL: 0.2400\n",
      "CE: 0.2002, OBJ: 0.1875, TOTAL: 0.1976\n",
      "CE: 0.6442, OBJ: 0.3867, TOTAL: 0.5927\n",
      "CE: 0.3454, OBJ: 0.2692, TOTAL: 0.3302\n",
      "CE: 0.5263, OBJ: 0.2981, TOTAL: 0.4807\n",
      "CE: 0.4038, OBJ: 0.3250, TOTAL: 0.3880\n",
      "CE: 0.0949, OBJ: 0.2932, TOTAL: 0.1346\n",
      "CE: 0.1996, OBJ: 0.3199, TOTAL: 0.2237\n",
      "CE: 0.3334, OBJ: 0.3186, TOTAL: 0.3305\n",
      "CE: 0.2738, OBJ: 0.1848, TOTAL: 0.2560\n",
      "CE: 0.2073, OBJ: 0.3401, TOTAL: 0.2339\n",
      "CE: 0.4633, OBJ: 0.2807, TOTAL: 0.4268\n",
      "CE: 0.3230, OBJ: 0.2937, TOTAL: 0.3171\n",
      "CE: 0.2273, OBJ: 0.4647, TOTAL: 0.2748\n",
      "CE: 0.3132, OBJ: 0.3919, TOTAL: 0.3289\n",
      "CE: 0.2179, OBJ: 0.3016, TOTAL: 0.2346\n",
      "CE: 0.1087, OBJ: 0.2821, TOTAL: 0.1434\n",
      "CE: 0.1333, OBJ: 0.3451, TOTAL: 0.1756\n",
      "CE: 0.3225, OBJ: 0.3410, TOTAL: 0.3262\n",
      "CE: 0.0673, OBJ: 0.2030, TOTAL: 0.0944\n",
      "CE: 0.6539, OBJ: 0.3731, TOTAL: 0.5978\n",
      "CE: 0.6021, OBJ: 0.2986, TOTAL: 0.5414\n",
      "CE: 0.2383, OBJ: 0.2260, TOTAL: 0.2358\n",
      "CE: 0.1737, OBJ: 0.2876, TOTAL: 0.1965\n",
      "CE: 0.0287, OBJ: 0.3231, TOTAL: 0.0876\n",
      "CE: 0.2076, OBJ: 0.4951, TOTAL: 0.2651\n",
      "CE: 0.2366, OBJ: 0.4223, TOTAL: 0.2737\n",
      "CE: 0.3087, OBJ: 0.2561, TOTAL: 0.2982\n",
      "CE: 0.2909, OBJ: 0.4164, TOTAL: 0.3160\n",
      "CE: 0.2765, OBJ: 0.3740, TOTAL: 0.2960\n",
      "CE: 0.3432, OBJ: 0.4950, TOTAL: 0.3736\n",
      "CE: 0.3480, OBJ: 0.2627, TOTAL: 0.3309\n",
      "CE: 0.2957, OBJ: 0.2477, TOTAL: 0.2861\n",
      "CE: 0.2568, OBJ: 0.2350, TOTAL: 0.2525\n",
      "CE: 0.2208, OBJ: 0.2870, TOTAL: 0.2340\n",
      "CE: 0.2320, OBJ: 0.2697, TOTAL: 0.2395\n",
      "CE: 0.4113, OBJ: 0.2865, TOTAL: 0.3863\n",
      "CE: 0.4380, OBJ: 0.3023, TOTAL: 0.4109\n",
      "CE: 0.2029, OBJ: 0.3959, TOTAL: 0.2415\n",
      "CE: 0.6828, OBJ: 0.3489, TOTAL: 0.6160\n",
      "CE: 0.6787, OBJ: 0.3005, TOTAL: 0.6031\n",
      "CE: 0.4245, OBJ: 0.2294, TOTAL: 0.3855\n",
      "CE: 0.5043, OBJ: 0.2952, TOTAL: 0.4625\n",
      "CE: 0.2562, OBJ: 0.2525, TOTAL: 0.2554\n",
      "CE: 0.3057, OBJ: 0.4364, TOTAL: 0.3318\n",
      "CE: 0.4139, OBJ: 0.3133, TOTAL: 0.3938\n",
      "CE: 0.2878, OBJ: 0.2968, TOTAL: 0.2896\n",
      "CE: 0.3292, OBJ: 0.4110, TOTAL: 0.3456\n",
      "CE: 0.2621, OBJ: 0.2940, TOTAL: 0.2685\n",
      "CE: 0.5175, OBJ: 0.3034, TOTAL: 0.4747\n",
      "CE: 0.1965, OBJ: 0.3394, TOTAL: 0.2251\n",
      "CE: 0.3603, OBJ: 0.3386, TOTAL: 0.3559\n",
      "CE: 0.6291, OBJ: 0.4174, TOTAL: 0.5868\n",
      "CE: 0.3191, OBJ: 0.2608, TOTAL: 0.3075\n",
      "CE: 0.2222, OBJ: 0.3901, TOTAL: 0.2558\n",
      "CE: 0.2323, OBJ: 0.3844, TOTAL: 0.2627\n",
      "CE: 0.3716, OBJ: 0.2910, TOTAL: 0.3555\n",
      "CE: 0.3386, OBJ: 0.4412, TOTAL: 0.3591\n",
      "CE: 0.2073, OBJ: 0.2244, TOTAL: 0.2107\n",
      "CE: 0.3022, OBJ: 0.3774, TOTAL: 0.3172\n",
      "CE: 0.2769, OBJ: 0.3464, TOTAL: 0.2908\n",
      "CE: 0.1405, OBJ: 0.1806, TOTAL: 0.1485\n",
      "CE: 0.2314, OBJ: 0.1453, TOTAL: 0.2142\n",
      "CE: 0.1911, OBJ: 0.2551, TOTAL: 0.2039\n",
      "CE: 0.1834, OBJ: 0.3343, TOTAL: 0.2136\n",
      "CE: 0.0742, OBJ: 0.3288, TOTAL: 0.1252\n",
      "CE: 0.1966, OBJ: 0.2413, TOTAL: 0.2056\n",
      "CE: 0.2111, OBJ: 0.1932, TOTAL: 0.2075\n",
      "CE: 0.5035, OBJ: 0.4889, TOTAL: 0.5006\n",
      "CE: 0.3748, OBJ: 0.2899, TOTAL: 0.3578\n",
      "CE: 0.3796, OBJ: 0.2880, TOTAL: 0.3613\n",
      "CE: 0.5570, OBJ: 0.4974, TOTAL: 0.5451\n",
      "CE: 0.4864, OBJ: 0.2784, TOTAL: 0.4448\n",
      "CE: 0.2909, OBJ: 0.2687, TOTAL: 0.2864\n",
      "CE: 0.1276, OBJ: 0.1999, TOTAL: 0.1421\n",
      "CE: 0.2153, OBJ: 0.2244, TOTAL: 0.2171\n",
      "CE: 0.5724, OBJ: 0.4732, TOTAL: 0.5526\n",
      "CE: 0.4150, OBJ: 0.4406, TOTAL: 0.4201\n",
      "CE: 0.1366, OBJ: 0.3143, TOTAL: 0.1721\n",
      "CE: 0.1631, OBJ: 0.3114, TOTAL: 0.1928\n",
      "CE: 0.0655, OBJ: 0.3313, TOTAL: 0.1187\n",
      "CE: 0.1326, OBJ: 0.2338, TOTAL: 0.1529\n",
      "CE: 0.1751, OBJ: 0.2037, TOTAL: 0.1808\n",
      "CE: 0.3082, OBJ: 0.3592, TOTAL: 0.3184\n",
      "CE: 0.1423, OBJ: 0.2508, TOTAL: 0.1640\n",
      "CE: 0.4415, OBJ: 0.3271, TOTAL: 0.4186\n",
      "CE: 0.3515, OBJ: 0.2864, TOTAL: 0.3385\n",
      "CE: 0.2054, OBJ: 0.3033, TOTAL: 0.2250\n",
      "CE: 0.2507, OBJ: 0.2865, TOTAL: 0.2579\n",
      "CE: 0.1879, OBJ: 0.3868, TOTAL: 0.2277\n",
      "CE: 0.3932, OBJ: 0.3551, TOTAL: 0.3856\n",
      "CE: 0.3113, OBJ: 0.4935, TOTAL: 0.3477\n",
      "CE: 0.2549, OBJ: 0.2689, TOTAL: 0.2577\n",
      "CE: 0.2488, OBJ: 0.2839, TOTAL: 0.2559\n",
      "CE: 0.1781, OBJ: 0.3298, TOTAL: 0.2084\n",
      "CE: 0.2729, OBJ: 0.2962, TOTAL: 0.2775\n",
      "CE: 0.2639, OBJ: 0.2629, TOTAL: 0.2637\n",
      "CE: 0.3444, OBJ: 0.3188, TOTAL: 0.3393\n",
      "CE: 0.1306, OBJ: 0.2048, TOTAL: 0.1454\n",
      "CE: 0.3138, OBJ: 0.2451, TOTAL: 0.3001\n",
      "CE: 0.5961, OBJ: 0.2955, TOTAL: 0.5360\n",
      "CE: 0.2272, OBJ: 0.4564, TOTAL: 0.2730\n",
      "CE: 0.3124, OBJ: 0.1294, TOTAL: 0.2758\n",
      "CE: 0.2274, OBJ: 0.1314, TOTAL: 0.2082\n",
      "CE: 0.1810, OBJ: 0.2409, TOTAL: 0.1930\n",
      "CE: 0.5043, OBJ: 0.3035, TOTAL: 0.4642\n",
      "CE: 0.4304, OBJ: 0.3266, TOTAL: 0.4097\n",
      "CE: 0.2819, OBJ: 0.3791, TOTAL: 0.3013\n",
      "CE: 0.2142, OBJ: 0.2636, TOTAL: 0.2241\n",
      "CE: 0.1974, OBJ: 0.2906, TOTAL: 0.2161\n",
      "CE: 0.4533, OBJ: 0.2568, TOTAL: 0.4140\n",
      "CE: 0.6572, OBJ: 0.3201, TOTAL: 0.5898\n",
      "CE: 0.3263, OBJ: 0.3379, TOTAL: 0.3286\n",
      "CE: 0.1168, OBJ: 0.2594, TOTAL: 0.1454\n",
      "CE: 0.5297, OBJ: 0.3175, TOTAL: 0.4873\n",
      "CE: 0.3063, OBJ: 0.2809, TOTAL: 0.3012\n",
      "CE: 0.4831, OBJ: 0.2869, TOTAL: 0.4439\n",
      "CE: 0.2278, OBJ: 0.2686, TOTAL: 0.2360\n",
      "CE: 0.1356, OBJ: 0.1751, TOTAL: 0.1435\n",
      "CE: 0.4385, OBJ: 0.3444, TOTAL: 0.4197\n",
      "CE: 0.4664, OBJ: 0.3181, TOTAL: 0.4368\n",
      "CE: 0.1979, OBJ: 0.2739, TOTAL: 0.2131\n",
      "CE: 0.2660, OBJ: 0.1549, TOTAL: 0.2438\n",
      "CE: 0.2619, OBJ: 0.3556, TOTAL: 0.2806\n",
      "CE: 0.3537, OBJ: 0.3434, TOTAL: 0.3517\n",
      "CE: 0.2828, OBJ: 0.2485, TOTAL: 0.2760\n",
      "CE: 0.4566, OBJ: 0.4854, TOTAL: 0.4624\n",
      "CE: 0.4104, OBJ: 0.3415, TOTAL: 0.3966\n",
      "CE: 0.4478, OBJ: 0.4720, TOTAL: 0.4527\n",
      "CE: 0.2889, OBJ: 0.4356, TOTAL: 0.3182\n",
      "CE: 0.6305, OBJ: 0.3358, TOTAL: 0.5716\n",
      "CE: 0.3764, OBJ: 0.4033, TOTAL: 0.3818\n",
      "CE: 0.4625, OBJ: 0.4251, TOTAL: 0.4550\n",
      "CE: 0.4881, OBJ: 0.2362, TOTAL: 0.4377\n",
      "CE: 0.3552, OBJ: 0.5086, TOTAL: 0.3858\n",
      "CE: 0.5090, OBJ: 0.3455, TOTAL: 0.4763\n",
      "CE: 0.2279, OBJ: 0.2077, TOTAL: 0.2239\n",
      "CE: 0.3140, OBJ: 0.3297, TOTAL: 0.3172\n",
      "CE: 0.1160, OBJ: 0.2734, TOTAL: 0.1475\n",
      "CE: 0.5231, OBJ: 0.4167, TOTAL: 0.5018\n",
      "CE: 0.5347, OBJ: 0.3294, TOTAL: 0.4936\n",
      "CE: 0.4253, OBJ: 0.2256, TOTAL: 0.3854\n",
      "CE: 0.1606, OBJ: 0.1623, TOTAL: 0.1610\n",
      "CE: 0.1446, OBJ: 0.2417, TOTAL: 0.1640\n",
      "CE: 0.2343, OBJ: 0.2516, TOTAL: 0.2377\n",
      "CE: 0.1472, OBJ: 0.2118, TOTAL: 0.1601\n",
      "CE: 0.3944, OBJ: 0.1915, TOTAL: 0.3538\n",
      "CE: 0.7457, OBJ: 0.3168, TOTAL: 0.6600\n",
      "CE: 0.1043, OBJ: 0.3641, TOTAL: 0.1562\n",
      "CE: 0.2897, OBJ: 0.4413, TOTAL: 0.3200\n",
      "CE: 0.1733, OBJ: 0.3040, TOTAL: 0.1994\n",
      "CE: 0.4226, OBJ: 0.2719, TOTAL: 0.3924\n",
      "CE: 0.2407, OBJ: 0.2971, TOTAL: 0.2519\n",
      "CE: 0.4145, OBJ: 0.5130, TOTAL: 0.4342\n",
      "CE: 0.5004, OBJ: 0.3480, TOTAL: 0.4699\n",
      "CE: 0.2662, OBJ: 0.4351, TOTAL: 0.3000\n",
      "CE: 0.2192, OBJ: 0.2645, TOTAL: 0.2282\n",
      "CE: 0.5650, OBJ: 0.3544, TOTAL: 0.5229\n",
      "CE: 0.3757, OBJ: 0.3850, TOTAL: 0.3776\n",
      "CE: 0.2219, OBJ: 0.2368, TOTAL: 0.2249\n",
      "CE: 0.0620, OBJ: 0.1322, TOTAL: 0.0760\n",
      "CE: 0.6543, OBJ: 0.4605, TOTAL: 0.6156\n",
      "CE: 0.2236, OBJ: 0.2989, TOTAL: 0.2387\n",
      "CE: 0.6273, OBJ: 0.2789, TOTAL: 0.5576\n",
      "CE: 0.2835, OBJ: 0.3554, TOTAL: 0.2979\n",
      "CE: 0.1969, OBJ: 0.4585, TOTAL: 0.2492\n",
      "CE: 0.3370, OBJ: 0.2927, TOTAL: 0.3281\n",
      "CE: 0.4139, OBJ: 0.2544, TOTAL: 0.3820\n",
      "CE: 0.2238, OBJ: 0.3049, TOTAL: 0.2400\n",
      "CE: 0.3559, OBJ: 0.4971, TOTAL: 0.3842\n",
      "CE: 0.4691, OBJ: 0.3899, TOTAL: 0.4533\n",
      "CE: 0.5085, OBJ: 0.2644, TOTAL: 0.4597\n",
      "CE: 0.4084, OBJ: 0.3941, TOTAL: 0.4055\n",
      "CE: 0.1639, OBJ: 0.3066, TOTAL: 0.1924\n",
      "CE: 0.4122, OBJ: 0.4396, TOTAL: 0.4177\n",
      "CE: 0.1672, OBJ: 0.3387, TOTAL: 0.2015\n",
      "CE: 0.2708, OBJ: 0.4127, TOTAL: 0.2991\n",
      "CE: 0.2718, OBJ: 0.4664, TOTAL: 0.3107\n",
      "CE: 0.2127, OBJ: 0.3021, TOTAL: 0.2305\n",
      "CE: 0.4458, OBJ: 0.2813, TOTAL: 0.4129\n",
      "CE: 0.2012, OBJ: 0.4094, TOTAL: 0.2428\n",
      "CE: 0.3465, OBJ: 0.3049, TOTAL: 0.3382\n",
      "CE: 0.5241, OBJ: 0.3501, TOTAL: 0.4893\n",
      "CE: 0.2700, OBJ: 0.4375, TOTAL: 0.3035\n",
      "CE: 0.0489, OBJ: 0.2885, TOTAL: 0.0968\n",
      "CE: 0.2808, OBJ: 0.2440, TOTAL: 0.2734\n",
      "CE: 0.2234, OBJ: 0.2090, TOTAL: 0.2205\n",
      "CE: 0.1000, OBJ: 0.4263, TOTAL: 0.1653\n",
      "CE: 0.3416, OBJ: 0.4944, TOTAL: 0.3722\n",
      "CE: 0.2997, OBJ: 0.3638, TOTAL: 0.3125\n",
      "CE: 0.5594, OBJ: 0.3628, TOTAL: 0.5201\n",
      "CE: 0.3142, OBJ: 0.3174, TOTAL: 0.3148\n",
      "CE: 0.2899, OBJ: 0.2778, TOTAL: 0.2875\n",
      "CE: 0.1148, OBJ: 0.2724, TOTAL: 0.1463\n",
      "CE: 0.4895, OBJ: 0.4681, TOTAL: 0.4852\n",
      "CE: 0.5075, OBJ: 0.2577, TOTAL: 0.4575\n",
      "CE: 0.4021, OBJ: 0.4988, TOTAL: 0.4215\n",
      "CE: 0.6168, OBJ: 0.3386, TOTAL: 0.5612\n",
      "CE: 0.2649, OBJ: 0.3015, TOTAL: 0.2722\n",
      "CE: 0.1090, OBJ: 0.2150, TOTAL: 0.1302\n",
      "CE: 0.0769, OBJ: 0.3562, TOTAL: 0.1328\n",
      "CE: 0.1128, OBJ: 0.1819, TOTAL: 0.1266\n",
      "CE: 0.7754, OBJ: 0.4465, TOTAL: 0.7096\n",
      "CE: 0.4242, OBJ: 0.2675, TOTAL: 0.3929\n",
      "CE: 0.3692, OBJ: 0.3715, TOTAL: 0.3697\n",
      "CE: 0.3194, OBJ: 0.2186, TOTAL: 0.2992\n",
      "CE: 0.7245, OBJ: 0.4183, TOTAL: 0.6633\n",
      "CE: 0.3507, OBJ: 0.3674, TOTAL: 0.3540\n",
      "CE: 0.3436, OBJ: 0.2476, TOTAL: 0.3244\n",
      "CE: 0.3919, OBJ: 0.2744, TOTAL: 0.3684\n",
      "CE: 0.3253, OBJ: 0.3275, TOTAL: 0.3258\n",
      "CE: 0.3539, OBJ: 0.3779, TOTAL: 0.3587\n",
      "CE: 0.2534, OBJ: 0.2720, TOTAL: 0.2571\n",
      "CE: 0.1876, OBJ: 0.3346, TOTAL: 0.2170\n",
      "CE: 0.2036, OBJ: 0.4259, TOTAL: 0.2480\n",
      "CE: 0.2893, OBJ: 0.2348, TOTAL: 0.2784\n",
      "CE: 0.5329, OBJ: 0.3680, TOTAL: 0.4999\n",
      "CE: 0.3024, OBJ: 0.2649, TOTAL: 0.2949\n",
      "CE: 0.1965, OBJ: 0.1709, TOTAL: 0.1914\n",
      "CE: 0.6418, OBJ: 0.4836, TOTAL: 0.6101\n",
      "CE: 0.1467, OBJ: 0.2498, TOTAL: 0.1673\n",
      "CE: 0.1359, OBJ: 0.3921, TOTAL: 0.1872\n",
      "CE: 0.3821, OBJ: 0.3135, TOTAL: 0.3684\n",
      "CE: 0.2148, OBJ: 0.4044, TOTAL: 0.2527\n",
      "CE: 0.3392, OBJ: 0.3577, TOTAL: 0.3429\n",
      "CE: 0.5055, OBJ: 0.2924, TOTAL: 0.4629\n",
      "CE: 0.1656, OBJ: 0.2642, TOTAL: 0.1853\n",
      "CE: 0.2490, OBJ: 0.3168, TOTAL: 0.2626\n",
      "CE: 0.5419, OBJ: 0.2470, TOTAL: 0.4829\n",
      "CE: 0.5856, OBJ: 0.2950, TOTAL: 0.5275\n",
      "CE: 0.2277, OBJ: 0.2851, TOTAL: 0.2391\n",
      "CE: 0.2899, OBJ: 0.3142, TOTAL: 0.2947\n",
      "CE: 0.6680, OBJ: 0.3170, TOTAL: 0.5978\n",
      "CE: 0.1453, OBJ: 0.1809, TOTAL: 0.1524\n",
      "CE: 0.1063, OBJ: 0.3139, TOTAL: 0.1478\n",
      "CE: 0.4646, OBJ: 0.4622, TOTAL: 0.4641\n",
      "CE: 0.4104, OBJ: 0.3269, TOTAL: 0.3937\n",
      "CE: 0.3495, OBJ: 0.2906, TOTAL: 0.3377\n",
      "CE: 0.3973, OBJ: 0.4457, TOTAL: 0.4070\n",
      "CE: 0.2973, OBJ: 0.4146, TOTAL: 0.3208\n",
      "CE: 0.5746, OBJ: 0.2805, TOTAL: 0.5158\n",
      "CE: 0.3010, OBJ: 0.3407, TOTAL: 0.3089\n",
      "CE: 0.1895, OBJ: 0.1721, TOTAL: 0.1860\n",
      "CE: 0.2259, OBJ: 0.2968, TOTAL: 0.2401\n",
      "CE: 0.2320, OBJ: 0.3335, TOTAL: 0.2523\n",
      "CE: 0.1804, OBJ: 0.2946, TOTAL: 0.2033\n",
      "CE: 0.1165, OBJ: 0.3125, TOTAL: 0.1557\n",
      "CE: 0.3142, OBJ: 0.2782, TOTAL: 0.3070\n",
      "CE: 0.5044, OBJ: 0.2709, TOTAL: 0.4577\n",
      "CE: 0.3722, OBJ: 0.1643, TOTAL: 0.3306\n",
      "CE: 0.5182, OBJ: 0.1858, TOTAL: 0.4518\n",
      "CE: 0.2303, OBJ: 0.3246, TOTAL: 0.2492\n",
      "CE: 0.1632, OBJ: 0.1325, TOTAL: 0.1571\n",
      "CE: 0.4490, OBJ: 0.2224, TOTAL: 0.4037\n",
      "CE: 0.3789, OBJ: 0.1837, TOTAL: 0.3399\n",
      "CE: 0.2989, OBJ: 0.3760, TOTAL: 0.3143\n",
      "CE: 0.1608, OBJ: 0.3536, TOTAL: 0.1994\n",
      "CE: 0.5890, OBJ: 0.4098, TOTAL: 0.5532\n",
      "CE: 0.2047, OBJ: 0.2256, TOTAL: 0.2089\n",
      "CE: 0.2210, OBJ: 0.3776, TOTAL: 0.2523\n",
      "CE: 0.4475, OBJ: 0.3170, TOTAL: 0.4214\n",
      "CE: 0.1464, OBJ: 0.3464, TOTAL: 0.1864\n",
      "CE: 0.5184, OBJ: 0.4317, TOTAL: 0.5011\n",
      "CE: 0.2581, OBJ: 0.3808, TOTAL: 0.2826\n",
      "CE: 0.2000, OBJ: 0.3516, TOTAL: 0.2303\n",
      "CE: 0.0778, OBJ: 0.3270, TOTAL: 0.1276\n",
      "CE: 0.3036, OBJ: 0.4942, TOTAL: 0.3417\n",
      "CE: 0.1499, OBJ: 0.1955, TOTAL: 0.1590\n",
      "CE: 0.4365, OBJ: 0.3502, TOTAL: 0.4193\n",
      "CE: 0.3891, OBJ: 0.2697, TOTAL: 0.3652\n",
      "CE: 0.2707, OBJ: 0.3156, TOTAL: 0.2797\n",
      "CE: 0.1906, OBJ: 0.2457, TOTAL: 0.2016\n",
      "CE: 0.2852, OBJ: 0.2555, TOTAL: 0.2792\n",
      "CE: 0.9055, OBJ: 0.5032, TOTAL: 0.8250\n",
      "CE: 0.2974, OBJ: 0.4091, TOTAL: 0.3197\n",
      "CE: 0.4291, OBJ: 0.2304, TOTAL: 0.3893\n",
      "CE: 0.2682, OBJ: 0.2452, TOTAL: 0.2636\n",
      "CE: 0.3381, OBJ: 0.2724, TOTAL: 0.3250\n",
      "CE: 0.3401, OBJ: 0.2625, TOTAL: 0.3246\n",
      "CE: 0.3827, OBJ: 0.3548, TOTAL: 0.3771\n",
      "CE: 0.2247, OBJ: 0.2564, TOTAL: 0.2311\n",
      "CE: 0.2256, OBJ: 0.4443, TOTAL: 0.2693\n",
      "CE: 0.3377, OBJ: 0.3743, TOTAL: 0.3451\n",
      "CE: 0.1622, OBJ: 0.2887, TOTAL: 0.1875\n",
      "CE: 0.1772, OBJ: 0.2848, TOTAL: 0.1987\n",
      "CE: 0.2567, OBJ: 0.2106, TOTAL: 0.2475\n",
      "CE: 0.2020, OBJ: 0.3266, TOTAL: 0.2269\n",
      "CE: 0.2637, OBJ: 0.3801, TOTAL: 0.2870\n",
      "CE: 0.1034, OBJ: 0.1861, TOTAL: 0.1199\n",
      "CE: 0.2720, OBJ: 0.3566, TOTAL: 0.2889\n",
      "CE: 0.2617, OBJ: 0.3961, TOTAL: 0.2886\n",
      "CE: 0.1612, OBJ: 0.2820, TOTAL: 0.1854\n",
      "CE: 0.5234, OBJ: 0.2779, TOTAL: 0.4743\n",
      "CE: 0.1730, OBJ: 0.3697, TOTAL: 0.2124\n",
      "CE: 0.3260, OBJ: 0.2677, TOTAL: 0.3144\n",
      "CE: 0.5737, OBJ: 0.2215, TOTAL: 0.5033\n",
      "CE: 0.3888, OBJ: 0.3611, TOTAL: 0.3833\n",
      "CE: 0.1550, OBJ: 0.2478, TOTAL: 0.1736\n",
      "CE: 0.3511, OBJ: 0.2342, TOTAL: 0.3277\n",
      "CE: 0.6143, OBJ: 0.3445, TOTAL: 0.5604\n",
      "CE: 0.1398, OBJ: 0.2828, TOTAL: 0.1684\n",
      "CE: 0.2201, OBJ: 0.2293, TOTAL: 0.2220\n",
      "CE: 0.3113, OBJ: 0.3586, TOTAL: 0.3208\n",
      "CE: 0.5557, OBJ: 0.2565, TOTAL: 0.4958\n",
      "CE: 0.2018, OBJ: 0.3015, TOTAL: 0.2217\n",
      "CE: 0.3676, OBJ: 0.5011, TOTAL: 0.3943\n",
      "CE: 0.2411, OBJ: 0.2557, TOTAL: 0.2440\n",
      "CE: 0.3381, OBJ: 0.2606, TOTAL: 0.3226\n",
      "CE: 0.4466, OBJ: 0.2676, TOTAL: 0.4108\n",
      "CE: 0.1511, OBJ: 0.2078, TOTAL: 0.1624\n",
      "CE: 0.1037, OBJ: 0.2003, TOTAL: 0.1230\n",
      "CE: 0.2212, OBJ: 0.2652, TOTAL: 0.2300\n",
      "CE: 0.2998, OBJ: 0.2866, TOTAL: 0.2972\n",
      "CE: 0.3755, OBJ: 0.4250, TOTAL: 0.3854\n",
      "CE: 0.3905, OBJ: 0.3215, TOTAL: 0.3767\n",
      "CE: 0.2351, OBJ: 0.4267, TOTAL: 0.2734\n",
      "CE: 0.3362, OBJ: 0.2195, TOTAL: 0.3128\n",
      "CE: 0.0556, OBJ: 0.1565, TOTAL: 0.0758\n",
      "CE: 0.8407, OBJ: 0.3689, TOTAL: 0.7464\n",
      "CE: 0.6118, OBJ: 0.3265, TOTAL: 0.5547\n",
      "CE: 0.1196, OBJ: 0.2522, TOTAL: 0.1461\n",
      "CE: 0.1502, OBJ: 0.4399, TOTAL: 0.2082\n",
      "CE: 0.5772, OBJ: 0.4014, TOTAL: 0.5420\n",
      "CE: 0.4484, OBJ: 0.2103, TOTAL: 0.4008\n",
      "CE: 0.5444, OBJ: 0.2590, TOTAL: 0.4873\n",
      "CE: 0.2529, OBJ: 0.2577, TOTAL: 0.2539\n",
      "CE: 0.3093, OBJ: 0.3926, TOTAL: 0.3259\n",
      "CE: 0.1611, OBJ: 0.2879, TOTAL: 0.1864\n",
      "CE: 0.1178, OBJ: 0.3631, TOTAL: 0.1669\n",
      "CE: 0.1964, OBJ: 0.3140, TOTAL: 0.2199\n",
      "CE: 0.2618, OBJ: 0.4596, TOTAL: 0.3013\n",
      "CE: 0.2137, OBJ: 0.3410, TOTAL: 0.2391\n",
      "CE: 0.4341, OBJ: 0.2578, TOTAL: 0.3988\n",
      "CE: 0.4009, OBJ: 0.4369, TOTAL: 0.4081\n",
      "CE: 0.4204, OBJ: 0.4924, TOTAL: 0.4348\n",
      "CE: 0.2011, OBJ: 0.2110, TOTAL: 0.2030\n",
      "CE: 0.4496, OBJ: 0.2473, TOTAL: 0.4091\n",
      "CE: 0.1882, OBJ: 0.2994, TOTAL: 0.2104\n",
      "CE: 0.2628, OBJ: 0.3540, TOTAL: 0.2811\n",
      "CE: 0.4700, OBJ: 0.4462, TOTAL: 0.4653\n",
      "CE: 0.3752, OBJ: 0.2328, TOTAL: 0.3467\n",
      "CE: 0.4191, OBJ: 0.4762, TOTAL: 0.4305\n",
      "CE: 0.3556, OBJ: 0.3063, TOTAL: 0.3457\n",
      "CE: 0.1596, OBJ: 0.2484, TOTAL: 0.1774\n",
      "CE: 0.3058, OBJ: 0.4002, TOTAL: 0.3247\n",
      "CE: 0.5205, OBJ: 0.2416, TOTAL: 0.4648\n",
      "CE: 0.5776, OBJ: 0.3635, TOTAL: 0.5348\n",
      "CE: 0.4638, OBJ: 0.3356, TOTAL: 0.4381\n",
      "CE: 0.3457, OBJ: 0.3801, TOTAL: 0.3526\n",
      "CE: 0.1827, OBJ: 0.2071, TOTAL: 0.1876\n",
      "CE: 0.4260, OBJ: 0.4005, TOTAL: 0.4209\n",
      "CE: 0.1101, OBJ: 0.3048, TOTAL: 0.1490\n",
      "CE: 0.2821, OBJ: 0.3119, TOTAL: 0.2880\n",
      "CE: 0.4410, OBJ: 0.3144, TOTAL: 0.4156\n",
      "CE: 0.5106, OBJ: 0.3623, TOTAL: 0.4809\n",
      "CE: 0.2180, OBJ: 0.3526, TOTAL: 0.2449\n",
      "CE: 0.3133, OBJ: 0.2397, TOTAL: 0.2986\n",
      "CE: 0.5259, OBJ: 0.3221, TOTAL: 0.4851\n",
      "CE: 0.1647, OBJ: 0.3428, TOTAL: 0.2004\n",
      "CE: 0.4070, OBJ: 0.2530, TOTAL: 0.3762\n",
      "CE: 0.3750, OBJ: 0.3439, TOTAL: 0.3688\n",
      "CE: 0.2590, OBJ: 0.4643, TOTAL: 0.3000\n",
      "CE: 0.3311, OBJ: 0.1976, TOTAL: 0.3044\n",
      "CE: 0.4385, OBJ: 0.2587, TOTAL: 0.4026\n",
      "CE: 0.2449, OBJ: 0.2668, TOTAL: 0.2493\n",
      "CE: 0.3338, OBJ: 0.2192, TOTAL: 0.3109\n",
      "CE: 0.4259, OBJ: 0.4126, TOTAL: 0.4232\n",
      "CE: 0.0976, OBJ: 0.3295, TOTAL: 0.1440\n",
      "CE: 0.4799, OBJ: 0.3561, TOTAL: 0.4552\n",
      "CE: 0.2269, OBJ: 0.3025, TOTAL: 0.2420\n",
      "CE: 0.5569, OBJ: 0.4350, TOTAL: 0.5325\n",
      "CE: 0.2293, OBJ: 0.2152, TOTAL: 0.2265\n",
      "CE: 0.3688, OBJ: 0.2467, TOTAL: 0.3444\n",
      "CE: 0.2643, OBJ: 0.2827, TOTAL: 0.2679\n",
      "CE: 0.3381, OBJ: 0.3272, TOTAL: 0.3360\n",
      "CE: 0.2856, OBJ: 0.2536, TOTAL: 0.2792\n",
      "CE: 0.1956, OBJ: 0.2908, TOTAL: 0.2146\n",
      "CE: 0.3977, OBJ: 0.3093, TOTAL: 0.3800\n",
      "CE: 0.3345, OBJ: 0.3655, TOTAL: 0.3407\n",
      "CE: 0.2720, OBJ: 0.2157, TOTAL: 0.2607\n",
      "CE: 0.1864, OBJ: 0.2259, TOTAL: 0.1943\n",
      "CE: 0.2607, OBJ: 0.3002, TOTAL: 0.2686\n",
      "CE: 0.0824, OBJ: 0.2454, TOTAL: 0.1150\n",
      "CE: 0.2817, OBJ: 0.2951, TOTAL: 0.2844\n",
      "CE: 0.3664, OBJ: 0.2582, TOTAL: 0.3448\n",
      "CE: 0.2280, OBJ: 0.3377, TOTAL: 0.2499\n",
      "CE: 0.1555, OBJ: 0.1742, TOTAL: 0.1593\n",
      "CE: 0.8063, OBJ: 0.2700, TOTAL: 0.6991\n",
      "CE: 0.4098, OBJ: 0.2027, TOTAL: 0.3684\n",
      "CE: 0.2933, OBJ: 0.3214, TOTAL: 0.2990\n",
      "CE: 0.3270, OBJ: 0.3362, TOTAL: 0.3289\n",
      "CE: 0.3223, OBJ: 0.2718, TOTAL: 0.3122\n",
      "CE: 0.4647, OBJ: 0.3722, TOTAL: 0.4462\n",
      "CE: 0.1756, OBJ: 0.2905, TOTAL: 0.1986\n",
      "CE: 0.2239, OBJ: 0.1837, TOTAL: 0.2158\n",
      "CE: 0.6622, OBJ: 0.2538, TOTAL: 0.5805\n",
      "CE: 0.2338, OBJ: 0.2472, TOTAL: 0.2365\n",
      "CE: 0.2543, OBJ: 0.3628, TOTAL: 0.2760\n",
      "CE: 0.1803, OBJ: 0.2936, TOTAL: 0.2030\n",
      "CE: 0.5539, OBJ: 0.3289, TOTAL: 0.5089\n",
      "CE: 0.2005, OBJ: 0.2696, TOTAL: 0.2143\n",
      "CE: 0.1778, OBJ: 0.3395, TOTAL: 0.2102\n",
      "CE: 0.4375, OBJ: 0.2981, TOTAL: 0.4096\n",
      "CE: 0.3515, OBJ: 0.3339, TOTAL: 0.3480\n",
      "CE: 0.0928, OBJ: 0.2866, TOTAL: 0.1316\n",
      "CE: 0.4579, OBJ: 0.3216, TOTAL: 0.4306\n",
      "CE: 0.3369, OBJ: 0.1999, TOTAL: 0.3095\n",
      "CE: 0.3437, OBJ: 0.4602, TOTAL: 0.3670\n",
      "CE: 0.4236, OBJ: 0.3057, TOTAL: 0.4001\n",
      "CE: 0.3921, OBJ: 0.2942, TOTAL: 0.3725\n",
      "CE: 0.1967, OBJ: 0.3541, TOTAL: 0.2282\n",
      "CE: 0.3009, OBJ: 0.3868, TOTAL: 0.3181\n",
      "CE: 0.4496, OBJ: 0.3498, TOTAL: 0.4296\n",
      "CE: 0.6161, OBJ: 0.3289, TOTAL: 0.5587\n",
      "CE: 0.5260, OBJ: 0.2989, TOTAL: 0.4806\n",
      "CE: 0.4377, OBJ: 0.4286, TOTAL: 0.4359\n",
      "CE: 0.4308, OBJ: 0.4170, TOTAL: 0.4280\n",
      "CE: 0.3372, OBJ: 0.2256, TOTAL: 0.3149\n",
      "CE: 0.3054, OBJ: 0.3354, TOTAL: 0.3114\n",
      "CE: 0.4555, OBJ: 0.4724, TOTAL: 0.4588\n",
      "CE: 0.1237, OBJ: 0.3246, TOTAL: 0.1639\n",
      "CE: 0.2752, OBJ: 0.2997, TOTAL: 0.2801\n",
      "CE: 0.1670, OBJ: 0.4010, TOTAL: 0.2138\n",
      "CE: 0.1648, OBJ: 0.2461, TOTAL: 0.1810\n",
      "CE: 0.4799, OBJ: 0.3460, TOTAL: 0.4531\n",
      "CE: 0.2169, OBJ: 0.3194, TOTAL: 0.2374\n",
      "CE: 0.1482, OBJ: 0.4158, TOTAL: 0.2017\n",
      "CE: 0.1215, OBJ: 0.2756, TOTAL: 0.1523\n",
      "CE: 0.5605, OBJ: 0.4283, TOTAL: 0.5341\n",
      "CE: 0.1808, OBJ: 0.3601, TOTAL: 0.2167\n",
      "CE: 0.5445, OBJ: 0.5225, TOTAL: 0.5401\n",
      "CE: 0.3752, OBJ: 0.1849, TOTAL: 0.3372\n",
      "CE: 0.1817, OBJ: 0.3062, TOTAL: 0.2066\n",
      "CE: 0.2518, OBJ: 0.3895, TOTAL: 0.2793\n",
      "CE: 0.4560, OBJ: 0.3738, TOTAL: 0.4396\n",
      "CE: 0.2982, OBJ: 0.4037, TOTAL: 0.3193\n",
      "CE: 0.3697, OBJ: 0.2182, TOTAL: 0.3394\n",
      "CE: 0.4115, OBJ: 0.2521, TOTAL: 0.3796\n",
      "CE: 0.0829, OBJ: 0.2440, TOTAL: 0.1151\n",
      "CE: 0.1105, OBJ: 0.2005, TOTAL: 0.1285\n",
      "CE: 0.3814, OBJ: 0.3980, TOTAL: 0.3847\n",
      "CE: 0.4676, OBJ: 0.3547, TOTAL: 0.4450\n",
      "CE: 0.2199, OBJ: 0.2912, TOTAL: 0.2342\n",
      "CE: 0.3747, OBJ: 0.3662, TOTAL: 0.3730\n",
      "CE: 0.2094, OBJ: 0.2701, TOTAL: 0.2215\n",
      "CE: 0.1295, OBJ: 0.3793, TOTAL: 0.1795\n",
      "CE: 0.2842, OBJ: 0.2242, TOTAL: 0.2722\n",
      "CE: 0.4001, OBJ: 0.2210, TOTAL: 0.3643\n",
      "CE: 0.0942, OBJ: 0.2642, TOTAL: 0.1282\n",
      "CE: 0.2587, OBJ: 0.3290, TOTAL: 0.2728\n",
      "CE: 0.7023, OBJ: 0.2975, TOTAL: 0.6213\n",
      "CE: 0.5785, OBJ: 0.3376, TOTAL: 0.5303\n",
      "CE: 0.4763, OBJ: 0.4196, TOTAL: 0.4650\n",
      "CE: 0.2645, OBJ: 0.2352, TOTAL: 0.2587\n",
      "CE: 0.1706, OBJ: 0.2999, TOTAL: 0.1965\n",
      "CE: 0.2718, OBJ: 0.2758, TOTAL: 0.2726\n",
      "CE: 0.5368, OBJ: 0.4257, TOTAL: 0.5145\n",
      "CE: 0.2742, OBJ: 0.3197, TOTAL: 0.2833\n",
      "CE: 0.4552, OBJ: 0.3992, TOTAL: 0.4440\n",
      "CE: 0.1906, OBJ: 0.2577, TOTAL: 0.2040\n",
      "CE: 0.1408, OBJ: 0.1928, TOTAL: 0.1512\n",
      "CE: 0.2810, OBJ: 0.2233, TOTAL: 0.2695\n",
      "CE: 0.2146, OBJ: 0.4302, TOTAL: 0.2577\n",
      "CE: 0.2934, OBJ: 0.4235, TOTAL: 0.3194\n",
      "CE: 0.3403, OBJ: 0.3637, TOTAL: 0.3450\n",
      "CE: 0.7865, OBJ: 0.3896, TOTAL: 0.7071\n",
      "CE: 0.5167, OBJ: 0.2600, TOTAL: 0.4654\n",
      "CE: 0.5580, OBJ: 0.3751, TOTAL: 0.5215\n",
      "CE: 0.2033, OBJ: 0.2373, TOTAL: 0.2101\n",
      "CE: 0.2080, OBJ: 0.3091, TOTAL: 0.2282\n",
      "CE: 0.1611, OBJ: 0.2587, TOTAL: 0.1807\n",
      "CE: 0.3090, OBJ: 0.3066, TOTAL: 0.3085\n",
      "CE: 0.7358, OBJ: 0.3384, TOTAL: 0.6563\n",
      "CE: 0.1993, OBJ: 0.2485, TOTAL: 0.2091\n",
      "CE: 0.1231, OBJ: 0.2886, TOTAL: 0.1562\n",
      "CE: 0.1592, OBJ: 0.3086, TOTAL: 0.1891\n",
      "CE: 0.4339, OBJ: 0.2811, TOTAL: 0.4034\n",
      "CE: 0.1729, OBJ: 0.3515, TOTAL: 0.2086\n",
      "CE: 0.2154, OBJ: 0.2188, TOTAL: 0.2161\n",
      "CE: 0.4173, OBJ: 0.2552, TOTAL: 0.3849\n",
      "CE: 0.3242, OBJ: 0.3476, TOTAL: 0.3289\n",
      "CE: 0.3687, OBJ: 0.3291, TOTAL: 0.3608\n",
      "CE: 0.3900, OBJ: 0.3986, TOTAL: 0.3917\n",
      "CE: 0.1881, OBJ: 0.2342, TOTAL: 0.1973\n",
      "CE: 0.4047, OBJ: 0.2806, TOTAL: 0.3799\n",
      "CE: 0.4026, OBJ: 0.3647, TOTAL: 0.3950\n",
      "CE: 0.3307, OBJ: 0.4255, TOTAL: 0.3497\n",
      "CE: 0.7630, OBJ: 0.3233, TOTAL: 0.6751\n",
      "CE: 0.1798, OBJ: 0.3314, TOTAL: 0.2102\n",
      "CE: 0.2758, OBJ: 0.2593, TOTAL: 0.2725\n",
      "CE: 0.2154, OBJ: 0.2977, TOTAL: 0.2319\n",
      "CE: 0.1670, OBJ: 0.3927, TOTAL: 0.2121\n",
      "CE: 0.2967, OBJ: 0.2448, TOTAL: 0.2863\n",
      "CE: 0.2495, OBJ: 0.2694, TOTAL: 0.2535\n",
      "CE: 0.2111, OBJ: 0.3409, TOTAL: 0.2370\n",
      "CE: 0.2978, OBJ: 0.2639, TOTAL: 0.2910\n",
      "CE: 0.5735, OBJ: 0.3419, TOTAL: 0.5272\n",
      "CE: 0.4165, OBJ: 0.2807, TOTAL: 0.3893\n",
      "CE: 0.3838, OBJ: 0.2848, TOTAL: 0.3640\n",
      "CE: 0.1221, OBJ: 0.3551, TOTAL: 0.1687\n",
      "CE: 0.1868, OBJ: 0.2677, TOTAL: 0.2030\n",
      "CE: 0.1391, OBJ: 0.3517, TOTAL: 0.1816\n",
      "CE: 0.1009, OBJ: 0.3229, TOTAL: 0.1453\n",
      "CE: 0.3120, OBJ: 0.1873, TOTAL: 0.2870\n",
      "CE: 0.2658, OBJ: 0.3403, TOTAL: 0.2807\n",
      "CE: 0.1416, OBJ: 0.2713, TOTAL: 0.1675\n",
      "CE: 0.2419, OBJ: 0.3373, TOTAL: 0.2610\n",
      "CE: 0.2610, OBJ: 0.2957, TOTAL: 0.2679\n",
      "CE: 0.4788, OBJ: 0.3796, TOTAL: 0.4590\n",
      "CE: 0.2604, OBJ: 0.2345, TOTAL: 0.2552\n",
      "CE: 0.2443, OBJ: 0.3731, TOTAL: 0.2700\n",
      "CE: 0.1917, OBJ: 0.3969, TOTAL: 0.2328\n",
      "CE: 0.0862, OBJ: 0.1823, TOTAL: 0.1054\n",
      "CE: 0.2528, OBJ: 0.3057, TOTAL: 0.2634\n",
      "CE: 0.2097, OBJ: 0.2053, TOTAL: 0.2088\n",
      "CE: 0.2454, OBJ: 0.4896, TOTAL: 0.2942\n",
      "CE: 0.1552, OBJ: 0.3035, TOTAL: 0.1848\n",
      "CE: 0.4552, OBJ: 0.1877, TOTAL: 0.4017\n",
      "CE: 0.1458, OBJ: 0.2392, TOTAL: 0.1645\n",
      "CE: 0.0857, OBJ: 0.2572, TOTAL: 0.1200\n",
      "CE: 0.2925, OBJ: 0.2324, TOTAL: 0.2805\n",
      "CE: 0.1952, OBJ: 0.2545, TOTAL: 0.2070\n",
      "CE: 0.1533, OBJ: 0.2801, TOTAL: 0.1786\n",
      "CE: 0.2576, OBJ: 0.3244, TOTAL: 0.2710\n",
      "CE: 0.1217, OBJ: 0.2699, TOTAL: 0.1513\n",
      "CE: 0.6364, OBJ: 0.3788, TOTAL: 0.5849\n",
      "CE: 0.0796, OBJ: 0.2713, TOTAL: 0.1179\n",
      "CE: 0.5634, OBJ: 0.2919, TOTAL: 0.5091\n",
      "CE: 0.2621, OBJ: 0.3477, TOTAL: 0.2792\n",
      "CE: 0.3465, OBJ: 0.2759, TOTAL: 0.3324\n",
      "CE: 0.2049, OBJ: 0.3960, TOTAL: 0.2431\n",
      "CE: 0.1015, OBJ: 0.2693, TOTAL: 0.1351\n",
      "CE: 0.4210, OBJ: 0.3047, TOTAL: 0.3977\n",
      "CE: 0.2899, OBJ: 0.1815, TOTAL: 0.2682\n",
      "CE: 0.2550, OBJ: 0.3996, TOTAL: 0.2839\n",
      "CE: 0.1649, OBJ: 0.2681, TOTAL: 0.1855\n",
      "CE: 0.4661, OBJ: 0.3042, TOTAL: 0.4337\n",
      "CE: 0.2220, OBJ: 0.2964, TOTAL: 0.2369\n",
      "CE: 0.2672, OBJ: 0.3720, TOTAL: 0.2882\n",
      "CE: 0.1224, OBJ: 0.3287, TOTAL: 0.1636\n",
      "CE: 0.4789, OBJ: 0.3295, TOTAL: 0.4491\n",
      "CE: 0.0614, OBJ: 0.2985, TOTAL: 0.1088\n",
      "CE: 0.2229, OBJ: 0.2521, TOTAL: 0.2287\n",
      "CE: 0.5521, OBJ: 0.2144, TOTAL: 0.4846\n",
      "CE: 0.3628, OBJ: 0.2928, TOTAL: 0.3488\n",
      "CE: 0.0790, OBJ: 0.3035, TOTAL: 0.1239\n",
      "CE: 0.4740, OBJ: 0.2874, TOTAL: 0.4366\n",
      "CE: 0.2001, OBJ: 0.3401, TOTAL: 0.2281\n",
      "CE: 0.2104, OBJ: 0.3341, TOTAL: 0.2352\n",
      "CE: 0.3490, OBJ: 0.3484, TOTAL: 0.3489\n",
      "CE: 0.2779, OBJ: 0.3245, TOTAL: 0.2872\n",
      "CE: 0.2176, OBJ: 0.3452, TOTAL: 0.2431\n",
      "CE: 0.1808, OBJ: 0.2257, TOTAL: 0.1898\n",
      "CE: 0.5165, OBJ: 0.2795, TOTAL: 0.4691\n",
      "CE: 0.3991, OBJ: 0.3324, TOTAL: 0.3857\n",
      "CE: 0.4206, OBJ: 0.2606, TOTAL: 0.3886\n",
      "CE: 0.2885, OBJ: 0.4619, TOTAL: 0.3232\n",
      "CE: 0.7068, OBJ: 0.3510, TOTAL: 0.6357\n",
      "CE: 0.3064, OBJ: 0.3625, TOTAL: 0.3176\n",
      "CE: 0.3123, OBJ: 0.3569, TOTAL: 0.3212\n",
      "CE: 0.7381, OBJ: 0.4974, TOTAL: 0.6900\n",
      "CE: 0.0593, OBJ: 0.2387, TOTAL: 0.0951\n",
      "CE: 0.3008, OBJ: 0.2373, TOTAL: 0.2881\n",
      "CE: 0.2439, OBJ: 0.2621, TOTAL: 0.2475\n",
      "CE: 0.1134, OBJ: 0.3138, TOTAL: 0.1535\n",
      "CE: 0.2194, OBJ: 0.4413, TOTAL: 0.2638\n",
      "CE: 0.4483, OBJ: 0.3556, TOTAL: 0.4298\n",
      "CE: 0.6896, OBJ: 0.2766, TOTAL: 0.6070\n",
      "CE: 0.1035, OBJ: 0.3009, TOTAL: 0.1430\n",
      "CE: 0.2596, OBJ: 0.2094, TOTAL: 0.2496\n",
      "CE: 0.2838, OBJ: 0.4714, TOTAL: 0.3213\n",
      "CE: 0.5061, OBJ: 0.2225, TOTAL: 0.4494\n",
      "CE: 0.3279, OBJ: 0.3010, TOTAL: 0.3225\n",
      "CE: 0.1723, OBJ: 0.3329, TOTAL: 0.2044\n",
      "CE: 0.3466, OBJ: 0.3541, TOTAL: 0.3481\n",
      "CE: 0.5367, OBJ: 0.3298, TOTAL: 0.4953\n",
      "CE: 0.1982, OBJ: 0.4058, TOTAL: 0.2397\n",
      "CE: 0.3063, OBJ: 0.2970, TOTAL: 0.3044\n",
      "CE: 0.5109, OBJ: 0.3473, TOTAL: 0.4781\n",
      "CE: 0.5260, OBJ: 0.2343, TOTAL: 0.4677\n",
      "CE: 0.0631, OBJ: 0.3582, TOTAL: 0.1221\n",
      "CE: 0.6086, OBJ: 0.5167, TOTAL: 0.5902\n",
      "CE: 0.6085, OBJ: 0.2969, TOTAL: 0.5462\n",
      "CE: 0.1873, OBJ: 0.2059, TOTAL: 0.1910\n",
      "CE: 0.2000, OBJ: 0.2805, TOTAL: 0.2161\n",
      "CE: 0.2391, OBJ: 0.3937, TOTAL: 0.2701\n",
      "CE: 0.5090, OBJ: 0.4155, TOTAL: 0.4903\n",
      "CE: 0.6010, OBJ: 0.5105, TOTAL: 0.5829\n",
      "CE: 0.4000, OBJ: 0.3116, TOTAL: 0.3823\n",
      "CE: 0.8163, OBJ: 0.4547, TOTAL: 0.7440\n",
      "CE: 0.2448, OBJ: 0.3438, TOTAL: 0.2646\n",
      "CE: 0.1139, OBJ: 0.2788, TOTAL: 0.1469\n",
      "CE: 0.3771, OBJ: 0.3335, TOTAL: 0.3684\n",
      "CE: 0.3222, OBJ: 0.2140, TOTAL: 0.3006\n",
      "CE: 0.3661, OBJ: 0.2743, TOTAL: 0.3478\n",
      "CE: 0.3623, OBJ: 0.2485, TOTAL: 0.3395\n",
      "CE: 0.3854, OBJ: 0.2190, TOTAL: 0.3521\n",
      "CE: 0.2274, OBJ: 0.3935, TOTAL: 0.2606\n",
      "CE: 0.3266, OBJ: 0.3663, TOTAL: 0.3346\n",
      "CE: 0.5596, OBJ: 0.4109, TOTAL: 0.5299\n",
      "CE: 0.2250, OBJ: 0.3789, TOTAL: 0.2558\n",
      "CE: 0.1684, OBJ: 0.3268, TOTAL: 0.2001\n",
      "CE: 0.2586, OBJ: 0.2118, TOTAL: 0.2492\n",
      "CE: 0.5988, OBJ: 0.2237, TOTAL: 0.5238\n",
      "CE: 0.4847, OBJ: 0.4628, TOTAL: 0.4803\n",
      "CE: 0.7326, OBJ: 0.3510, TOTAL: 0.6563\n",
      "CE: 0.3754, OBJ: 0.2768, TOTAL: 0.3557\n",
      "CE: 0.1817, OBJ: 0.2928, TOTAL: 0.2039\n",
      "CE: 0.2322, OBJ: 0.3103, TOTAL: 0.2479\n",
      "CE: 0.2001, OBJ: 0.4270, TOTAL: 0.2454\n",
      "CE: 0.3012, OBJ: 0.2608, TOTAL: 0.2931\n",
      "CE: 0.3422, OBJ: 0.3014, TOTAL: 0.3341\n",
      "CE: 0.1903, OBJ: 0.5626, TOTAL: 0.2647\n",
      "CE: 0.4196, OBJ: 0.4488, TOTAL: 0.4255\n",
      "CE: 0.4388, OBJ: 0.4249, TOTAL: 0.4361\n",
      "CE: 0.1815, OBJ: 0.4370, TOTAL: 0.2326\n",
      "CE: 0.2208, OBJ: 0.2954, TOTAL: 0.2357\n",
      "CE: 0.2357, OBJ: 0.3944, TOTAL: 0.2675\n",
      "CE: 0.2339, OBJ: 0.2558, TOTAL: 0.2383\n",
      "CE: 0.3658, OBJ: 0.4480, TOTAL: 0.3822\n",
      "CE: 0.4575, OBJ: 0.4332, TOTAL: 0.4526\n",
      "CE: 0.2071, OBJ: 0.4313, TOTAL: 0.2519\n",
      "CE: 0.2492, OBJ: 0.1671, TOTAL: 0.2328\n",
      "CE: 0.2205, OBJ: 0.2212, TOTAL: 0.2207\n",
      "CE: 0.2343, OBJ: 0.3736, TOTAL: 0.2621\n",
      "CE: 0.3314, OBJ: 0.2556, TOTAL: 0.3162\n",
      "CE: 0.1695, OBJ: 0.2599, TOTAL: 0.1876\n",
      "CE: 0.3948, OBJ: 0.3454, TOTAL: 0.3849\n",
      "CE: 0.1889, OBJ: 0.3486, TOTAL: 0.2208\n",
      "CE: 0.1311, OBJ: 0.1755, TOTAL: 0.1400\n",
      "CE: 0.1976, OBJ: 0.2293, TOTAL: 0.2039\n",
      "CE: 0.4463, OBJ: 0.1843, TOTAL: 0.3939\n",
      "CE: 0.1139, OBJ: 0.2855, TOTAL: 0.1482\n",
      "CE: 0.2718, OBJ: 0.1622, TOTAL: 0.2499\n",
      "CE: 0.1431, OBJ: 0.2730, TOTAL: 0.1691\n",
      "CE: 0.1144, OBJ: 0.3417, TOTAL: 0.1599\n",
      "CE: 0.1734, OBJ: 0.3781, TOTAL: 0.2143\n",
      "CE: 0.6343, OBJ: 0.3148, TOTAL: 0.5704\n",
      "CE: 0.7107, OBJ: 0.3367, TOTAL: 0.6359\n",
      "CE: 0.7470, OBJ: 0.3810, TOTAL: 0.6738\n",
      "CE: 0.6327, OBJ: 0.4305, TOTAL: 0.5923\n",
      "CE: 0.5215, OBJ: 0.3786, TOTAL: 0.4929\n",
      "CE: 0.1131, OBJ: 0.1833, TOTAL: 0.1272\n",
      "CE: 0.5011, OBJ: 0.3783, TOTAL: 0.4765\n",
      "CE: 0.2223, OBJ: 0.2700, TOTAL: 0.2319\n",
      "CE: 0.5303, OBJ: 0.3163, TOTAL: 0.4875\n",
      "CE: 0.2859, OBJ: 0.3500, TOTAL: 0.2987\n",
      "CE: 0.2088, OBJ: 0.3240, TOTAL: 0.2319\n",
      "CE: 0.2997, OBJ: 0.3030, TOTAL: 0.3004\n",
      "CE: 0.2275, OBJ: 0.1904, TOTAL: 0.2201\n",
      "CE: 0.3890, OBJ: 0.3333, TOTAL: 0.3778\n",
      "CE: 0.1099, OBJ: 0.3695, TOTAL: 0.1618\n",
      "CE: 0.4285, OBJ: 0.3455, TOTAL: 0.4119\n",
      "CE: 0.3520, OBJ: 0.4166, TOTAL: 0.3650\n",
      "CE: 0.3658, OBJ: 0.3276, TOTAL: 0.3581\n",
      "CE: 0.2460, OBJ: 0.3258, TOTAL: 0.2620\n",
      "CE: 0.7355, OBJ: 0.3604, TOTAL: 0.6605\n",
      "CE: 0.3212, OBJ: 0.2598, TOTAL: 0.3090\n",
      "CE: 0.3090, OBJ: 0.3396, TOTAL: 0.3151\n",
      "CE: 0.3766, OBJ: 0.2795, TOTAL: 0.3572\n",
      "CE: 0.1694, OBJ: 0.3017, TOTAL: 0.1958\n",
      "CE: 0.3367, OBJ: 0.2919, TOTAL: 0.3278\n",
      "CE: 0.3914, OBJ: 0.4221, TOTAL: 0.3976\n",
      "CE: 0.5821, OBJ: 0.3573, TOTAL: 0.5371\n",
      "CE: 0.4464, OBJ: 0.2472, TOTAL: 0.4065\n",
      "CE: 0.4410, OBJ: 0.2178, TOTAL: 0.3963\n",
      "CE: 0.4855, OBJ: 0.4095, TOTAL: 0.4703\n",
      "CE: 0.0667, OBJ: 0.3209, TOTAL: 0.1176\n",
      "CE: 0.4007, OBJ: 0.3688, TOTAL: 0.3943\n",
      "CE: 0.3801, OBJ: 0.3214, TOTAL: 0.3683\n",
      "CE: 0.3763, OBJ: 0.4068, TOTAL: 0.3824\n",
      "CE: 0.2733, OBJ: 0.2981, TOTAL: 0.2782\n",
      "CE: 0.4947, OBJ: 0.4662, TOTAL: 0.4890\n",
      "CE: 0.4113, OBJ: 0.4387, TOTAL: 0.4168\n",
      "CE: 0.3885, OBJ: 0.3067, TOTAL: 0.3721\n",
      "CE: 0.3739, OBJ: 0.3167, TOTAL: 0.3624\n",
      "CE: 0.2312, OBJ: 0.3419, TOTAL: 0.2533\n",
      "CE: 0.4484, OBJ: 0.3273, TOTAL: 0.4242\n",
      "CE: 0.2880, OBJ: 0.4004, TOTAL: 0.3105\n",
      "CE: 0.2198, OBJ: 0.3740, TOTAL: 0.2506\n",
      "CE: 0.1898, OBJ: 0.2309, TOTAL: 0.1980\n",
      "CE: 0.1896, OBJ: 0.2800, TOTAL: 0.2077\n",
      "CE: 0.3476, OBJ: 0.3104, TOTAL: 0.3402\n",
      "CE: 0.3288, OBJ: 0.3916, TOTAL: 0.3414\n",
      "CE: 0.2829, OBJ: 0.4215, TOTAL: 0.3106\n",
      "CE: 0.1301, OBJ: 0.2953, TOTAL: 0.1631\n",
      "CE: 0.0589, OBJ: 0.2759, TOTAL: 0.1023\n",
      "CE: 0.5558, OBJ: 0.3375, TOTAL: 0.5122\n",
      "CE: 0.4380, OBJ: 0.4228, TOTAL: 0.4350\n",
      "CE: 0.3954, OBJ: 0.3434, TOTAL: 0.3850\n",
      "CE: 0.4769, OBJ: 0.4281, TOTAL: 0.4671\n",
      "CE: 0.6121, OBJ: 0.2632, TOTAL: 0.5424\n",
      "CE: 0.1789, OBJ: 0.3791, TOTAL: 0.2189\n",
      "CE: 0.2948, OBJ: 0.3469, TOTAL: 0.3052\n",
      "CE: 0.4262, OBJ: 0.2519, TOTAL: 0.3913\n",
      "CE: 0.3043, OBJ: 0.5893, TOTAL: 0.3613\n",
      "CE: 0.3786, OBJ: 0.2962, TOTAL: 0.3621\n",
      "CE: 0.3827, OBJ: 0.3423, TOTAL: 0.3746\n",
      "CE: 0.1590, OBJ: 0.2907, TOTAL: 0.1853\n",
      "CE: 0.3862, OBJ: 0.3203, TOTAL: 0.3730\n",
      "CE: 0.3639, OBJ: 0.4563, TOTAL: 0.3823\n",
      "CE: 0.2638, OBJ: 0.3624, TOTAL: 0.2835\n",
      "CE: 0.2466, OBJ: 0.3592, TOTAL: 0.2691\n",
      "CE: 0.0469, OBJ: 0.1385, TOTAL: 0.0652\n",
      "CE: 0.2747, OBJ: 0.3530, TOTAL: 0.2903\n",
      "CE: 0.2792, OBJ: 0.2155, TOTAL: 0.2664\n",
      "CE: 0.0791, OBJ: 0.1277, TOTAL: 0.0888\n",
      "CE: 0.1328, OBJ: 0.2124, TOTAL: 0.1488\n",
      "CE: 0.3402, OBJ: 0.3613, TOTAL: 0.3444\n",
      "CE: 0.2145, OBJ: 0.4041, TOTAL: 0.2524\n",
      "CE: 0.3231, OBJ: 0.3347, TOTAL: 0.3254\n",
      "CE: 0.1752, OBJ: 0.2922, TOTAL: 0.1986\n",
      "CE: 0.2001, OBJ: 0.3316, TOTAL: 0.2264\n",
      "CE: 0.5056, OBJ: 0.3084, TOTAL: 0.4662\n",
      "CE: 0.2962, OBJ: 0.2252, TOTAL: 0.2820\n",
      "CE: 0.1791, OBJ: 0.2492, TOTAL: 0.1931\n",
      "CE: 0.2628, OBJ: 0.2507, TOTAL: 0.2604\n",
      "CE: 0.4664, OBJ: 0.3171, TOTAL: 0.4365\n",
      "CE: 0.5021, OBJ: 0.3758, TOTAL: 0.4768\n",
      "CE: 0.2188, OBJ: 0.3259, TOTAL: 0.2402\n",
      "CE: 0.5910, OBJ: 0.3224, TOTAL: 0.5372\n",
      "CE: 0.2665, OBJ: 0.3160, TOTAL: 0.2764\n",
      "CE: 0.1749, OBJ: 0.4082, TOTAL: 0.2216\n",
      "CE: 0.4714, OBJ: 0.3828, TOTAL: 0.4537\n",
      "CE: 0.2992, OBJ: 0.2638, TOTAL: 0.2921\n",
      "CE: 0.1190, OBJ: 0.2017, TOTAL: 0.1355\n",
      "CE: 0.1899, OBJ: 0.4304, TOTAL: 0.2380\n",
      "CE: 0.4469, OBJ: 0.2333, TOTAL: 0.4042\n",
      "CE: 0.2082, OBJ: 0.3810, TOTAL: 0.2428\n",
      "CE: 0.0567, OBJ: 0.2711, TOTAL: 0.0996\n",
      "CE: 0.1988, OBJ: 0.2123, TOTAL: 0.2015\n",
      "CE: 0.0996, OBJ: 0.3155, TOTAL: 0.1427\n",
      "CE: 0.2683, OBJ: 0.2964, TOTAL: 0.2739\n",
      "CE: 0.2814, OBJ: 0.3764, TOTAL: 0.3004\n",
      "CE: 0.1088, OBJ: 0.3332, TOTAL: 0.1537\n",
      "CE: 0.3287, OBJ: 0.2552, TOTAL: 0.3140\n",
      "CE: 0.3803, OBJ: 0.2573, TOTAL: 0.3557\n",
      "CE: 0.2511, OBJ: 0.3391, TOTAL: 0.2687\n",
      "CE: 0.2585, OBJ: 0.3491, TOTAL: 0.2767\n",
      "CE: 0.2181, OBJ: 0.3924, TOTAL: 0.2530\n",
      "CE: 0.4406, OBJ: 0.3764, TOTAL: 0.4278\n",
      "CE: 0.5644, OBJ: 0.3667, TOTAL: 0.5248\n",
      "CE: 0.5845, OBJ: 0.3827, TOTAL: 0.5441\n",
      "CE: 0.2103, OBJ: 0.3315, TOTAL: 0.2345\n",
      "CE: 0.2255, OBJ: 0.1893, TOTAL: 0.2183\n",
      "CE: 0.2754, OBJ: 0.3621, TOTAL: 0.2928\n",
      "CE: 0.3344, OBJ: 0.3397, TOTAL: 0.3355\n",
      "CE: 0.3609, OBJ: 0.2646, TOTAL: 0.3417\n",
      "CE: 0.1602, OBJ: 0.3998, TOTAL: 0.2081\n",
      "CE: 0.6557, OBJ: 0.3367, TOTAL: 0.5919\n",
      "CE: 0.2588, OBJ: 0.4784, TOTAL: 0.3027\n",
      "CE: 0.0564, OBJ: 0.2151, TOTAL: 0.0881\n",
      "CE: 0.3675, OBJ: 0.4509, TOTAL: 0.3842\n",
      "CE: 1.3038, OBJ: 0.3601, TOTAL: 1.1151\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6726, training_loss=0.6164255496684973, metrics={'train_runtime': 3141.2755, 'train_samples_per_second': 17.128, 'train_steps_per_second': 2.141, 'total_flos': 9210499958439936.0, 'train_loss': 0.6164255496684973, 'epoch': 1.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @title training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2w-9WK0wn19e"
   },
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 391,
     "status": "ok",
     "timestamp": 1762742173518,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "D_aNbUR2i_4x",
    "outputId": "395ee75d-11c1-4de0-897f-72e170d53d7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the drunky emperor destroyed the peaceful town\n"
     ]
    }
   ],
   "source": [
    "test_input = \"The[0] horrible[1] dictator[0] destroyed[0] the[0] peaceful[1] town[0].\"\n",
    "enc = tokenizer(test_input, return_tensors=\"pt\").to(model.device)\n",
    "out = model.generate(**enc, max_new_tokens=64)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 413,
     "status": "ok",
     "timestamp": 1762742280463,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "ddujs5Fkn8Cs",
    "outputId": "97d6f5f8-29c1-4edd-c033-e6811a105ce7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "checkpoint_path = os.path.join(folder_path,\n",
    "    \"model_checkpoints/wnc-mpqa-flan_t5-2loss-11_09/checkpoint-6000\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "trained_model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_path)\n",
    "trained_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 951,
     "status": "ok",
     "timestamp": 1762742323342,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "MFbELsR9oNWi",
    "outputId": "aca75050-d0fd-43b0-9796-559f07517f49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dictator destroyed the peaceful town.\n"
     ]
    }
   ],
   "source": [
    "text = \"neutralize: The[0] horrible[1] dictator[0] destroyed[0] the[0] peaceful[1] town[0].\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(trained_model.device)\n",
    "with torch.no_grad():\n",
    "    outputs = trained_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=64,\n",
    "        num_beams=4,      # optional: beam search\n",
    "        do_sample=False,  # deterministic\n",
    "    )\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mddJE0oaogD9"
   },
   "source": [
    "## WNC test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2999,
     "status": "ok",
     "timestamp": 1762742517104,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "cpjN0nX7oY_F",
    "outputId": "8ede2c7e-2b0a-4278-9f04-34e68d67defb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1000/1000 [00:00<00:00, 6970.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies as needed:\n",
    "# pip install kagglehub[hf-datasets]\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "\n",
    "\n",
    "file_path = os.path.join(folder_path, \"data/WNC/WNC/biased.word.test\")\n",
    "df_mcn = pd.read_csv(\n",
    "    file_path,\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"id\",\"src_tok\",\"tgt_tok\",\"biased\",\"neutral\",\"src_POS\",\"tgt_PARSE\"],\n",
    "    usecols=[\"biased\",\"neutral\"]\n",
    ")\n",
    "\n",
    "lexicon_path=os.path.join(folder_path, \"data/subjectivity_clues_hltemnlp05/subjclueslen1-HLTEMNLP05.tff\")\n",
    "def load_mpqa_lexicon(path):\n",
    "    subj_dict = {}\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            entry = {}\n",
    "            for p in parts:\n",
    "                if '=' in p: # Check if '=' exists in the part\n",
    "                    key, value = p.split('=', 1) # Split only once\n",
    "                    entry[key] = value\n",
    "            word = entry.get(\"word1\", \"\").lower()\n",
    "            subj_type = entry.get(\"type\", \"\")\n",
    "            polarity = entry.get(\"priorpolarity\", \"\")\n",
    "            if word:\n",
    "                subj_dict[word] = {\"type\": subj_type, \"polarity\": polarity}\n",
    "    return subj_dict\n",
    "mpqa_dict = load_mpqa_lexicon(lexicon_path)\n",
    "\n",
    "def tag_subjectivity(sentence, lexicon):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    flags = []\n",
    "    for tok in tokens:\n",
    "        w = tok.lower()\n",
    "        if w in lexicon:\n",
    "            subj_type = lexicon[w][\"type\"]\n",
    "            flags.append(1 if subj_type in [\"strongsubj\"] else 0) #\"weaksubj\"\n",
    "        else:\n",
    "            flags.append(0)\n",
    "    return tokens, flags\n",
    "tqdm.pandas()\n",
    "\n",
    "df_mcn[\"tokens_flags\"] = df_mcn[\"biased\"].progress_apply(\n",
    "    lambda x: tag_subjectivity(x, mpqa_dict)\n",
    ")\n",
    "\n",
    "df_mcn[\"tokens\"] = df_mcn[\"tokens_flags\"].apply(lambda x: x[0])\n",
    "df_mcn[\"flags\"] = df_mcn[\"tokens_flags\"].apply(lambda x: x[1])\n",
    "\n",
    "df_mcn = df_mcn.drop(columns=[\"tokens_flags\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4619
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1762744661562,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "IBvsSUzTxOZs",
    "outputId": "29813830-3327-4d02-a6c9-079be7de4f52"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df_mcn\",\n  \"rows\": 1000,\n  \"fields\": [\n    {\n      \"column\": \"biased\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"in 2001, oxford high school was threatened with a lawsuit by the freud family, who rightfully accused the school of excluding their daughter because her academic results were not of a high enough standard.\",\n          \"faith in the theravda tradition is generally confidence based on first-hand understanding of a concept - especially in the primary texts as faith in the reality of the enlightenment of the lord buddha (pali: tathgatabodhisaddh) or in the pli commentaries as:\",\n          \"the town is famous for a fascinating cave called the king's cave and is home to the notre dame university louaize.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"neutral\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"in 2001, oxford high school was threatened with a lawsuit by the freud family, who accused the school of excluding their daughter because her academic results were not of a high enough standard.\",\n          \"faith in the theravda tradition is generally confidence based on first-hand understanding of a concept - especially in the primary texts as faith in the reality of the enlightenment of the buddha (pali: tathgatabodhisaddh) or in the pli commentaries as:\",\n          \"the town is famous for a cave called the king's cave and is home to the notre dame university louaize.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tokens\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"flags\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "df_mcn"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-965fd522-0ab8-4e09-83bd-4bd02eb314ce\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>biased</th>\n",
       "      <th>neutral</th>\n",
       "      <th>tokens</th>\n",
       "      <th>flags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>in april 2009 a brazilian human rights group, ...</td>\n",
       "      <td>in april 2009 a brazilian human rights group, ...</td>\n",
       "      <td>[in, april, 2009, a, brazilian, human, rights,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the 51 day standoff and ensuing murder of 76 m...</td>\n",
       "      <td>the 51 day standoff and ensuing deaths of 76 m...</td>\n",
       "      <td>[the, 51, day, standoff, and, ensuing, murder,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mark oaten (born 8 march 1964, watford) is a d...</td>\n",
       "      <td>mark oaten (born 8 march 1964, watford) is a l...</td>\n",
       "      <td>[mark, oaten, (, born, 8, march, 1964, ,, watf...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>another infamous period of colonisation in anc...</td>\n",
       "      <td>another period of colonisation in ancient time...</td>\n",
       "      <td>[another, infamous, period, of, colonisation, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>photo sequence of astonishing 2005 chicagoland...</td>\n",
       "      <td>photo sequence of 2005 chicagoland crash with ...</td>\n",
       "      <td>[photo, sequence, of, astonishing, 2005, chica...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>especially as the cost to refurbish and modify...</td>\n",
       "      <td>especially as the cost to refurbish and modify...</td>\n",
       "      <td>[especially, as, the, cost, to, refurbish, and...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>blood libels against jews are incorrect accusa...</td>\n",
       "      <td>blood libels against jews are accusations that...</td>\n",
       "      <td>[blood, libels, against, jews, are, incorrect,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>jacob collier (born august 2, 1994) is a singe...</td>\n",
       "      <td>jacob collier (born august 2, 1994) is a singe...</td>\n",
       "      <td>[jacob, collier, (, born, august, 2, ,, 1994, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>they immediately decided to apply it to the an...</td>\n",
       "      <td>they immediately decided to apply it to the ne...</td>\n",
       "      <td>[they, immediately, decided, to, apply, it, to...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>the third section deals with the supposed pupp...</td>\n",
       "      <td>the third section deals with the supposed pupp...</td>\n",
       "      <td>[the, third, section, deals, with, the, suppos...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows  4 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-965fd522-0ab8-4e09-83bd-4bd02eb314ce')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-965fd522-0ab8-4e09-83bd-4bd02eb314ce button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-965fd522-0ab8-4e09-83bd-4bd02eb314ce');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-39243c5d-bdac-434b-bdbb-68226c3ed48a\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-39243c5d-bdac-434b-bdbb-68226c3ed48a')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-39243c5d-bdac-434b-bdbb-68226c3ed48a button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "  <div id=\"id_399e5e80-9670-405e-a87d-8526fb6ce7c6\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_mcn')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_399e5e80-9670-405e-a87d-8526fb6ce7c6 button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('df_mcn');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                                biased  \\\n",
       "0    in april 2009 a brazilian human rights group, ...   \n",
       "1    the 51 day standoff and ensuing murder of 76 m...   \n",
       "2    mark oaten (born 8 march 1964, watford) is a d...   \n",
       "3    another infamous period of colonisation in anc...   \n",
       "4    photo sequence of astonishing 2005 chicagoland...   \n",
       "..                                                 ...   \n",
       "995  especially as the cost to refurbish and modify...   \n",
       "996  blood libels against jews are incorrect accusa...   \n",
       "997  jacob collier (born august 2, 1994) is a singe...   \n",
       "998  they immediately decided to apply it to the an...   \n",
       "999  the third section deals with the supposed pupp...   \n",
       "\n",
       "                                               neutral  \\\n",
       "0    in april 2009 a brazilian human rights group, ...   \n",
       "1    the 51 day standoff and ensuing deaths of 76 m...   \n",
       "2    mark oaten (born 8 march 1964, watford) is a l...   \n",
       "3    another period of colonisation in ancient time...   \n",
       "4    photo sequence of 2005 chicagoland crash with ...   \n",
       "..                                                 ...   \n",
       "995  especially as the cost to refurbish and modify...   \n",
       "996  blood libels against jews are accusations that...   \n",
       "997  jacob collier (born august 2, 1994) is a singe...   \n",
       "998  they immediately decided to apply it to the ne...   \n",
       "999  the third section deals with the supposed pupp...   \n",
       "\n",
       "                                                tokens  \\\n",
       "0    [in, april, 2009, a, brazilian, human, rights,...   \n",
       "1    [the, 51, day, standoff, and, ensuing, murder,...   \n",
       "2    [mark, oaten, (, born, 8, march, 1964, ,, watf...   \n",
       "3    [another, infamous, period, of, colonisation, ...   \n",
       "4    [photo, sequence, of, astonishing, 2005, chica...   \n",
       "..                                                 ...   \n",
       "995  [especially, as, the, cost, to, refurbish, and...   \n",
       "996  [blood, libels, against, jews, are, incorrect,...   \n",
       "997  [jacob, collier, (, born, august, 2, ,, 1994, ...   \n",
       "998  [they, immediately, decided, to, apply, it, to...   \n",
       "999  [the, third, section, deals, with, the, suppos...   \n",
       "\n",
       "                                                 flags  \n",
       "0    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...  \n",
       "3              [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "4                    [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]  \n",
       "..                                                 ...  \n",
       "995  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "996  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "997  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "998               [0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]  \n",
       "999  [0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_mcn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1762742587243,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "LyvTmOaEpTCe",
    "outputId": "b7f48c41-2cda-4096-f1c0-cd0c9f06feca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /content/drive/MyDrive/CS4650/project/data/WNC/processed/mpqa_lexicon/biased_word_test.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "output_path = os.path.join(folder_path, \"data/WNC/processed/mpqa_lexicon/biased_word_test.csv\")\n",
    "output_dir = os.path.dirname(output_path)\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "df_mcn.to_csv(output_path, index=False)\n",
    "print(\"Saved:\", output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 50,
     "status": "ok",
     "timestamp": 1762742667837,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "XgqAodS1pbKa",
    "outputId": "52a22d9d-06f8-4fa8-a7cd-f4172bc8ea39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /content/drive/MyDrive/CS4650/project/data/WNC/processed/mpqa_lexicon/input_target_pair_biased_word_test.csv\n"
     ]
    }
   ],
   "source": [
    "mcn_processed_path=os.path.join(folder_path, \"data/WNC/processed/mpqa_lexicon/biased_word_test.csv\")\n",
    "df=pd.read_csv(mcn_processed_path)\n",
    "\n",
    "def format_input(tokens, flags):\n",
    "    words = tokens.strip(\"[]\").split(\", \")\n",
    "    flags = flags.strip(\"[]\").split(\", \")\n",
    "    words = [w.replace(\"'\", \"\") for w in words]\n",
    "    combined = \" \".join([f\"{w}[{f}]\" for w, f in zip(words, flags)])\n",
    "    return \"neutralize: \" + combined\n",
    "\n",
    "df[\"input_text\"] = [format_input(t, f) for t, f in zip(df[\"tokens\"], df[\"flags\"])]\n",
    "df[\"target_text\"] = df[\"neutral\"]\n",
    "output_path = os.path.join(folder_path, \"data/WNC/processed/mpqa_lexicon/input_target_pair_biased_word_test.csv\")\n",
    "output_dir = os.path.dirname(output_path)\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "df[[\"input_text\", \"target_text\"]].to_csv(output_path, index=False)\n",
    "print(\"Saved:\", output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_lAtRFu8pwoM"
   },
   "source": [
    "## WNC test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X8Iu978qpzXr"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336,
     "referenced_widgets": [
      "891ccf2daed244639133ebc7eb386f9f",
      "77463bb1c1b74777b29b3a07cec07675",
      "43587b1be55d444e9c39dfc32f07124a",
      "01950bc3bd0a4de08630aeda16c95f9a",
      "b452f3b15b2141798f8e6f0e8b54c82d",
      "02857d4860ae4e55ae0b76b4b514987c",
      "a718fd6162524e1ca3c8ce2489bceed8",
      "5979af5630e14f7e9d697dfab810aeea",
      "ef1477585cf8432ea25696a87cf9612c",
      "c8e21ff9da4b431da32866a33acd8b16",
      "82c72b74d13c401694edd910d86db0e4",
      "2c561bc4f4954f988d95960b1d15dd04",
      "9e25167ed313465fb5b63a3498c18c18",
      "4bbd988667234db8aa5339ea276ae55b",
      "7aa582ee9421467db98487fe678dbbfe",
      "4892a74630454f76bcc88f8abb03fe47",
      "f09aa5b5ebe4413cab9f4038e16a6798",
      "fc2afb9f5b04420e8c4caecc501bdfea",
      "4a10b2a54ae0494b9d68e64277e26cd1",
      "a941b6381a14462886456a69b2df1226",
      "970755e5da1b4f109959982044dc41e8",
      "a077d12a2065457c9b70d37715c4b29c",
      "5e3ba0c221f34110b1d6aaa6e73551fb",
      "f071980ea3c948a6b0a14054d8929b1b",
      "563c21d3975b406ebb20557235a121c8",
      "feca801cb60a45e08d1183d6722ab72f",
      "6d6494437590447e94f8d6ea427763ec",
      "e8ed2480d9d143e08df54d71cf56f4c7",
      "fa02292d27cb4d5d9415b8332c6d7402",
      "3e5556b94a6349e1862e6ae568ae7480",
      "2d22d785764c47568de236732523490b",
      "e6eecdb0210e4e6cb9a3b2cbe52d462b",
      "d257d31e3c0c48d096168360ba872a4c",
      "2ddfe90f870c4a12a036ce65041dc360",
      "855d8d32957e462da4176ab721965b78",
      "860e72673b2942c3b7995ffc835228dc",
      "2a671b26309f4efeb0abe8a3af67621d",
      "91d6fd92c1ca4fa59d4e5db4dcdefa61",
      "65f10e5c3c5746a5a2486316189c9b87",
      "d9273cbf5fb44a9b997d21876ac65da3",
      "90d536cee008470eb133c4449c79dfc2",
      "52eb91254f8746e4808d52fcd57d44f3",
      "9f9b18307012407b8188550ab3ca8189",
      "41603d97e2f94ba7a984354ad33f9975",
      "c45eada58df445faa5b2c68ec1856cf0",
      "ca29feb83d6b474f94928e70e40b2613",
      "59f0f24f337948e29e64927b99f6ffd1",
      "dc0e2516ea88418fbd23d32ee63d75ee",
      "57d5995774f04cd18d4732dfde5c8477",
      "a5449cf8527b4becaf39aa6feccafd66",
      "fa0bde3a677b4376b22c6d0fe366198a",
      "a024042a937346688ca077f96c03976c",
      "7a93bcf9c5944603bd37d292218f8778",
      "16a72e2157084a3d9eb25dc2a3bc0c5b",
      "7ac5008f255641dc8fc5c26a752467b3"
     ]
    },
    "executionInfo": {
     "elapsed": 2319,
     "status": "ok",
     "timestamp": 1762746125700,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "ituEjT8PqJmY",
    "outputId": "b304b66a-e587-4b1b-dbd8-c75e764b61e1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "891ccf2daed244639133ebc7eb386f9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c561bc4f4954f988d95960b1d15dd04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e3ba0c221f34110b1d6aaa6e73551fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ddfe90f870c4a12a036ce65041dc360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c45eada58df445faa5b2c68ec1856cf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# @title data\n",
    "dataset_test = load_dataset(\"csv\", data_files=os.path.join(folder_path, \"data/WNC/processed/mpqa_lexicon/input_target_pair_biased_word_test.csv\"))\n",
    "max_length = 128\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "def preprocess_test(batch):\n",
    "    inputs = tokenizer(batch[\"input_text\"], max_length=max_length, truncation=True, padding=\"max_length\")\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(batch[\"target_text\"], max_length=max_length, truncation=True, padding=\"max_length\")\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "test_dataset = dataset_test.map(preprocess_test, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30566,
     "status": "ok",
     "timestamp": 1762746160001,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "sXHhRflPqcF9",
    "outputId": "159bf6c3-5a6a-4d63-805b-8338fa67aa21"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @title load model\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "checkpoint_path = os.path.join(folder_path,\n",
    "    \"model_checkpoints/wnc-mpqa-flan_t5-2loss-11_09/checkpoint-6000\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "trained_model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_path)\n",
    "trained_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9844,
     "status": "ok",
     "timestamp": 1762746626185,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "GGk-8a9EqjEg",
    "outputId": "b6848629-90bc-4698-a630-8238675a72b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (0.4.6)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.0.2)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.36.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (25.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.20.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2025.10.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.12/dist-packages (2.5.1)\n",
      "Requirement already satisfied: portalocker in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (3.2.0)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2024.11.6)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2.0.2)\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (5.4.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-80161039.py:42: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "# @title bleu score\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "!pip install evaluate\n",
    "!pip install sacrebleu\n",
    "import evaluate\n",
    "import torch\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    # Ensure predictions and labels are on the correct device and are of the correct type\n",
    "    preds = torch.tensor(preds, dtype=torch.int64)\n",
    "    labels = torch.tensor(labels, dtype=torch.int64)\n",
    "\n",
    "    # Replace potential out-of-vocabulary predicted IDs with pad_token_id\n",
    "    preds[preds >= tokenizer.vocab_size] = tokenizer.pad_token_id\n",
    "    preds[preds < 0] = tokenizer.pad_token_id # Also handle potential negative indices\n",
    "\n",
    "    # Replace -100 with pad_token_id in labels for correct decoding\n",
    "    labels = labels.clone()\n",
    "    labels[labels == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Ensure references are in the correct format for sacrebleu\n",
    "    # sacrebleu expects a list of lists for references\n",
    "    bleu = metric.compute(predictions=decoded_preds, references=[[l] for l in decoded_labels])\n",
    "    return {\"bleu\": bleu[\"score\"]}\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./eval_results\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,\n",
    "    report_to=\"none\",\n",
    "    generation_max_length=128,     # allow up to 128 tokens in output\n",
    "    generation_num_beams=4,        # optional, improves quality (beam search)\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=trained_model,\n",
    "    args=args,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "executionInfo": {
     "elapsed": 262605,
     "status": "ok",
     "timestamp": 1762746888833,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "UKt7rZVcr6J7",
    "outputId": "7b1c916c-01a1-466e-c1f0-4f52b98735e3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 04:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_train_loss': 0.2901538908481598, 'eval_train_model_preparation_time': 0.0068, 'eval_train_bleu': 69.8724397721992, 'eval_train_runtime': 262.6013, 'eval_train_samples_per_second': 3.808, 'eval_train_steps_per_second': 0.476}\n"
     ]
    }
   ],
   "source": [
    "results = trainer.evaluate(test_dataset)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "executionInfo": {
     "elapsed": 260162,
     "status": "error",
     "timestamp": 1762747162552,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "uNUBzTLZso3B",
    "outputId": "f7462b11-e479-4425-a5f0-96e93b5a1c5f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-3172449636.py:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OverflowError",
     "evalue": "out of range integral type conversion attempted",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3172449636.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# run prediction on the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m preds=tokenizer.batch_decode(\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m )\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_decode\u001b[0;34m(self, sequences, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3883\u001b[0m         \"\"\"\n\u001b[1;32m   3884\u001b[0m         return [\n\u001b[0;32m-> 3885\u001b[0;31m             self.decode(\n\u001b[0m\u001b[1;32m   3886\u001b[0m                 \u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3887\u001b[0m                 \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3922\u001b[0m         \u001b[0mtoken_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_py_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3924\u001b[0;31m         return self._decode(\n\u001b[0m\u001b[1;32m   3925\u001b[0m             \u001b[0mtoken_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3926\u001b[0m             \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m    680\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m             \u001b[0mtoken_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m         clean_up_tokenization_spaces = (\n",
      "\u001b[0;31mOverflowError\u001b[0m: out of range integral type conversion attempted"
     ]
    }
   ],
   "source": [
    "# @title run predictions on test set\n",
    "\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "# make sure predict_with_generate=True so it uses model.generate()\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./eval_results\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,  # turn off on GPU\n",
    "    report_to=\"none\",\n",
    "    generation_max_length=128,     # allow up to 128 tokens in output\n",
    "    generation_num_beams=4,        # optional, improves quality (beam search)\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=trained_model,\n",
    "    args=args,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "predictions = trainer.predict(test_dataset[\"train\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rwaziVkC7HI5"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Replace potential out-of-vocabulary predicted IDs with pad_token_id\n",
    "predictions.predictions[predictions.predictions >= tokenizer.vocab_size] = tokenizer.pad_token_id\n",
    "predictions.predictions[predictions.predictions < 0] = tokenizer.pad_token_id # Also handle potential negative indices\n",
    "\n",
    "preds=tokenizer.batch_decode(\n",
    "    predictions.predictions, skip_special_tokens=True\n",
    ")\n",
    "labels = tokenizer.batch_decode(\n",
    "    predictions.label_ids, skip_special_tokens=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 694
    },
    "executionInfo": {
     "elapsed": 460,
     "status": "ok",
     "timestamp": 1762747255778,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "bVl4nt2JuB18",
    "outputId": "84f78eb8-4e14-46d4-ce4d-d63ea2ee1494"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"display(df_results\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"input_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"neutralize: they[0] see[0] the[0] calling[0] of[0] such[0] councils[0] ,[0] for[0] example[0] ,[0] by[0] a[0] roman[0] emperor[0] lacking[1] the[0] divine[1] authority[0] as[0] preposterous[1] and[0] assert[0] that[0] the[0] emperors[0] used[0] the[0] councils[0] to[0] exercise[0] their[0] influence[0] to[0] shape[0] and[0] institute[0] christianity[0] to[0] their[0] liking[1] .[0]\",\n          \"neutralize: the[0] 51[0] day[0] standoff[0] and[0] ensuing[0] murder[0] of[0] 76[0] men[0] ,[0] women[0] ,[0] and[0] children[0] --[0] the[0] branch[0] davidians[0] --[0] in[0] waco[0] ,[0] texas[0] .[0]\",\n          \"neutralize: jesus[0] of[0] nazareth[0] is[0] probably[0] mentioned[0] in[0] two[0] passages[0] of[0] the[0] work[0] the[0] antiquities[0] of[0] the[0] jews[0] by[0] the[0] jewish[0] historian[0] josephus[0] ,[0] written[0] in[0] the[0] late[0] first[0] century[0] ad[0] .[0]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"they see the calling of such councils, for example, by a roman emperor lacking the divine authority as groundless and assert that the emperors used the councils to exercise their influence to shape and institute christianity to their liking.\",\n          \"the 51 day standoff and ensuing deaths of 76 men, women, and children--the branch davidians--in waco, texas.\",\n          \"jesus of nazareth is possibly mentioned in two passages of the work the antiquities of the jews by the jewish historian josephus, written in the late first century ad.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"predicted_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"they see the calling of such councils, for example, by a roman emperor lacking the divine authority as preposterous and assert that the emperors used the divine authority.\",\n          \"the 51 day standoff and ensuing murder of 76 men, women, and children -- the branch davidians -- in waco, texas.\",\n          \"jesus of nazareth is probably mentioned in two passages of the work the antiquities of the jews by the jewish historian josephus, written in the late first century.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-0ef266b7-c8e1-47b1-b8f7-d6ac0c7d0648\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "      <th>predicted_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutralize: in[0] april[0] 2009[0] a[0] brazil...</td>\n",
       "      <td>in april 2009 a brazilian human rights group, ...</td>\n",
       "      <td>in april 2009 a brazilian human rights group, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutralize: the[0] 51[0] day[0] standoff[0] an...</td>\n",
       "      <td>the 51 day standoff and ensuing deaths of 76 m...</td>\n",
       "      <td>the 51 day standoff and ensuing murder of 76 m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutralize: mark[0] oaten[0] ([0] born[0] 8[0]...</td>\n",
       "      <td>mark oaten (born 8 march 1964, watford) is a l...</td>\n",
       "      <td>mark oaten (born 8 march 1964, watford) is a l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutralize: another[0] infamous[1] period[0] o...</td>\n",
       "      <td>another period of colonisation in ancient time...</td>\n",
       "      <td>another period of colonisation in ancient time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutralize: photo[0] sequence[0] of[0] astonis...</td>\n",
       "      <td>photo sequence of 2005 chicagoland crash with ...</td>\n",
       "      <td>photo sequence of 2005 chicagoland crash with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>neutralize: jesus[0] of[0] nazareth[0] is[0] p...</td>\n",
       "      <td>jesus of nazareth is possibly mentioned in two...</td>\n",
       "      <td>jesus of nazareth is probably mentioned in two...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>neutralize: israeli[0] attempts[0] to[0] reduc...</td>\n",
       "      <td>israeli efforts to reduce gazan civilian casua...</td>\n",
       "      <td>israeli attempts to reduce gazan civilian casu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>neutralize: his[0] 45-year[0] career[0] exceed...</td>\n",
       "      <td>his 45-year career was longer than that of any...</td>\n",
       "      <td>his 45-year career exceeded that of any other ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>neutralize: they[0] see[0] the[0] calling[0] o...</td>\n",
       "      <td>they see the calling of such councils, for exa...</td>\n",
       "      <td>they see the calling of such councils, for exa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>neutralize: in[0] the[0] earliest[0] models[0]...</td>\n",
       "      <td>in the earliest models (earlier than a macinto...</td>\n",
       "      <td>in the earliest models (later than a macintosh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0ef266b7-c8e1-47b1-b8f7-d6ac0c7d0648')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-0ef266b7-c8e1-47b1-b8f7-d6ac0c7d0648 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-0ef266b7-c8e1-47b1-b8f7-d6ac0c7d0648');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-3782c02f-fc20-4084-9d55-3598dc30fd31\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3782c02f-fc20-4084-9d55-3598dc30fd31')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-3782c02f-fc20-4084-9d55-3598dc30fd31 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                          input_text  \\\n",
       "0  neutralize: in[0] april[0] 2009[0] a[0] brazil...   \n",
       "1  neutralize: the[0] 51[0] day[0] standoff[0] an...   \n",
       "2  neutralize: mark[0] oaten[0] ([0] born[0] 8[0]...   \n",
       "3  neutralize: another[0] infamous[1] period[0] o...   \n",
       "4  neutralize: photo[0] sequence[0] of[0] astonis...   \n",
       "5  neutralize: jesus[0] of[0] nazareth[0] is[0] p...   \n",
       "6  neutralize: israeli[0] attempts[0] to[0] reduc...   \n",
       "7  neutralize: his[0] 45-year[0] career[0] exceed...   \n",
       "8  neutralize: they[0] see[0] the[0] calling[0] o...   \n",
       "9  neutralize: in[0] the[0] earliest[0] models[0]...   \n",
       "\n",
       "                                         target_text  \\\n",
       "0  in april 2009 a brazilian human rights group, ...   \n",
       "1  the 51 day standoff and ensuing deaths of 76 m...   \n",
       "2  mark oaten (born 8 march 1964, watford) is a l...   \n",
       "3  another period of colonisation in ancient time...   \n",
       "4  photo sequence of 2005 chicagoland crash with ...   \n",
       "5  jesus of nazareth is possibly mentioned in two...   \n",
       "6  israeli efforts to reduce gazan civilian casua...   \n",
       "7  his 45-year career was longer than that of any...   \n",
       "8  they see the calling of such councils, for exa...   \n",
       "9  in the earliest models (earlier than a macinto...   \n",
       "\n",
       "                                      predicted_text  \n",
       "0  in april 2009 a brazilian human rights group, ...  \n",
       "1  the 51 day standoff and ensuing murder of 76 m...  \n",
       "2  mark oaten (born 8 march 1964, watford) is a l...  \n",
       "3  another period of colonisation in ancient time...  \n",
       "4  photo sequence of 2005 chicagoland crash with ...  \n",
       "5  jesus of nazareth is probably mentioned in two...  \n",
       "6  israeli attempts to reduce gazan civilian casu...  \n",
       "7  his 45-year career exceeded that of any other ...  \n",
       "8  they see the calling of such councils, for exa...  \n",
       "9  in the earliest models (later than a macintosh...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_results = pd.DataFrame({\n",
    "    \"input_text\": dataset_test[\"train\"][\"input_text\"],\n",
    "    \"target_text\": labels,\n",
    "    \"predicted_text\": preds\n",
    "})\n",
    "df_results.to_csv(os.path.join(folder_path, \"output/WNC/predictions_testset_wnc-mpqa-flan_t5-2loss-11_09_2.csv\"), index=False)\n",
    "\n",
    "display(df_results.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26166,
     "status": "ok",
     "timestamp": 1762747349618,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "GcalFgkqt882",
    "outputId": "2e4a401a-301c-4884-97ed-f9e09ea88907"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean objectivity score: 0.7474374459087849\n",
      "std: 0.1988511689698736\n",
      "total percent of objective sentences: 0.858\n"
     ]
    }
   ],
   "source": [
    "# @title objectivity evaluation\n",
    "import numpy as np\n",
    "clf_name = \"GroNLP/mdebertav3-subjectivity-english\"\n",
    "clf_tok = AutoTokenizer.from_pretrained(clf_name)\n",
    "clf = AutoModelForSequenceClassification.from_pretrained(clf_name).to(trained_model.device).eval()\n",
    "\n",
    "# predicted text\n",
    "path=os.path.join(folder_path, \"output/WNC/predictions_testset_wnc-mpqa-flan_t5-2loss-11_09_2.csv\")\n",
    "df=pd.read_csv(path)\n",
    "\n",
    "scores = []\n",
    "count_objective=0\n",
    "for text in df[\"predicted_text\"]:\n",
    "    tokens = clf_tok(text, return_tensors=\"pt\", truncation=True).to(trained_model.device)\n",
    "    with torch.no_grad():\n",
    "        probs = clf(**tokens).logits.softmax(dim=-1)\n",
    "        p_objective = probs[0][0].item()  # index 0 = objective\n",
    "        if p_objective>0.5:\n",
    "          count_objective+=1\n",
    "\n",
    "    scores.append(p_objective)\n",
    "\n",
    "print(\"Mean objectivity score:\", np.mean(scores))\n",
    "print(\"std:\", np.std(scores))\n",
    "print(\"total percent of objective sentences:\", count_objective/len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23688,
     "status": "ok",
     "timestamp": 1762747393772,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "SEmiP1xSven6",
    "outputId": "f76df050-b2cf-4989-a30d-c8cd6a459dbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean objectivity score for original text: 0.7246217877790332\n",
      "std: 0.20445795079084508\n",
      "total percent of objective sentences: 0.4085\n"
     ]
    }
   ],
   "source": [
    "# original text:\n",
    "path=os.path.join(folder_path, \"data/WNC/processed/mpqa_lexicon/biased_word_test.csv\")\n",
    "df=pd.read_csv(path)\n",
    "count_objective=0\n",
    "for text in df[\"biased\"]:\n",
    "  tokens = clf_tok(text, return_tensors=\"pt\", truncation=True).to(trained_model.device)\n",
    "  with torch.no_grad():\n",
    "      probs = clf(**tokens).logits.softmax(dim=-1)\n",
    "      p_objective = probs[0][0].item()  # index 0 = objective\n",
    "      if p_objective>0.5:\n",
    "          count_objective+=1\n",
    "  scores.append(p_objective)\n",
    "\n",
    "\n",
    "print(\"Mean objectivity score for original text:\", np.mean(scores))\n",
    "print(\"std:\", np.std(scores))\n",
    "print(\"total percent of objective sentences:\", count_objective/len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23111,
     "status": "ok",
     "timestamp": 1762747416882,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "OfUaP-DmvwJO",
    "outputId": "a9d8dd96-2c80-4522-dbb4-4233b00d291f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean objectivity score for original text: 0.7344526325513919\n",
      "std: 0.20066150870500848\n",
      "total percent of objective sentences: 0.291\n"
     ]
    }
   ],
   "source": [
    "# target text:\n",
    "path=os.path.join(folder_path, \"data/WNC/processed/mpqa_lexicon/biased_word_test.csv\")\n",
    "df=pd.read_csv(path)\n",
    "count_objective=0\n",
    "for text in df[\"neutral\"]:\n",
    "  tokens = clf_tok(text, return_tensors=\"pt\", truncation=True).to(trained_model.device)\n",
    "  with torch.no_grad():\n",
    "      probs = clf(**tokens).logits.softmax(dim=-1)\n",
    "      p_objective = probs[0][0].item()  # index 0 = objective\n",
    "      if p_objective>0.5:\n",
    "          count_objective+=1\n",
    "  scores.append(p_objective)\n",
    "\n",
    "\n",
    "print(\"Mean objectivity score for original text:\", np.mean(scores))\n",
    "print(\"std:\", np.std(scores))\n",
    "print(\"total percent of objective sentences:\", count_objective/len(scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1762747527042,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "P9Qys7CN78kx",
    "outputId": "2bd37981-44be-428b-e04e-fadffc81701c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>in april 2009 a brazilian human rights group, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the 51 day standoff and ensuing deaths of 76 m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mark oaten (born 8 march 1964, watford) is a l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>another period of colonisation in ancient time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>photo sequence of 2005 chicagoland crash with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>especially as the cost to refurbish and modify...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>blood libels against jews are accusations that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>jacob collier (born august 2, 1994) is a singe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>they immediately decided to apply it to the ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>the third section deals with the supposed pupp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows  1 columns</p>\n",
       "</div><br><label><b>dtype:</b> object</label>"
      ],
      "text/plain": [
       "0      in april 2009 a brazilian human rights group, ...\n",
       "1      the 51 day standoff and ensuing deaths of 76 m...\n",
       "2      mark oaten (born 8 march 1964, watford) is a l...\n",
       "3      another period of colonisation in ancient time...\n",
       "4      photo sequence of 2005 chicagoland crash with ...\n",
       "                             ...                        \n",
       "995    especially as the cost to refurbish and modify...\n",
       "996    blood libels against jews are accusations that...\n",
       "997    jacob collier (born august 2, 1994) is a singe...\n",
       "998    they immediately decided to apply it to the ne...\n",
       "999    the third section deals with the supposed pupp...\n",
       "Name: neutral, Length: 1000, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path=os.path.join(folder_path, \"data/WNC/processed/mpqa_lexicon/biased_word_test.csv\")\n",
    "df=pd.read_csv(path)\n",
    "display(df[\"neutral\"])\n",
    "# for text in df[\"neutral\"]:\n",
    "#   print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1762747518068,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "gSjGoYUJ8FM2",
    "outputId": "439fce4e-9ca9-4b66-8466-6fffc9162bc2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>in april 2009 a brazilian human rights group, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the 51 day standoff and ensuing deaths of 76 m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mark oaten (born 8 march 1964, watford) is a l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>another period of colonisation in ancient time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>photo sequence of 2005 chicagoland crash with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>especially as the cost to refurbish and modify...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>blood libels against jews are accusations that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>jacob collier (born august 2, 1994) is a singe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>they immediately decided to apply it to the ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>the third section deals with the supposed pupp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows  1 columns</p>\n",
       "</div><br><label><b>dtype:</b> object</label>"
      ],
      "text/plain": [
       "0      in april 2009 a brazilian human rights group, ...\n",
       "1      the 51 day standoff and ensuing deaths of 76 m...\n",
       "2      mark oaten (born 8 march 1964, watford) is a l...\n",
       "3      another period of colonisation in ancient time...\n",
       "4      photo sequence of 2005 chicagoland crash with ...\n",
       "                             ...                        \n",
       "995    especially as the cost to refurbish and modify...\n",
       "996    blood libels against jews are accusations that...\n",
       "997    jacob collier (born august 2, 1994) is a singe...\n",
       "998    they immediately decided to apply it to the ne...\n",
       "999    the third section deals with the supposed pupp...\n",
       "Name: target_text, Length: 1000, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path=os.path.join(folder_path, \"output/WNC/predictions_testset_wnc-mpqa-flan_t5-2loss-11_09_2.csv\")\n",
    "df=pd.read_csv(path)\n",
    "display(df[\"target_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22649,
     "status": "ok",
     "timestamp": 1762747439533,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "eOtjKbHI7oui",
    "outputId": "d85a8b10-ebcc-4b6f-be3a-65a902e2f9a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean objectivity score: 0.7538096265494824\n",
      "std: 0.19177044989900252\n",
      "total percent of objective sentences: 0.872\n"
     ]
    }
   ],
   "source": [
    "# target text\n",
    "path=os.path.join(folder_path, \"output/WNC/predictions_testset_wnc-mpqa-flan_t5-2loss-11_09_2.csv\")\n",
    "df=pd.read_csv(path)\n",
    "\n",
    "scores = []\n",
    "count_objective=0\n",
    "for text in df[\"target_text\"]:\n",
    "    tokens = clf_tok(text, return_tensors=\"pt\", truncation=True).to(trained_model.device)\n",
    "    with torch.no_grad():\n",
    "        probs = clf(**tokens).logits.softmax(dim=-1)\n",
    "        p_objective = probs[0][0].item()  # index 0 = objective\n",
    "        if p_objective>0.5:\n",
    "          count_objective+=1\n",
    "\n",
    "    scores.append(p_objective)\n",
    "\n",
    "print(\"Mean objectivity score:\", np.mean(scores))\n",
    "print(\"std:\", np.std(scores))\n",
    "print(\"total percent of objective sentences:\", count_objective/len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23414,
     "status": "ok",
     "timestamp": 1762747737296,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "6ef38eda",
    "outputId": "e0ffb730-1936-4bc7-c28f-93f131a4c2a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean objectivity score for tokenized and decoded biased text: 0.70147567717731\n",
      "std: 0.20773824429799856\n",
      "total percent of objective sentences: 0.816\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and decode df[\"biased\"]\n",
    "path=os.path.join(folder_path, \"data/WNC/processed/mpqa_lexicon/biased_word_test.csv\")\n",
    "df=pd.read_csv(path)\n",
    "biased_tokenized_decoded = []\n",
    "for text in df[\"biased\"]:\n",
    "    # Tokenize the text\n",
    "    tokens = tokenizer(text, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    # Decode the tokens back to text\n",
    "    decoded_text = tokenizer.batch_decode(tokens[\"input_ids\"], skip_special_tokens=True)\n",
    "    biased_tokenized_decoded.append(decoded_text[0]) # batch_decode returns a list of lists\n",
    "\n",
    "# Calculate objectivity score for the tokenized and decoded biased text\n",
    "scores_biased_processed = []\n",
    "count_objective_biased_processed = 0\n",
    "for text in biased_tokenized_decoded:\n",
    "    tokens = clf_tok(text, return_tensors=\"pt\", truncation=True).to(trained_model.device)\n",
    "    with torch.no_grad():\n",
    "        probs = clf(**tokens).logits.softmax(dim=-1)\n",
    "        p_objective = probs[0][0].item()  # index 0 = objective\n",
    "        if p_objective > 0.5:\n",
    "            count_objective_biased_processed += 1\n",
    "    scores_biased_processed.append(p_objective)\n",
    "\n",
    "print(\"Mean objectivity score for tokenized and decoded biased text:\", np.mean(scores_biased_processed))\n",
    "print(\"std:\", np.std(scores_biased_processed))\n",
    "print(\"total percent of objective sentences:\", count_objective_biased_processed / len(scores_biased_processed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NhvDjSlGMsgd"
   },
   "source": [
    "# tweets dataset training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kzLj-l-KXZKo"
   },
   "source": [
    "## data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FZd4fGZVMsOo"
   },
   "outputs": [],
   "source": [
    "# @title load and clean\n",
    "\n",
    "import re\n",
    "\n",
    "LABEL_HASHTAGS = {\n",
    "    \"sarcasm\", \"irony\", \"sarcastic\", \"ironic\",\n",
    "    \"satire\", \"satirical\", \"joke\", \"funny\"\n",
    "}\n",
    "\n",
    "def clean_tweet(text):\n",
    "    # lower-case for consistency\n",
    "    text = text.strip()\n",
    "\n",
    "    # 1. remove @mentions\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "\n",
    "    # 2. remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\.\\S+\", \"\", text)\n",
    "\n",
    "    # 3. process hashtags\n",
    "    def replace_hashtag(match):\n",
    "        tag = match.group(1).lower()\n",
    "        if tag in LABEL_HASHTAGS:\n",
    "            return \"\"              # remove style hashtags\n",
    "        else:\n",
    "            return tag             # keep content hashtags but drop '#'\n",
    "\n",
    "    text = re.sub(r\"#(\\w+)\", replace_hashtag, text)\n",
    "\n",
    "    # 4. collapse repeated spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 784
    },
    "executionInfo": {
     "elapsed": 933,
     "status": "ok",
     "timestamp": 1763341322488,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "iVSIZWBuUgtm",
    "outputId": "958676d2-1be0-4db6-da4c-8d15fe5969a2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"print(df_tw[\\\"class\\\"]\",\n  \"rows\": 25,\n  \"fields\": [\n    {\n      \"column\": \"tweets\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 25,\n        \"samples\": [\n          \"for the car ride when I get to listen to you and jess the whole time? Yeah woo can't wait either.\",\n          \"Here we go another day in paradise\",\n          \"Be aware dirty step to get money staylight staywhite moralneeded @\\u2026\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"class\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"figurative\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-90af6e84-f917-4326-af3a-478532fe6ef4\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Be aware dirty step to get money staylight sta...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>for people who don't understand diy artattack</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dailymail readers being sensible as always sho...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why do I get the feeling you like games?</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>- You probably just missed the text.</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tune in to Nigezie and be treated to Rachel Pl...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What iz thiz?!?!? A friggin DC love fest??!?!?...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>man, i wish i could sexually harass an intoxic...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>for the car ride when I get to listen to you a...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Aamir calls bajrangibhaijaan as Salman's best ...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Sorry you can't understand my dark sense of hu...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>work yay fuck goodnight  @ G Town</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Come on..it's against the Mets....that's like ...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>I thought hot spot policing was a plan?? (Whic...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>dont be daft Dr Ciara. Measles doesnt kill, it...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>I dont think any TV show could be more epic th...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Here we go another day in paradise</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>sure does...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>My jobs great because I love what I do (operat...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>quotestags_app funnyquotes insult sarcasticquo...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Loving the coverage of theopen</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Did Strop just pitch around a double with the ...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>You keep them close to see what they're up to....</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Thanks for giving my personal information and ...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>sure but we got to watch a random soccer match...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-90af6e84-f917-4326-af3a-478532fe6ef4')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-90af6e84-f917-4326-af3a-478532fe6ef4 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-90af6e84-f917-4326-af3a-478532fe6ef4');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-42dc535e-9fd6-4762-aec8-d95b19d454e8\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-42dc535e-9fd6-4762-aec8-d95b19d454e8')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-42dc535e-9fd6-4762-aec8-d95b19d454e8 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                               tweets       class\n",
       "0   Be aware dirty step to get money staylight sta...  figurative\n",
       "1       for people who don't understand diy artattack  figurative\n",
       "2   dailymail readers being sensible as always sho...  figurative\n",
       "3            Why do I get the feeling you like games?  figurative\n",
       "4                - You probably just missed the text.  figurative\n",
       "5   Tune in to Nigezie and be treated to Rachel Pl...  figurative\n",
       "6   What iz thiz?!?!? A friggin DC love fest??!?!?...  figurative\n",
       "7   man, i wish i could sexually harass an intoxic...  figurative\n",
       "8   for the car ride when I get to listen to you a...  figurative\n",
       "9   Aamir calls bajrangibhaijaan as Salman's best ...  figurative\n",
       "10  Sorry you can't understand my dark sense of hu...  figurative\n",
       "11                 work yay fuck goodnight  @ G Town  figurative\n",
       "12  Come on..it's against the Mets....that's like ...  figurative\n",
       "13  I thought hot spot policing was a plan?? (Whic...  figurative\n",
       "14  dont be daft Dr Ciara. Measles doesnt kill, it...  figurative\n",
       "15  I dont think any TV show could be more epic th...  figurative\n",
       "16                 Here we go another day in paradise  figurative\n",
       "17                                       sure does...  figurative\n",
       "18  My jobs great because I love what I do (operat...  figurative\n",
       "19  quotestags_app funnyquotes insult sarcasticquo...  figurative\n",
       "20                     Loving the coverage of theopen  figurative\n",
       "21  Did Strop just pitch around a double with the ...  figurative\n",
       "22  You keep them close to see what they're up to....  figurative\n",
       "23  Thanks for giving my personal information and ...  figurative\n",
       "24  sure but we got to watch a random soccer match...  figurative"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class\n",
      "figurative    21229\n",
      "irony         20884\n",
      "sarcasm       20674\n",
      "regular       18595\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tweets_path = os.path.join(folder_path, \"data/tweets/train.csv\")\n",
    "df_tw = pd.read_csv(tweets_path)\n",
    "\n",
    "# Keep only non empty tweets\n",
    "df_tw = df_tw.dropna(subset=[\"tweets\"])\n",
    "df_tw = df_tw[df_tw[\"tweets\"].str.len() > 3].reset_index(drop=True)\n",
    "\n",
    "\n",
    "df_tw[\"tweets\"] = df_tw[\"tweets\"].apply(clean_tweet)\n",
    "df_tw = df_tw[df_tw[\"tweets\"].str.len() > 3]\n",
    "\n",
    "display(df_tw.head(25))\n",
    "print(df_tw[\"class\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2526
    },
    "executionInfo": {
     "elapsed": 7016,
     "status": "ok",
     "timestamp": 1763341472881,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "cFi6ODFPVcxJ",
    "outputId": "4eec81fe-b553-4c1d-9d3e-aa7a8affd1b4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 81382/81382 [00:06<00:00, 11762.10it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df_tw[[\\\"tweets\\\", \\\"tokens\\\", \\\"flags\\\"]]\",\n  \"rows\": 25,\n  \"fields\": [\n    {\n      \"column\": \"tweets\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 25,\n        \"samples\": [\n          \"for the car ride when I get to listen to you and jess the whole time? Yeah woo can't wait either.\",\n          \"Here we go another day in paradise\",\n          \"Be aware dirty step to get money staylight staywhite moralneeded @\\u2026\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tokens\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"flags\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-c818ca17-46f2-48db-9e22-40f611017ab8\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>tokens</th>\n",
       "      <th>flags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Be aware dirty step to get money staylight sta...</td>\n",
       "      <td>[Be, aware, dirty, step, to, get, money, stayl...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>for people who don't understand diy artattack</td>\n",
       "      <td>[for, people, who, do, n't, understand, diy, a...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dailymail readers being sensible as always sho...</td>\n",
       "      <td>[dailymail, readers, being, sensible, as, alwa...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why do I get the feeling you like games?</td>\n",
       "      <td>[Why, do, I, get, the, feeling, you, like, gam...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>- You probably just missed the text.</td>\n",
       "      <td>[-, You, probably, just, missed, the, text, .]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tune in to Nigezie and be treated to Rachel Pl...</td>\n",
       "      <td>[Tune, in, to, Nigezie, and, be, treated, to, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What iz thiz?!?!? A friggin DC love fest??!?!?...</td>\n",
       "      <td>[What, iz, thiz, ?, !, ?, !, ?, A, friggin, DC...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>man, i wish i could sexually harass an intoxic...</td>\n",
       "      <td>[man, ,, i, wish, i, could, sexually, harass, ...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>for the car ride when I get to listen to you a...</td>\n",
       "      <td>[for, the, car, ride, when, I, get, to, listen...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Aamir calls bajrangibhaijaan as Salman's best ...</td>\n",
       "      <td>[Aamir, calls, bajrangibhaijaan, as, Salman, '...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Sorry you can't understand my dark sense of hu...</td>\n",
       "      <td>[Sorry, you, ca, n't, understand, my, dark, se...</td>\n",
       "      <td>[1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>work yay fuck goodnight  @ G Town</td>\n",
       "      <td>[work, yay, fuck, goodnight, , @, G, Town]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Come on..it's against the Mets....that's like ...</td>\n",
       "      <td>[Come, on, .., it, 's, against, the, Mets, ......</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>I thought hot spot policing was a plan?? (Whic...</td>\n",
       "      <td>[I, thought, hot, spot, policing, was, a, plan...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>dont be daft Dr Ciara. Measles doesnt kill, it...</td>\n",
       "      <td>[dont, be, daft, Dr, Ciara, ., Measles, doesnt...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>I dont think any TV show could be more epic th...</td>\n",
       "      <td>[I, dont, think, any, TV, show, could, be, mor...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Here we go another day in paradise</td>\n",
       "      <td>[Here, we, go, another, day, in, paradise]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>sure does...</td>\n",
       "      <td>[sure, does, ...]</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>My jobs great because I love what I do (operat...</td>\n",
       "      <td>[My, jobs, great, because, I, love, what, I, d...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>quotestags_app funnyquotes insult sarcasticquo...</td>\n",
       "      <td>[quotestags_app, funnyquotes, insult, sarcasti...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Loving the coverage of theopen</td>\n",
       "      <td>[Loving, the, coverage, of, theopen]</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Did Strop just pitch around a double with the ...</td>\n",
       "      <td>[Did, Strop, just, pitch, around, a, double, w...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>You keep them close to see what they're up to....</td>\n",
       "      <td>[You, keep, them, close, to, see, what, they, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Thanks for giving my personal information and ...</td>\n",
       "      <td>[Thanks, for, giving, my, personal, informatio...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>sure but we got to watch a random soccer match...</td>\n",
       "      <td>[sure, but, we, got, to, watch, a, random, soc...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c818ca17-46f2-48db-9e22-40f611017ab8')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-c818ca17-46f2-48db-9e22-40f611017ab8 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-c818ca17-46f2-48db-9e22-40f611017ab8');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-d52dc905-819d-4132-8f27-c415398a6056\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d52dc905-819d-4132-8f27-c415398a6056')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-d52dc905-819d-4132-8f27-c415398a6056 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                               tweets  \\\n",
       "0   Be aware dirty step to get money staylight sta...   \n",
       "1       for people who don't understand diy artattack   \n",
       "2   dailymail readers being sensible as always sho...   \n",
       "3            Why do I get the feeling you like games?   \n",
       "4                - You probably just missed the text.   \n",
       "5   Tune in to Nigezie and be treated to Rachel Pl...   \n",
       "6   What iz thiz?!?!? A friggin DC love fest??!?!?...   \n",
       "7   man, i wish i could sexually harass an intoxic...   \n",
       "8   for the car ride when I get to listen to you a...   \n",
       "9   Aamir calls bajrangibhaijaan as Salman's best ...   \n",
       "10  Sorry you can't understand my dark sense of hu...   \n",
       "11                 work yay fuck goodnight  @ G Town   \n",
       "12  Come on..it's against the Mets....that's like ...   \n",
       "13  I thought hot spot policing was a plan?? (Whic...   \n",
       "14  dont be daft Dr Ciara. Measles doesnt kill, it...   \n",
       "15  I dont think any TV show could be more epic th...   \n",
       "16                 Here we go another day in paradise   \n",
       "17                                       sure does...   \n",
       "18  My jobs great because I love what I do (operat...   \n",
       "19  quotestags_app funnyquotes insult sarcasticquo...   \n",
       "20                     Loving the coverage of theopen   \n",
       "21  Did Strop just pitch around a double with the ...   \n",
       "22  You keep them close to see what they're up to....   \n",
       "23  Thanks for giving my personal information and ...   \n",
       "24  sure but we got to watch a random soccer match...   \n",
       "\n",
       "                                               tokens  \\\n",
       "0   [Be, aware, dirty, step, to, get, money, stayl...   \n",
       "1   [for, people, who, do, n't, understand, diy, a...   \n",
       "2   [dailymail, readers, being, sensible, as, alwa...   \n",
       "3   [Why, do, I, get, the, feeling, you, like, gam...   \n",
       "4      [-, You, probably, just, missed, the, text, .]   \n",
       "5   [Tune, in, to, Nigezie, and, be, treated, to, ...   \n",
       "6   [What, iz, thiz, ?, !, ?, !, ?, A, friggin, DC...   \n",
       "7   [man, ,, i, wish, i, could, sexually, harass, ...   \n",
       "8   [for, the, car, ride, when, I, get, to, listen...   \n",
       "9   [Aamir, calls, bajrangibhaijaan, as, Salman, '...   \n",
       "10  [Sorry, you, ca, n't, understand, my, dark, se...   \n",
       "11        [work, yay, fuck, goodnight, , @, G, Town]   \n",
       "12  [Come, on, .., it, 's, against, the, Mets, ......   \n",
       "13  [I, thought, hot, spot, policing, was, a, plan...   \n",
       "14  [dont, be, daft, Dr, Ciara, ., Measles, doesnt...   \n",
       "15  [I, dont, think, any, TV, show, could, be, mor...   \n",
       "16         [Here, we, go, another, day, in, paradise]   \n",
       "17                                  [sure, does, ...]   \n",
       "18  [My, jobs, great, because, I, love, what, I, d...   \n",
       "19  [quotestags_app, funnyquotes, insult, sarcasti...   \n",
       "20               [Loving, the, coverage, of, theopen]   \n",
       "21  [Did, Strop, just, pitch, around, a, double, w...   \n",
       "22  [You, keep, them, close, to, see, what, they, ...   \n",
       "23  [Thanks, for, giving, my, personal, informatio...   \n",
       "24  [sure, but, we, got, to, watch, a, random, soc...   \n",
       "\n",
       "                                                flags  \n",
       "0                [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "1                            [0, 0, 0, 0, 0, 1, 0, 0]  \n",
       "2                      [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  \n",
       "3                      [0, 0, 0, 0, 0, 1, 0, 1, 0, 0]  \n",
       "4                            [0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "5   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "6   [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, ...  \n",
       "7    [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "8   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "9   [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "10                  [1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0]  \n",
       "11                           [0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "12  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...  \n",
       "13  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...  \n",
       "14  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "15  [0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...  \n",
       "16                              [0, 0, 0, 0, 0, 0, 1]  \n",
       "17                                          [1, 0, 0]  \n",
       "18  [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "19                                 [0, 0, 1, 0, 0, 0]  \n",
       "20                                    [0, 0, 0, 0, 0]  \n",
       "21  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "22         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "23  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...  \n",
       "24            [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @title flag with lexicon\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "df_tw[\"tokens_flags\"] = df_tw[\"tweets\"].progress_apply(\n",
    "    lambda x: tag_subjectivity(x, mpqa_dict)\n",
    ")\n",
    "df_tw[\"tokens\"] = df_tw[\"tokens_flags\"].apply(lambda x: x[0])\n",
    "df_tw[\"flags\"]  = df_tw[\"tokens_flags\"].apply(lambda x: x[1])\n",
    "df_tw = df_tw.drop(columns=[\"tokens_flags\"])\n",
    "\n",
    "df_tw[[\"tweets\", \"tokens\", \"flags\"]].head(25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 518,
     "status": "ok",
     "timestamp": 1763341826599,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "CBO_JEtHV2jl",
    "outputId": "2edb0a34-c31c-419e-ebf3-d78d4c0f3fb7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"display(df_tw\",\n  \"rows\": 25,\n  \"fields\": [\n    {\n      \"column\": \"tweets\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 25,\n        \"samples\": [\n          \"for the car ride when I get to listen to you and jess the whole time? Yeah woo can't wait either.\",\n          \"Here we go another day in paradise\",\n          \"Be aware dirty step to get money staylight staywhite moralneeded @\\u2026\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"class\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"figurative\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tokens\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"flags\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"input_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 25,\n        \"samples\": [\n          \"neutralize: for[0] the[0] car[0] ride[0] when[0] I[0] get[0] to[0] listen[0] to[0] you[0] and[0] jess[0] the[0] whole[0] time[0] ?[0] Yeah[1] woo[1] ca[0] n't[0] wait[0] either[0] .[0]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 25,\n        \"samples\": [\n          \"for the car ride when I get to listen to you and jess the whole time? Yeah woo can't wait either.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-60f9cee9-9165-45c4-a9d2-313452c0a299\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>class</th>\n",
       "      <th>tokens</th>\n",
       "      <th>flags</th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Be aware dirty step to get money staylight sta...</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[Be, aware, dirty, step, to, get, money, stayl...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>neutralize: Be[0] aware[1] dirty[0] step[0] to...</td>\n",
       "      <td>Be aware dirty step to get money staylight sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>for people who don't understand diy artattack</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[for, people, who, do, n't, understand, diy, a...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "      <td>neutralize: for[0] people[0] who[0] do[0] n't[...</td>\n",
       "      <td>for people who don't understand diy artattack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dailymail readers being sensible as always sho...</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[dailymail, readers, being, sensible, as, alwa...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>neutralize: dailymail[0] readers[0] being[0] s...</td>\n",
       "      <td>dailymail readers being sensible as always sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why do I get the feeling you like games?</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[Why, do, I, get, the, feeling, you, like, gam...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 1, 0, 0]</td>\n",
       "      <td>neutralize: Why[0] do[0] I[0] get[0] the[0] fe...</td>\n",
       "      <td>Why do I get the feeling you like games?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>- You probably just missed the text.</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[-, You, probably, just, missed, the, text, .]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>neutralize: -[0] You[0] probably[0] just[0] mi...</td>\n",
       "      <td>- You probably just missed the text.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tune in to Nigezie and be treated to Rachel Pl...</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[Tune, in, to, Nigezie, and, be, treated, to, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>neutralize: Tune[0] in[0] to[0] Nigezie[0] and...</td>\n",
       "      <td>Tune in to Nigezie and be treated to Rachel Pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What iz thiz?!?!? A friggin DC love fest??!?!?...</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[What, iz, thiz, ?, !, ?, !, ?, A, friggin, DC...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>neutralize: What[0] iz[0] thiz[0] ?[0] ![0] ?[...</td>\n",
       "      <td>What iz thiz?!?!? A friggin DC love fest??!?!?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>man, i wish i could sexually harass an intoxic...</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[man, ,, i, wish, i, could, sexually, harass, ...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>neutralize: man[0] ,[0] i[0] wish[1] i[0] coul...</td>\n",
       "      <td>man, i wish i could sexually harass an intoxic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>for the car ride when I get to listen to you a...</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[for, the, car, ride, when, I, get, to, listen...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>neutralize: for[0] the[0] car[0] ride[0] when[...</td>\n",
       "      <td>for the car ride when I get to listen to you a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Aamir calls bajrangibhaijaan as Salman's best ...</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[Aamir, calls, bajrangibhaijaan, as, Salman, '...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>neutralize: Aamir[0] calls[0] bajrangibhaijaan...</td>\n",
       "      <td>Aamir calls bajrangibhaijaan as Salman's best ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Sorry you can't understand my dark sense of hu...</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[Sorry, you, ca, n't, understand, my, dark, se...</td>\n",
       "      <td>[1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0]</td>\n",
       "      <td>neutralize: Sorry[1] you[0] ca[0] n't[0] under...</td>\n",
       "      <td>Sorry you can't understand my dark sense of hu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>work yay fuck goodnight  @ G Town</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[work, yay, fuck, goodnight, , @, G, Town]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>neutralize: work[0] yay[0] fuck[0] goodnight[0...</td>\n",
       "      <td>work yay fuck goodnight  @ G Town</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Come on..it's against the Mets....that's like ...</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[Come, on, .., it, 's, against, the, Mets, ......</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>neutralize: Come[0] on[0] ..[0] it[0] 's[0] ag...</td>\n",
       "      <td>Come on..it's against the Mets....that's like ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>I thought hot spot policing was a plan?? (Whic...</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[I, thought, hot, spot, policing, was, a, plan...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>neutralize: I[0] thought[0] hot[0] spot[0] pol...</td>\n",
       "      <td>I thought hot spot policing was a plan?? (Whic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>dont be daft Dr Ciara. Measles doesnt kill, it...</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[dont, be, daft, Dr, Ciara, ., Measles, doesnt...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>neutralize: dont[0] be[0] daft[0] Dr[0] Ciara[...</td>\n",
       "      <td>dont be daft Dr Ciara. Measles doesnt kill, it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>I dont think any TV show could be more epic th...</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[I, dont, think, any, TV, show, could, be, mor...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>neutralize: I[0] dont[0] think[1] any[0] TV[0]...</td>\n",
       "      <td>I dont think any TV show could be more epic th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Here we go another day in paradise</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[Here, we, go, another, day, in, paradise]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1]</td>\n",
       "      <td>neutralize: Here[0] we[0] go[0] another[0] day...</td>\n",
       "      <td>Here we go another day in paradise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>sure does...</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[sure, does, ...]</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "      <td>neutralize: sure[1] does[0] ...[0]</td>\n",
       "      <td>sure does...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>My jobs great because I love what I do (operat...</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[My, jobs, great, because, I, love, what, I, d...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>neutralize: My[0] jobs[0] great[1] because[0] ...</td>\n",
       "      <td>My jobs great because I love what I do (operat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>quotestags_app funnyquotes insult sarcasticquo...</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[quotestags_app, funnyquotes, insult, sarcasti...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0]</td>\n",
       "      <td>neutralize: quotestags_app[0] funnyquotes[0] i...</td>\n",
       "      <td>quotestags_app funnyquotes insult sarcasticquo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Loving the coverage of theopen</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[Loving, the, coverage, of, theopen]</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "      <td>neutralize: Loving[0] the[0] coverage[0] of[0]...</td>\n",
       "      <td>Loving the coverage of theopen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Did Strop just pitch around a double with the ...</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[Did, Strop, just, pitch, around, a, double, w...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>neutralize: Did[0] Strop[0] just[0] pitch[0] a...</td>\n",
       "      <td>Did Strop just pitch around a double with the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>You keep them close to see what they're up to....</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[You, keep, them, close, to, see, what, they, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>neutralize: You[0] keep[0] them[0] close[0] to...</td>\n",
       "      <td>You keep them close to see what they're up to....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Thanks for giving my personal information and ...</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[Thanks, for, giving, my, personal, informatio...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>neutralize: Thanks[0] for[0] giving[0] my[0] p...</td>\n",
       "      <td>Thanks for giving my personal information and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>sure but we got to watch a random soccer match...</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[sure, but, we, got, to, watch, a, random, soc...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>neutralize: sure[1] but[0] we[0] got[0] to[0] ...</td>\n",
       "      <td>sure but we got to watch a random soccer match...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-60f9cee9-9165-45c4-a9d2-313452c0a299')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-60f9cee9-9165-45c4-a9d2-313452c0a299 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-60f9cee9-9165-45c4-a9d2-313452c0a299');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-dc80f5ee-d3f0-468b-8078-f222445dc8e3\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dc80f5ee-d3f0-468b-8078-f222445dc8e3')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-dc80f5ee-d3f0-468b-8078-f222445dc8e3 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                               tweets       class  \\\n",
       "0   Be aware dirty step to get money staylight sta...  figurative   \n",
       "1       for people who don't understand diy artattack  figurative   \n",
       "2   dailymail readers being sensible as always sho...  figurative   \n",
       "3            Why do I get the feeling you like games?  figurative   \n",
       "4                - You probably just missed the text.  figurative   \n",
       "5   Tune in to Nigezie and be treated to Rachel Pl...  figurative   \n",
       "6   What iz thiz?!?!? A friggin DC love fest??!?!?...  figurative   \n",
       "7   man, i wish i could sexually harass an intoxic...  figurative   \n",
       "8   for the car ride when I get to listen to you a...  figurative   \n",
       "9   Aamir calls bajrangibhaijaan as Salman's best ...  figurative   \n",
       "10  Sorry you can't understand my dark sense of hu...  figurative   \n",
       "11                 work yay fuck goodnight  @ G Town  figurative   \n",
       "12  Come on..it's against the Mets....that's like ...  figurative   \n",
       "13  I thought hot spot policing was a plan?? (Whic...  figurative   \n",
       "14  dont be daft Dr Ciara. Measles doesnt kill, it...  figurative   \n",
       "15  I dont think any TV show could be more epic th...  figurative   \n",
       "16                 Here we go another day in paradise  figurative   \n",
       "17                                       sure does...  figurative   \n",
       "18  My jobs great because I love what I do (operat...  figurative   \n",
       "19  quotestags_app funnyquotes insult sarcasticquo...  figurative   \n",
       "20                     Loving the coverage of theopen  figurative   \n",
       "21  Did Strop just pitch around a double with the ...  figurative   \n",
       "22  You keep them close to see what they're up to....  figurative   \n",
       "23  Thanks for giving my personal information and ...  figurative   \n",
       "24  sure but we got to watch a random soccer match...  figurative   \n",
       "\n",
       "                                               tokens  \\\n",
       "0   [Be, aware, dirty, step, to, get, money, stayl...   \n",
       "1   [for, people, who, do, n't, understand, diy, a...   \n",
       "2   [dailymail, readers, being, sensible, as, alwa...   \n",
       "3   [Why, do, I, get, the, feeling, you, like, gam...   \n",
       "4      [-, You, probably, just, missed, the, text, .]   \n",
       "5   [Tune, in, to, Nigezie, and, be, treated, to, ...   \n",
       "6   [What, iz, thiz, ?, !, ?, !, ?, A, friggin, DC...   \n",
       "7   [man, ,, i, wish, i, could, sexually, harass, ...   \n",
       "8   [for, the, car, ride, when, I, get, to, listen...   \n",
       "9   [Aamir, calls, bajrangibhaijaan, as, Salman, '...   \n",
       "10  [Sorry, you, ca, n't, understand, my, dark, se...   \n",
       "11        [work, yay, fuck, goodnight, , @, G, Town]   \n",
       "12  [Come, on, .., it, 's, against, the, Mets, ......   \n",
       "13  [I, thought, hot, spot, policing, was, a, plan...   \n",
       "14  [dont, be, daft, Dr, Ciara, ., Measles, doesnt...   \n",
       "15  [I, dont, think, any, TV, show, could, be, mor...   \n",
       "16         [Here, we, go, another, day, in, paradise]   \n",
       "17                                  [sure, does, ...]   \n",
       "18  [My, jobs, great, because, I, love, what, I, d...   \n",
       "19  [quotestags_app, funnyquotes, insult, sarcasti...   \n",
       "20               [Loving, the, coverage, of, theopen]   \n",
       "21  [Did, Strop, just, pitch, around, a, double, w...   \n",
       "22  [You, keep, them, close, to, see, what, they, ...   \n",
       "23  [Thanks, for, giving, my, personal, informatio...   \n",
       "24  [sure, but, we, got, to, watch, a, random, soc...   \n",
       "\n",
       "                                                flags  \\\n",
       "0                [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "1                            [0, 0, 0, 0, 0, 1, 0, 0]   \n",
       "2                      [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]   \n",
       "3                      [0, 0, 0, 0, 0, 1, 0, 1, 0, 0]   \n",
       "4                            [0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "5   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "6   [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, ...   \n",
       "7    [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "8   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "9   [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "10                  [1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0]   \n",
       "11                           [0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "12  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "13  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "14  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "15  [0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...   \n",
       "16                              [0, 0, 0, 0, 0, 0, 1]   \n",
       "17                                          [1, 0, 0]   \n",
       "18  [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "19                                 [0, 0, 1, 0, 0, 0]   \n",
       "20                                    [0, 0, 0, 0, 0]   \n",
       "21  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "22         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "23  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "24            [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "                                           input_text  \\\n",
       "0   neutralize: Be[0] aware[1] dirty[0] step[0] to...   \n",
       "1   neutralize: for[0] people[0] who[0] do[0] n't[...   \n",
       "2   neutralize: dailymail[0] readers[0] being[0] s...   \n",
       "3   neutralize: Why[0] do[0] I[0] get[0] the[0] fe...   \n",
       "4   neutralize: -[0] You[0] probably[0] just[0] mi...   \n",
       "5   neutralize: Tune[0] in[0] to[0] Nigezie[0] and...   \n",
       "6   neutralize: What[0] iz[0] thiz[0] ?[0] ![0] ?[...   \n",
       "7   neutralize: man[0] ,[0] i[0] wish[1] i[0] coul...   \n",
       "8   neutralize: for[0] the[0] car[0] ride[0] when[...   \n",
       "9   neutralize: Aamir[0] calls[0] bajrangibhaijaan...   \n",
       "10  neutralize: Sorry[1] you[0] ca[0] n't[0] under...   \n",
       "11  neutralize: work[0] yay[0] fuck[0] goodnight[0...   \n",
       "12  neutralize: Come[0] on[0] ..[0] it[0] 's[0] ag...   \n",
       "13  neutralize: I[0] thought[0] hot[0] spot[0] pol...   \n",
       "14  neutralize: dont[0] be[0] daft[0] Dr[0] Ciara[...   \n",
       "15  neutralize: I[0] dont[0] think[1] any[0] TV[0]...   \n",
       "16  neutralize: Here[0] we[0] go[0] another[0] day...   \n",
       "17                 neutralize: sure[1] does[0] ...[0]   \n",
       "18  neutralize: My[0] jobs[0] great[1] because[0] ...   \n",
       "19  neutralize: quotestags_app[0] funnyquotes[0] i...   \n",
       "20  neutralize: Loving[0] the[0] coverage[0] of[0]...   \n",
       "21  neutralize: Did[0] Strop[0] just[0] pitch[0] a...   \n",
       "22  neutralize: You[0] keep[0] them[0] close[0] to...   \n",
       "23  neutralize: Thanks[0] for[0] giving[0] my[0] p...   \n",
       "24  neutralize: sure[1] but[0] we[0] got[0] to[0] ...   \n",
       "\n",
       "                                          target_text  \n",
       "0   Be aware dirty step to get money staylight sta...  \n",
       "1       for people who don't understand diy artattack  \n",
       "2   dailymail readers being sensible as always sho...  \n",
       "3            Why do I get the feeling you like games?  \n",
       "4                - You probably just missed the text.  \n",
       "5   Tune in to Nigezie and be treated to Rachel Pl...  \n",
       "6   What iz thiz?!?!? A friggin DC love fest??!?!?...  \n",
       "7   man, i wish i could sexually harass an intoxic...  \n",
       "8   for the car ride when I get to listen to you a...  \n",
       "9   Aamir calls bajrangibhaijaan as Salman's best ...  \n",
       "10  Sorry you can't understand my dark sense of hu...  \n",
       "11                 work yay fuck goodnight  @ G Town  \n",
       "12  Come on..it's against the Mets....that's like ...  \n",
       "13  I thought hot spot policing was a plan?? (Whic...  \n",
       "14  dont be daft Dr Ciara. Measles doesnt kill, it...  \n",
       "15  I dont think any TV show could be more epic th...  \n",
       "16                 Here we go another day in paradise  \n",
       "17                                       sure does...  \n",
       "18  My jobs great because I love what I do (operat...  \n",
       "19  quotestags_app funnyquotes insult sarcasticquo...  \n",
       "20                     Loving the coverage of theopen  \n",
       "21  Did Strop just pitch around a double with the ...  \n",
       "22  You keep them close to see what they're up to....  \n",
       "23  Thanks for giving my personal information and ...  \n",
       "24  sure but we got to watch a random soccer match...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def format_input(tokens, flags):\n",
    "    combined = \" \".join(f\"{w}[{f}]\" for w, f in zip(tokens, flags))\n",
    "    return \"neutralize: \" + combined\n",
    "\n",
    "df_tw[\"input_text\"] = [\n",
    "    format_input(t, f) for t, f in zip(df_tw[\"tokens\"], df_tw[\"flags\"])\n",
    "]\n",
    "df_tw[\"target_text\"] = df_tw[\"tweets\"]\n",
    "\n",
    "display(df_tw.head(25))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2yNR3Xj6WxqY"
   },
   "outputs": [],
   "source": [
    "# @title save cleaned flagged dataset\n",
    "all_out = os.path.join(folder_path, \"data/tweets/processed/mpqa_lexicon/flagged_train.csv\")\n",
    "os.makedirs(os.path.dirname(all_out), exist_ok=True)\n",
    "df_tw.to_csv(all_out, index=False)\n",
    "\n",
    "all_out = os.path.join(folder_path, \"data/tweets/processed/mpqa_lexicon/input_target_pair_tweets_train.csv\")\n",
    "os.makedirs(os.path.dirname(all_out), exist_ok=True)\n",
    "df_tw[[\"input_text\", \"target_text\"]].to_csv(all_out, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96RPMRRTXhhP"
   },
   "source": [
    "## Finetune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9P_Xxmprhtsj"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345,
     "referenced_widgets": [
      "330565840a77488cb67341b02905d37a",
      "334b55d7160342298363e9a28f2fcb84",
      "9d07a07ca29c405c95332524cf1571ac",
      "d8c73ca29dca4e3da889198fe0ba7831",
      "2c2a9f2220f1411b8bf2b03e6f851ac2",
      "ac9247ed2a5643229573a85a4343c119",
      "06515260f42e40d78b7fe031066b7314",
      "9389e3244a3842f9ba0319bf58013456",
      "0b7922fe580849059b8ae91f287981c7",
      "400bc57d8b8b45e582b19dd974cffb37",
      "d5769859b0034ce584e147fd3d905855",
      "56cc9319549f4e4d8d771ce54df19483",
      "8f33e346105f4250b15b3bf0700e37e5",
      "816123a1d6394df0b25c1e15e223142b",
      "88365c36fd76430aa19bf68b2a4f7fdd",
      "6217095e97314cffab56e22676c0bf96",
      "f09b46772aca4927a71c74439a6bbdf0",
      "2aae7c78807d46ae97d90f56bd9acd4a",
      "d0b1ccbf04c244199b5fb2d68d7baf44",
      "9637af73dfed47b5a2d822a531785dc7",
      "f65df6ae10ab46c8895b6e033cc0a3e9",
      "5853b4dcee0f4b06a5246c8e8c77d61b",
      "8189b59d902a4b5a8fa04bc164f5de1c",
      "2f14ce19688f4a608ad348477a092947",
      "1c84ae399704403b8308b241687f977b",
      "532c763831fb4aad9de2e6b310ceeacc",
      "b7593ff51a4e4c199afb6f509833c591",
      "e6033e5ffd4f43dd8906015aabf5657c",
      "39c203075e1b4b06b389c98b7c88aff2",
      "2c44cb86a4dd4eb19930e6470fe89444",
      "20b7d1d556614dcd9e20055f34a8c778",
      "fb1a8c27574d4fa0ba775aaab5688d98",
      "3640ae7ec2b34e0f8087ad42a9ecf1ee",
      "64bbfd1afe8442efad81c2d55c04bf4f",
      "d4b78cc5fe9c4353bde17035673b5566",
      "f2da3245ca7348ad9bba5b6cdc2a8a74",
      "d1bedec004b84a7e86ba3119365798c3",
      "39c2066f8e6146969c30a22deb18b84d",
      "9ba6b10842e94e9d8ab5876d941883b3",
      "1c2e7d47491f4bb99a39a85a0dbdd7f5",
      "11ea0f8322594bdaa4ef72c03e6ded87",
      "e927bb1c3d13419095849a8d66263edc",
      "7a729bba8cec41a8939c01e74e3417cb",
      "97d0135de8b34f138567daf1cc6811c4",
      "2ccf92e203c94c9f887936eead6efac7",
      "507437e7f77b45b08c1a5bc00e90c79e",
      "9ee76951e8d44cae8ef9b57b8c91a7df",
      "3adcd88a32914d369b7c80b484836fe5",
      "959c8398f4c54e6b8b60b83eb8cca2a4",
      "308ed0c4bd894ed28705d6ba49249263",
      "cb26e43816114201b9b1a744042cf613",
      "6fc3cdf4bfa4466eb871d728ae6a1a9f",
      "cb458d6e35b34db8950afa2868aca009",
      "14c2b2f465f647b4b5cd912462a67162",
      "ec7a896e91c64eb19fb025c51bd52aee",
      "5cf313235acc40a58c2010bc07300b58",
      "809aeb4e2ac743e79cbf55d90da5837f",
      "67275b0cb4b54ad5bb77dc05bb44c2f8",
      "878796fe91084abeb9059b25b6d2d002",
      "80e20344a8a5404a8cf73081ecf08fb9",
      "04aeed22e5db4250a6f5f9065a2c5987",
      "5e694842e9b3465c80938d0b6064ac7e",
      "0a079e8f9bdc486eb34001fb90c5c964",
      "2ddf0e585b0a49269f7e1c7f2ccd152c",
      "52ba9b011df94da08c4b60f37c228b4c",
      "cb00b67cd3394338940a4630df671f90",
      "fabef84864ed4788b5994aaf4d1542b6",
      "bb34d3b0de404b198c8adfe832b0751c",
      "ff644624b03e44c3ad78d416d6ced567",
      "bcc03a3abf53493ca2be69fd313463b4",
      "8229cf259a47476e8a98e558b80e5d9e",
      "2f7989bb88e14e239521a89cdab46ff3",
      "822f6cb310144f8cae38cbe0dd126142",
      "cc01dbfc108b470ea4808e2ccb768917",
      "e8874c17b6d04f8aa14ea19d2c78d155",
      "8f5cc81e85354d08855771a58001210b",
      "0793cea924de49c99aa39bda433510f4"
     ]
    },
    "executionInfo": {
     "elapsed": 51681,
     "status": "ok",
     "timestamp": 1763409567247,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "XuGQLCYlfWxC",
    "outputId": "13d20476-a248-453b-b916-ed0e21aa1054"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "330565840a77488cb67341b02905d37a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/412 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56cc9319549f4e4d8d771ce54df19483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8189b59d902a4b5a8fa04bc164f5de1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/16.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64bbfd1afe8442efad81c2d55c04bf4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/23.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ccf92e203c94c9f887936eead6efac7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/173 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cf313235acc40a58c2010bc07300b58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/881 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fabef84864ed4788b5994aaf4d1542b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @title load wnc fine tuned model, tokenizer, and clf\n",
    "\n",
    "# clf:\n",
    "classifier_name = \"GroNLP/mdebertav3-subjectivity-english\"\n",
    "clf_tokenizer = AutoTokenizer.from_pretrained(classifier_name)\n",
    "clf_model = AutoModelForSequenceClassification.from_pretrained(classifier_name)\n",
    "clf_model = clf_model.to(device)\n",
    "clf_model.eval()\n",
    "for p in clf_model.parameters():\n",
    "  p.requires_grad = False\n",
    "\n",
    "# wnc fine tuned model:\n",
    "checkpoint_path = os.path.join(folder_path,\n",
    "    \"model_checkpoints/wnc-mpqa-flan_t5-2loss-11_09/checkpoint-6000\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_path)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "998006532d4d49b29a940c287e4a98d0",
      "2f7ec7e46a924843b84f2c7615fd4139",
      "ff3c8da9641f49678a34289a9923f4a3",
      "6a9085093bb6426d9633252e3f7eaa36",
      "c258279eab47410ba683a41f9ca16484",
      "c35651cd316046f8bd8665f90cbfee76",
      "78a39a9d1ea24eba88fd0909273100ca",
      "b61321499721409bab975e2c13ab1aa5",
      "cf6568e952234f6cb19318cce74ec5f8",
      "c9cb9441676140aba7849967d2e3f74b",
      "f772240ae2404a66882307e96f9cc286",
      "4ce6e1d7e8df450fbd586387fc19da8d",
      "58daa4586cdb4208b2f1fde49902f160",
      "37024a405a074bc8acf8fa5f5282a7f7",
      "c4296ab1bfe04b11a6199c387751859f",
      "393bfefc676c48ad9bc76cb892eb20c3",
      "e6613567a67d487a8185cb8339fa7718",
      "aa460ec2afc34bce87fc59e579df46d9",
      "52994d18207b47f68e4d0d27c81551f8",
      "4cf26fcc9459491b9416ad4ca222556c",
      "34ac89955fc1450c8f321844cb1289ed",
      "959f27fbb9b242c284ced60d5d8900b1"
     ]
    },
    "executionInfo": {
     "elapsed": 57155,
     "status": "ok",
     "timestamp": 1763414501809,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "QoC6W-H5ZCLT",
    "outputId": "373bc902-cf97-4fd2-f2ef-29957638c56e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "998006532d4d49b29a940c287e4a98d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/81382 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ce6e1d7e8df450fbd586387fc19da8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/81382 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @title prepare dataset\n",
    "\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import ast\n",
    "import os\n",
    "\n",
    "# Load tweet csv\n",
    "tw_path = os.path.join(\n",
    "    folder_path,\n",
    "    \"data/tweets/processed/mpqa_lexicon/flagged_train.csv\"\n",
    ")\n",
    "\n",
    "df_tw = pd.read_csv(tw_path)\n",
    "\n",
    "display(df_tw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1763412618468,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "kintXzr-lP1l",
    "outputId": "4d2fd4e0-8cea-428b-a9c8-f5e7a99e38e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    ['Be', 'aware', 'dirty', 'step', 'to', 'get', ...\n",
      "1    ['for', 'people', 'who', 'do', \"n't\", 'underst...\n",
      "2    ['dailymail', 'readers', 'being', 'sensible', ...\n",
      "3    ['Why', 'do', 'I', 'get', 'the', 'feeling', 'y...\n",
      "4    ['-', 'You', 'probably', 'just', 'missed', 'th...\n",
      "Name: tokens, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_tw[\"tokens\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1763414501835,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "4eXK7MeBa9tC",
    "outputId": "2a266834-a3dc-4053-baee-b06c1f40c9b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels', 'raw_input'],\n",
      "    num_rows: 81382\n",
      "})\n",
      "{'input_ids': [493, 2718, 13086, 1147, 12, 129, 540, 1049, 2242, 1049, 13698, 4854, 25797, 3320, 233, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [493, 2718, 13086, 1147, 12, 129, 540, 1049, 2242, 1049, 13698, 4854, 25797, 3320, 233, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'raw_input': ['Be aware dirty step to get money staylight staywhite moralneeded @']}\n"
     ]
    }
   ],
   "source": [
    "keep = [\"input_ids\", \"attention_mask\", \"labels\" , \"raw_input\"] #\"flags\"\n",
    "\n",
    "tokenized_tw = tokenized_tw.remove_columns(\n",
    "    [col for col in tokenized_tw.column_names if col not in keep]\n",
    ")\n",
    "print(tokenized_tw)\n",
    "print(tokenized_tw[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1763413662455,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "PnvaPfJbdR6V",
    "outputId": "528ce455-7c82-4f08-de09-3c25c9d88547"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: [('abandoned', {'type': 'weaksubj', 'polarity': 'negative'}), ('abandonment', {'type': 'weaksubj', 'polarity': 'negative'}), ('abandon', {'type': 'weaksubj', 'polarity': 'negative'}), ('abase', {'type': 'strongsubj', 'polarity': 'negative'}), ('abasement', {'type': 'strongsubj', 'polarity': 'negative'}), ('abash', {'type': 'strongsubj', 'polarity': 'negative'}), ('abate', {'type': 'weaksubj', 'polarity': 'negative'}), ('abdicate', {'type': 'weaksubj', 'polarity': 'negative'}), ('aberration', {'type': 'strongsubj', 'polarity': 'negative'}), ('abhor', {'type': 'strongsubj', 'polarity': 'negative'})]\n",
      "Types:  ['strongsubj', 'weaksubj']\n",
      "polarities:  ['both', 'negative', 'neutral', 'positive', 'weakneg']\n"
     ]
    }
   ],
   "source": [
    "# @title mpqa dict\n",
    "lexicon_path=os.path.join(folder_path, \"data/subjectivity_clues_hltemnlp05/subjclueslen1-HLTEMNLP05.tff\")\n",
    "def load_mpqa_lexicon(path):\n",
    "    subj_dict = {}\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            entry = {}\n",
    "            for p in parts:\n",
    "                if '=' in p:\n",
    "                    key, value = p.split('=', 1)\n",
    "                    entry[key] = value\n",
    "            word = entry.get(\"word1\", \"\").lower()\n",
    "            subj_type = entry.get(\"type\", \"\")\n",
    "            polarity = entry.get(\"priorpolarity\", \"\")\n",
    "            if word:\n",
    "                subj_dict[word] = {\"type\": subj_type, \"polarity\": polarity}\n",
    "    return subj_dict\n",
    "mpqa_dict = load_mpqa_lexicon(lexicon_path)\n",
    "print(\"Sample:\", list(mpqa_dict.items())[:10])\n",
    "types = sorted({v[\"type\"] for v in mpqa_dict.values()})\n",
    "polarities = sorted({v[\"polarity\"] for v in mpqa_dict.values()})\n",
    "print(\"Types: \", types)\n",
    "print(\"polarities: \", polarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "61uVNCDNhUwh"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "class DualLossWithRF(Seq2SeqTrainer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        classifier,\n",
    "        clf_tokenizer,\n",
    "        mpqa_lexicon,\n",
    "        ref_model=None,\n",
    "        lambda_ce=1.0,\n",
    "        lambda_rl=1.0,\n",
    "        lambda_kl=0.02,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.classifier = classifier.eval()\n",
    "        self.clf_tokenizer = clf_tokenizer\n",
    "        self.mpqa_lexicon = mpqa_lexicon\n",
    "        self.ref_model = ref_model.eval() if ref_model is not None else None\n",
    "\n",
    "        for p in self.classifier.parameters():\n",
    "            p.requires_grad = False\n",
    "        if self.ref_model is not None:\n",
    "            for p in self.ref_model.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        self.ce_loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "        self.lambda_ce = lambda_ce\n",
    "        self.lambda_rl = lambda_rl\n",
    "        self.lambda_kl = lambda_kl\n",
    "\n",
    "    # -----------------------------\n",
    "    # token cleaning for MPQA\n",
    "    # -----------------------------\n",
    "    def clean_word(self, tok):\n",
    "        w = tok.lower()\n",
    "        w = re.sub(r\"[^a-z]+\", \"\", w)\n",
    "        return w\n",
    "\n",
    "    # -----------------------------\n",
    "    # MPQA subjectivity on output\n",
    "    # -----------------------------\n",
    "    def tag_subjectivity(self, sentence):\n",
    "        tokens = sentence.split()\n",
    "        flags = []\n",
    "        for tok in tokens:\n",
    "            w = self.clean_word(tok)\n",
    "            if w in self.mpqa_lexicon:\n",
    "                subj_type = self.mpqa_lexicon[w][\"type\"]\n",
    "                if subj_type in [\"strongsubj\", \"weaksubj\"]:\n",
    "                    flags.append(1)\n",
    "                else:\n",
    "                    flags.append(0)\n",
    "            else:\n",
    "                flags.append(0)\n",
    "        return flags\n",
    "\n",
    "    # -----------------------------\n",
    "    # simple token overlap similarity\n",
    "    # -----------------------------\n",
    "    def similarity(self, src, tgt):\n",
    "        src_toks = src.lower().split()\n",
    "        tgt_toks = tgt.lower().split()\n",
    "        if len(src_toks) == 0 or len(tgt_toks) == 0:\n",
    "            return 0.0\n",
    "        src_set = set(src_toks)\n",
    "        tgt_set = set(tgt_toks)\n",
    "        inter = len(src_set & tgt_set)\n",
    "        denom = len(src_set) + len(tgt_set)\n",
    "        return 2.0 * inter / denom\n",
    "\n",
    "    # -----------------------------\n",
    "    # main loss\n",
    "    # -----------------------------\n",
    "    # def get_train_dataloader(self):\n",
    "    #   # force keep all columns, including raw_input\n",
    "    #   self._remove_unused_columns = False\n",
    "    #   return super().get_train_dataloader()\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        # separate raw_input so we do not feed it to the model\n",
    "        # print(inputs.keys())\n",
    "        raw_inputs = inputs[\"raw_input\"][0]\n",
    "        model_inputs = {k: v for k, v in inputs.items() if k != \"raw_input\"}\n",
    "\n",
    "        outputs = model(**model_inputs)\n",
    "        logits = outputs.logits\n",
    "        labels = model_inputs[\"labels\"]\n",
    "\n",
    "        # 1. CE loss\n",
    "        ce_loss = self.ce_loss_fn(\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            labels.view(-1)\n",
    "        )\n",
    "\n",
    "        # 2. KL to reference model to prevent nonsense\n",
    "        kl_loss = torch.tensor(0.0, device=logits.device)\n",
    "        if self.ref_model is not None and self.lambda_kl > 0:\n",
    "            with torch.no_grad():\n",
    "                ref_outputs = self.ref_model(**model_inputs)\n",
    "                ref_logits = ref_outputs.logits\n",
    "\n",
    "            log_p = F.log_softmax(logits, dim=-1)\n",
    "            log_q = F.log_softmax(ref_logits, dim=-1)\n",
    "            p = log_p.exp()\n",
    "            kl_loss = F.kl_div(log_p, log_q.exp(), reduction=\"batchmean\")\n",
    "\n",
    "        # 3. sampling\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        if self.state.global_step < 200:\n",
    "            sampled_ids = logits.argmax(dim=-1)\n",
    "        else:\n",
    "            dist = torch.distributions.Categorical(probs)\n",
    "            sampled_ids = dist.sample()\n",
    "\n",
    "        sampled_texts = self.processing_class.batch_decode(\n",
    "            sampled_ids, skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        print(\"\\n==== DEBUG SAMPLE ====\")\n",
    "        for i in range(min(2, len(raw_inputs))):\n",
    "            print(f\"[{i}] SRC: {raw_inputs[i]}\")\n",
    "            print(f\"[{i}] OUT: {sampled_texts[i]}\")\n",
    "        print(\"=====================\\n\")\n",
    "\n",
    "        # 4. classifier reward\n",
    "        clf_rewards = []\n",
    "        for txt in sampled_texts:\n",
    "            toks = self.clf_tokenizer(\n",
    "                txt,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=128\n",
    "            ).to(logits.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pred = self.classifier(**toks).logits.softmax(dim=-1)\n",
    "                clf_rewards.append(pred[0][0])\n",
    "\n",
    "        clf_rewards = torch.stack(clf_rewards)  # [batch]\n",
    "\n",
    "        # 5. subjectivity reward from MPQA on output\n",
    "        subj_ratios = []\n",
    "        for txt in sampled_texts:\n",
    "            flags = self.tag_subjectivity(txt)\n",
    "            if len(flags) == 0:\n",
    "                subj_ratios.append(0.0)\n",
    "            else:\n",
    "                subj_ratios.append(float(sum(flags)) / float(len(flags)))\n",
    "\n",
    "        subj_ratio = torch.tensor(subj_ratios, device=logits.device).float()\n",
    "        reward_subj = 1.0 - subj_ratio  # higher if less subjective\n",
    "\n",
    "        # 6. similarity based shaping and empty penalty\n",
    "        sim_list = []\n",
    "        empty_penalties = []\n",
    "        copy_penalties = []\n",
    "        halluc_penalties = []\n",
    "\n",
    "        for src, out in zip(raw_inputs, sampled_texts):\n",
    "            s = self.similarity(src, out)\n",
    "            sim_list.append(s)\n",
    "\n",
    "            if len(out.strip()) == 0:\n",
    "                empty_penalties.append(1.0)\n",
    "            else:\n",
    "                empty_penalties.append(0.0)\n",
    "\n",
    "            # penalize almost exact copies\n",
    "            copy_penalties.append(max(0.0, s - 0.95))\n",
    "            # penalize extremely off topic rewrites\n",
    "            halluc_penalties.append(max(0.0, 0.3 - s))\n",
    "\n",
    "        sim = torch.tensor(sim_list, device=logits.device).float()\n",
    "        empty_penalty = torch.tensor(empty_penalties, device=logits.device).float()\n",
    "        copy_penalty = torch.tensor(copy_penalties, device=logits.device).float()\n",
    "        halluc_penalty = torch.tensor(halluc_penalties, device=logits.device).float()\n",
    "\n",
    "        # 7. combine into final scalar reward per example\n",
    "        base_reward = 0.5 * reward_subj + 0.5 * clf_rewards\n",
    "\n",
    "        shaped_reward = (\n",
    "            base_reward\n",
    "            + 0.3 * (sim - 0.5)          # reward similarity around 0.5\n",
    "            - 1.0 * empty_penalty        # punish empty\n",
    "            - 0.5 * copy_penalty         # punish too similar\n",
    "            - 0.5 * halluc_penalty       # punish too dissimilar\n",
    "        )\n",
    "\n",
    "        # clip and normalize rewards\n",
    "        reward = shaped_reward.clamp(-1.0, 1.0)\n",
    "        reward_mean = reward.mean()\n",
    "        reward_std = reward.std() + 1e-5\n",
    "        norm_reward = (reward - reward_mean) / reward_std\n",
    "\n",
    "        print(\"---- REWARD DEBUG ----\")\n",
    "        print(\"Classifier reward:\", clf_rewards.tolist())\n",
    "        print(\"Subjectivity reward:\", reward_subj.tolist())\n",
    "        print(\"Similarity:\", sim.tolist())\n",
    "        print(\"Empty penalty:\", empty_penalty.tolist())\n",
    "        print(\"Copy penalty:\", copy_penalty.tolist())\n",
    "        print(\"Halluc penalty:\", halluc_penalty.tolist())\n",
    "        print(\"Base reward:\", base_reward.tolist())\n",
    "        print(\"Shaped reward:\", shaped_reward.tolist())\n",
    "        print(\"Norm reward:\", norm_reward.tolist())\n",
    "        print(\"----------------------\")\n",
    "\n",
    "\n",
    "        # 8. RL loss with normalized advantage\n",
    "        log_probs = torch.log(probs + 1e-12)\n",
    "        log_probs_sampled = log_probs.gather(\n",
    "            dim=-1, index=sampled_ids.unsqueeze(-1)\n",
    "        ).squeeze(-1)\n",
    "\n",
    "        seq_log_prob = log_probs_sampled.mean(dim=1)\n",
    "        rl_loss = - (norm_reward.detach() * seq_log_prob).mean()\n",
    "\n",
    "        # handle lambdas\n",
    "        if self.lambda_ce == 0.0:\n",
    "            ce_loss = torch.tensor(0.0, device=logits.device)\n",
    "        if self.lambda_rl == 0.0:\n",
    "            rl_loss = torch.tensor(0.0, device=logits.device)\n",
    "        if self.lambda_kl == 0.0 or self.ref_model is None:\n",
    "            kl_loss = torch.tensor(0.0, device=logits.device)\n",
    "\n",
    "        total_loss = (\n",
    "            self.lambda_ce * ce_loss\n",
    "            + self.lambda_rl * rl_loss\n",
    "            + self.lambda_kl * kl_loss\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"CE: {ce_loss.item():.4f}, \"\n",
    "            f\"RL: {rl_loss.item():.4f}, \"\n",
    "            f\"KL: {kl_loss.item():.4f}, \"\n",
    "            f\"TOTAL: {total_loss.item():.4f}\"\n",
    "        )\n",
    "\n",
    "        return (total_loss, outputs) if return_outputs else total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1394,
     "status": "ok",
     "timestamp": 1763415395574,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "PccXYyL3b50p",
    "outputId": "a598dec5-ed53-4775-f613-6e38b85a17f2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-587777770.py:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `DualLossWithRF.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "from transformers import default_data_collator\n",
    "\n",
    "def custom_collator(features):\n",
    "    # extract raw strings\n",
    "    raw_inputs = [f[\"raw_input\"] for f in features]\n",
    "\n",
    "    # remove raw_input before tensorization\n",
    "    features_no_raw = []\n",
    "    for f in features:\n",
    "        f = f.copy()\n",
    "        f.pop(\"raw_input\")\n",
    "        features_no_raw.append(f)\n",
    "\n",
    "    # let HF create tensors\n",
    "    batch = default_data_collator(features_no_raw)\n",
    "\n",
    "    # add raw_input back as plain Python list\n",
    "    batch[\"raw_input\"] = raw_inputs\n",
    "\n",
    "    return batch\n",
    "\n",
    "ref_model = AutoModelForSeq2SeqLM.from_pretrained(os.path.join(folder_path,\"model_checkpoints/wnc-mpqa-flan_t5-2loss-11_09/checkpoint-6000\"))\n",
    "ref_model.to(device)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=os.path.join(folder_path, \"model_checkpoints/tweets-mpqa-flan_t5-2lossrf-11_17\"),\n",
    "    save_steps=500,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=True,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = DualLossWithRF(\n",
    "    model=model,                      # current trainable model\n",
    "    tokenizer=tokenizer,\n",
    "    classifier=clf_model,\n",
    "    clf_tokenizer=clf_tokenizer,\n",
    "    mpqa_lexicon=mpqa_dict,\n",
    "    ref_model=ref_model,\n",
    "    lambda_ce=0.6,\n",
    "    lambda_rl=0.4,\n",
    "    lambda_kl=0.0,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_tw,\n",
    "    data_collator=custom_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 50142,
     "status": "error",
     "timestamp": 1763415446612,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "Gg4JYwT-enD3",
    "outputId": "eede2d23-1f90-4e6a-c559-c6782c01d090"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: Pope's plane makes a couple loops over NC on way to see President - would be if he flew over Pope AFB!\n",
      "[0] OUT: Pope's plane makes a couple loops over NC on way to see President - would be if he flew over Pope AFB! PLUS Benedictitori all this Patrick PLUS all\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.4542481601238251, 0.3181358873844147, 0.20519524812698364, 0.181120827794075, 0.18091970682144165, 0.31189846992492676, 0.1649015247821808, 0.2656792104244232]\n",
      "Subjectivity reward: [0.9655172228813171, 0.9375, 0.484375, 0.9399999976158142, 0.7142857313156128, 0.9615384340286255, 0.75, 0.8285714387893677]\n",
      "Similarity: [0.8936170339584351]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.0]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.7098826766014099, 0.6278179287910461, 0.3447851240634918, 0.560560405254364, 0.4476027190685272, 0.6367184519767761, 0.4574507474899292, 0.5471253395080566]\n",
      "Shaped reward: [0.8279677629470825, 0.7459030151367188, 0.4628702402114868, 0.6786454916000366, 0.5656878352165222, 0.7548035383224487, 0.5755358934402466, 0.665210485458374]\n",
      "Norm reward: [1.4098410606384277, 0.722754955291748, -1.6469335556030273, 0.15964213013648987, -0.7860944867134094, 0.7972744703292847, -0.7036417126655579, 0.04715770483016968]\n",
      "----------------------\n",
      "CE: 2.5948, RL: -0.3682, KL: 0.0000, TOTAL: 1.4096\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='109' max='10173' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  109/10173 00:49 < 1:18:02, 2.15 it/s, Epoch 0.01/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: I should have used the font...\n",
      "[0] OUT: I should have used the font...kosten Terri You howeverrian Merci I You Again Sage This however I This Grade Again First First Surenire Finally Benedict Grade PLEASE Seriously First First Seriously Grade Grade Word I\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.2180417776107788, 0.271100252866745, 0.42507699131965637, 0.1779623031616211, 0.17705917358398438, 0.19131311774253845, 0.2952345311641693, 0.15088607370853424]\n",
      "Subjectivity reward: [0.800000011920929, 0.90625, 0.90625, 0.6904761791229248, 0.8611111044883728, 0.836363673210144, 0.9166666865348816, 0.7843137383460999]\n",
      "Similarity: [0.3571428656578064]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.0]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.5090209245681763, 0.5886751413345337, 0.665663480758667, 0.43421924114227295, 0.519085168838501, 0.5138384103775024, 0.6059505939483643, 0.46759989857673645]\n",
      "Shaped reward: [0.4661637842655182, 0.5458179712295532, 0.6228063106536865, 0.39136210083961487, 0.4762280285358429, 0.47098127007484436, 0.5630934238433838, 0.42474275827407837]\n",
      "Norm reward: [-0.37873080372810364, 0.6620415449142456, 1.6679816246032715, -1.3560997247695923, -0.24723002314567566, -0.31578487157821655, 0.8877649307250977, -0.9199435114860535]\n",
      "----------------------\n",
      "CE: 2.6325, RL: -0.3416, KL: 0.0000, TOTAL: 1.4428\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: &amp; gee....what is the stance of the participants on gun rights?\n",
      "[0] OUT: &amp; gee....what is the stance of the participants on gun rights?GH depune vieii mitigation upon year subject respectively pertain upon  upon relev aside clearly eachiola aflarium asideiola afla all uponiola relative\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.2769095003604889, 0.7545285224914551, 0.21908995509147644, 0.15533123910427094, 0.14527709782123566, 0.22955158352851868, 0.22193396091461182, 0.4228500723838806]\n",
      "Subjectivity reward: [0.9354838728904724, 0.9473684430122375, 0.8108108043670654, 0.9090908765792847, 0.8333333134651184, 0.9375, 0.9200000166893005, 0.949999988079071]\n",
      "Similarity: [0.4736842215061188]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.0]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.6061967015266418, 0.8509484529495239, 0.5149503946304321, 0.5322110652923584, 0.48930519819259644, 0.5835257768630981, 0.5709669589996338, 0.6864250302314758]\n",
      "Shaped reward: [0.5983019471168518, 0.8430536985397339, 0.5070556402206421, 0.5243163108825684, 0.4814104735851288, 0.5756310224533081, 0.5630722045898438, 0.6785302758216858]\n",
      "Norm reward: [0.0160956010222435, 2.1110148429870605, -0.7649147510528564, -0.6171743869781494, -0.9844210743904114, -0.1779530942440033, -0.2854485809803009, 0.7027990818023682]\n",
      "----------------------\n",
      "CE: 2.2193, RL: -0.2420, KL: 0.0000, TOTAL: 1.2348\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: If someone starts out by stating how educated they are, they are just about to say something stupid, hostile, or offensive. comedy\n",
      "[0] OUT: If someone starts out by stating how educated they are, they are just about to say something stupid, hostile, or offensive. comedyran any arata aside either either depending either literally depending either you whatever either either either either either aside either depending either you depending seriously depune anywhere honestly\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.13871720433235168, 0.45265042781829834, 0.1587323546409607, 0.1652432382106781, 0.596697986125946, 0.15700748562812805, 0.14190612733364105, 0.17316949367523193]\n",
      "Subjectivity reward: [0.8367347121238708, 0.9444444179534912, 0.837837815284729, 0.8421052694320679, 0.8846153616905212, 0.8709677457809448, 0.7941176295280457, 0.9743589758872986]\n",
      "Similarity: [0.7407407164573669]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.0]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.48772597312927246, 0.6985474228858948, 0.49828508496284485, 0.5036742687225342, 0.7406566739082336, 0.5139876008033752, 0.46801188588142395, 0.5737642049789429]\n",
      "Shaped reward: [0.559948205947876, 0.7707696557044983, 0.570507287979126, 0.5758965015411377, 0.8128789067268372, 0.5862098336219788, 0.5402340888977051, 0.6459864377975464]\n",
      "Norm reward: [-0.7047114968299866, 1.3345030546188354, -0.6025766134262085, -0.5504482984542847, 1.7418135404586792, -0.45069044828414917, -0.8954004645347595, 0.12751126289367676]\n",
      "----------------------\n",
      "CE: 2.2383, RL: -0.2442, KL: 0.0000, TOTAL: 1.2453\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: Do you like and travel? Here are our 14 Worst Travel Tips You Need To Know &amp; not to consider! travel\n",
      "[0] OUT: Do you like and travel? Here are our 14 Worst Travel Tips You Need To Know &amp; not to consider! travel HERE Back  all PLUS each  PLUS all Trust Explore Week All Week all Seriously all Week Week\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.2548968493938446, 0.18985217809677124, 0.25919196009635925, 0.19343817234039307, 0.21002553403377533, 0.22289836406707764, 0.18232449889183044, 0.1427413523197174]\n",
      "Subjectivity reward: [0.7894736528396606, 0.9130434989929199, 0.9375, 0.9375, 0.9032257795333862, 0.875, 0.9512194991111755, 0.9090908765792847]\n",
      "Similarity: [0.8181818127632141]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.0]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.5221852660179138, 0.551447868347168, 0.5983459949493408, 0.5654690861701965, 0.5566256642341614, 0.5489491820335388, 0.5667719841003418, 0.5259160995483398]\n",
      "Shaped reward: [0.6176398396492004, 0.6469024419784546, 0.6938005685806274, 0.6609236598014832, 0.652080237865448, 0.6444037556648254, 0.6622265577316284, 0.6213706731796265]\n",
      "Norm reward: [-1.3333038091659546, -0.12457907199859619, 1.81260085105896, 0.4545831084251404, 0.08929561823606491, -0.22779011726379395, 0.5084007978439331, -1.1791975498199463]\n",
      "----------------------\n",
      "CE: 1.9803, RL: 0.0723, KL: 0.0000, TOTAL: 1.2171\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: late 9/11 boithank you for being that taste and go\n",
      "[0] OUT: late 9/11 boithank you for being that taste and go all trziu late late which late trziu late late late later late late late\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.14824524521827698, 0.18953028321266174, 0.1769738644361496, 0.2627423107624054, 0.15767459571361542, 0.13454677164554596, 0.1815483123064041, 0.17262369394302368]\n",
      "Subjectivity reward: [0.9583333134651184, 0.8148148059844971, 0.9534883499145508, 0.75, 0.9166666865348816, 0.7222222089767456, 0.9047619104385376, 0.7272727489471436]\n",
      "Similarity: [0.75]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.0]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.5532892942428589, 0.5021725296974182, 0.5652310848236084, 0.5063711404800415, 0.5371706485748291, 0.4283844828605652, 0.5431551337242126, 0.4499482214450836]\n",
      "Shaped reward: [0.6282892823219299, 0.5771725177764893, 0.6402310729026794, 0.5813711285591125, 0.6121706366539001, 0.5033844709396362, 0.6181551218032837, 0.524948239326477]\n",
      "Norm reward: [0.8612170219421387, -0.17280937731266022, 1.1027840375900269, -0.08787687867879868, 0.5351575613021851, -1.6654466390609741, 0.6562159657478333, -1.2292393445968628]\n",
      "----------------------\n",
      "CE: 2.0974, RL: -0.0286, KL: 0.0000, TOTAL: 1.2470\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: lmaoooo same  that was bad sarcasm I was laughing while typing \"nothing was funny\"\n",
      "[0] OUT: lmaoooo same  that was bad sarcasm I was laughing while typing \"nothing was funny\"grade trziu aside aside off pr trziu which\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.16790072619915009, 0.5259610414505005, 0.5349573493003845, 0.16792020201683044, 0.36116570234298706, 0.1862860918045044, 0.1474360078573227, 0.5248806476593018]\n",
      "Subjectivity reward: [0.9047619104385376, 0.931034505367279, 1.0, 0.7777777910232544, 1.0, 0.782608687877655, 0.8461538553237915, 0.8947368264198303]\n",
      "Similarity: [0.7333333492279053]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.0]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.536331295967102, 0.7284977436065674, 0.7674787044525146, 0.4728490114212036, 0.6805828809738159, 0.4844473898410797, 0.4967949390411377, 0.7098087072372437]\n",
      "Shaped reward: [0.6063312888145447, 0.79849773645401, 0.8374786972999573, 0.5428490042686462, 0.7505828738212585, 0.5544474124908447, 0.5667949318885803, 0.7798087000846863]\n",
      "Norm reward: [-0.5936722159385681, 0.9634144902229309, 1.2792694568634033, -1.1080565452575684, 0.5751698017120361, -1.0140769481658936, -0.9140274524688721, 0.8119809031486511]\n",
      "----------------------\n",
      "CE: 1.6546, RL: -0.0571, KL: 0.0000, TOTAL: 0.9699\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: Often Love is is the reason why many people go to WAR/ And peace is what they fighting FOR.\n",
      "[0] OUT: Often love is the the reason why many people go to WAR/ And peace is what they fighting FOR. aside. Fifth Faith Wake which Peace\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.15152542293071747, 0.1898861527442932, 0.3657844364643097, 0.9279137253761292, 0.23187997937202454, 0.23226398229599, 0.17460113763809204, 0.6906871199607849]\n",
      "Subjectivity reward: [0.7599999904632568, 0.9200000166893005, 0.9090908765792847, 0.9444444179534912, 1.0, 0.90625, 0.7647058963775635, 0.9130434989929199]\n",
      "Similarity: [0.8717948794364929]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.0]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.45576271414756775, 0.5549430847167969, 0.637437641620636, 0.9361790418624878, 0.6159399747848511, 0.5692570209503174, 0.46965351700782776, 0.8018653392791748]\n",
      "Shaped reward: [0.5673011541366577, 0.6664815545082092, 0.7489761114120483, 1.047717571258545, 0.7274784445762634, 0.6807954907417297, 0.5811920166015625, 0.9134038090705872]\n",
      "Norm reward: [-1.1078872680664062, -0.45539847016334534, 0.08731739223003387, 1.738755464553833, -0.05411163717508316, -0.3612298369407654, -1.016502022743225, 1.169055700302124]\n",
      "----------------------\n",
      "CE: 1.6735, RL: -0.2416, KL: 0.0000, TOTAL: 0.9074\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: Good thing they weren't lions. The world might care then! nonewshere - 3 dead, 37 wounded in Chicago\n",
      "[0] OUT: Good thing they weren't lions. The world might care then! nonewshere - 3 dead, 37 wounded in Chicago depune Again Grade Grade All either either Grade Grade Sure either haine departe haine afla aside\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.18154224753379822, 0.2658461332321167, 0.4374231994152069, 0.13831225037574768, 0.24338263273239136, 0.5914614200592041, 0.8975257873535156, 0.4816516041755676]\n",
      "Subjectivity reward: [0.8529411554336548, 0.75, 1.0, 0.75, 0.8999999761581421, 0.8333333134651184, 1.0, 0.8421052694320679]\n",
      "Similarity: [0.782608687877655]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.0]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.5172417163848877, 0.5079230666160583, 0.7187116146087646, 0.44415611028671265, 0.5716912746429443, 0.7123973369598389, 0.9487628936767578, 0.6618784666061401]\n",
      "Shaped reward: [0.6020243167877197, 0.5927056670188904, 0.8034942150115967, 0.5289387106895447, 0.6564738750457764, 0.7971799373626709, 1.0335454940795898, 0.7466610670089722]\n",
      "Norm reward: [-0.7474214434623718, -0.808565616607666, 0.57451993227005, -1.2269713878631592, -0.3901515603065491, 0.5330889225006104, 1.863889455795288, 0.20161019265651703]\n",
      "----------------------\n",
      "CE: 1.4330, RL: -0.1971, KL: 0.0000, TOTAL: 0.7809\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: An excerpt from Summer Storms. artknb floridamindset\n",
      "[0] OUT: An excerpt from Summer Storms. artknb floridamindset\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.8457464575767517, 0.8725091218948364, 0.18745161592960358, 0.23747700452804565, 0.20954664051532745, 0.14691247045993805, 0.14123742282390594, 0.27994638681411743]\n",
      "Subjectivity reward: [1.0, 0.9583333134651184, 0.8181818127632141, 0.8333333134651184, 0.8421052694320679, 0.8125, 0.7916666865348816, 0.9473684430122375]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.9228732585906982, 0.9154212474822998, 0.5028167366981506, 0.535405158996582, 0.5258259773254395, 0.4797062277793884, 0.46645206212997437, 0.6136574149131775]\n",
      "Shaped reward: [1.0478732585906982, 1.0404212474822998, 0.6278167963027954, 0.660405158996582, 0.6508259773254395, 0.6047062873840332, 0.5914521217346191, 0.7386574745178223]\n",
      "Norm reward: [1.5637112855911255, 1.5637112855911255, -0.6261278986930847, -0.43438559770584106, -0.49074727296829224, -0.7621047496795654, -0.8400892019271851, 0.02603279985487461]\n",
      "----------------------\n",
      "CE: 1.1825, RL: -0.0831, KL: 0.0000, TOTAL: 0.6762\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: 'ban of shortselling' day back in 08-09ish, flash crash day cause that guy but not many rallies like this\n",
      "[0] OUT: 'ban of shortselling' day back in 08-09ish, flash crash day cause that guy but not many rallies like this should aside your\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.19691196084022522, 0.18960681557655334, 0.1699962466955185, 0.28227919340133667, 0.34658440947532654, 0.1491413563489914, 0.19598597288131714, 0.6745516061782837]\n",
      "Subjectivity reward: [0.8181818127632141, 0.7272727489471436, 0.8500000238418579, 0.875, 0.8999999761581421, 0.699999988079071, 0.4333333373069763, 1.0]\n",
      "Similarity: [0.9230769276618958]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.0]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.5075469017028809, 0.45843976736068726, 0.5099981427192688, 0.5786396265029907, 0.6232922077178955, 0.4245706796646118, 0.31465965509414673, 0.8372758030891418]\n",
      "Shaped reward: [0.6344699859619141, 0.5853628516197205, 0.636921226978302, 0.7055627107620239, 0.7502152919769287, 0.551493763923645, 0.44158273935317993, 0.964198887348175]\n",
      "Norm reward: [-0.1561099886894226, -0.47216033935546875, -0.14033396542072296, 0.30143821239471436, 0.5888193249702454, -0.6901395916938782, -1.397519826889038, 1.9660038948059082]\n",
      "----------------------\n",
      "CE: 1.2451, RL: -0.0486, KL: 0.0000, TOTAL: 0.7276\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: It's been a long ass day, I couldn't be happier to come home to see I can't even park in my own driveway.\n",
      "[0] OUT: It's been a long ass day, I couldn't be happier to come home to see I can't even park in my own driveway.GH\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.22491256892681122, 0.1657629758119583, 0.7684386968612671, 0.6066124439239502, 0.7256459593772888, 0.2923736572265625, 0.1669188290834427, 0.22514238953590393]\n",
      "Subjectivity reward: [0.9130434989929199, 0.6666666269302368, 1.0, 0.5882352590560913, 1.0, 1.0, 0.6666666269302368, 0.8333333134651184]\n",
      "Similarity: [0.9523809552192688]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.0023809524718672037]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.5689780116081238, 0.41621479392051697, 0.8842193484306335, 0.5974238514900208, 0.8628230094909668, 0.6461868286132812, 0.41679272055625916, 0.5292378664016724]\n",
      "Shaped reward: [0.70350182056427, 0.5507386326789856, 1.0187432765960693, 0.731947660446167, 0.997346818447113, 0.7807106375694275, 0.5513165593147278, 0.6637616753578186]\n",
      "Norm reward: [-0.25133129954338074, -1.1256417036056519, 1.4456183910369873, -0.08852707594633102, 1.4304333925247192, 0.19055834412574768, -1.1223340034484863, -0.47877630591392517]\n",
      "----------------------\n",
      "CE: 1.0590, RL: -0.0990, KL: 0.0000, TOTAL: 0.5958\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: Can we just take a moment to appreciate this sign? mylife signs nss @ Conch\n",
      "[0] OUT: Can we just take a moment to appreciate this sign? mylife signs nss @ Conch... please\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.21134218573570251, 0.1544412523508072, 0.17637501657009125, 0.28242623805999756, 0.6230787634849548, 0.182335764169693, 0.1811874806880951, 0.25587281584739685]\n",
      "Subjectivity reward: [0.8125, 0.875, 0.875, 0.699999988079071, 0.8333333134651184, 0.7894736528396606, 0.8947368264198303, 0.8333333134651184]\n",
      "Similarity: [0.9032257795333862]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.0]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.5119211077690125, 0.514720618724823, 0.5256875157356262, 0.4912131130695343, 0.7282060384750366, 0.4859046936035156, 0.5379621386528015, 0.5446030497550964]\n",
      "Shaped reward: [0.6328888535499573, 0.6356883645057678, 0.646655261516571, 0.6121808290481567, 0.8491737842559814, 0.6068724393844604, 0.6589298844337463, 0.6655707955360413]\n",
      "Norm reward: [-0.3935095965862274, -0.35751578211784363, -0.21651239693164825, -0.6597564220428467, 2.3873050212860107, -0.7280073761940002, -0.058695342391729355, 0.02668805979192257]\n",
      "----------------------\n",
      "CE: 0.9412, RL: -0.0284, KL: 0.0000, TOTAL: 0.5533\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: Love being called a female dog, It just warms my heart, You know.\n",
      "[0] OUT: Love being called a female dog, It just warms my heart, You know.han vieii all\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.15866124629974365, 0.6360092163085938, 0.22078277170658112, 0.17580212652683258, 0.18626675009727478, 0.5423831343650818, 0.1432250291109085, 0.4571247398853302]\n",
      "Subjectivity reward: [0.800000011920929, 0.7857142686843872, 0.9090908765792847, 0.7857142686843872, 0.9200000166893005, 0.8571428656578064, 0.8571428656578064, 0.8799999952316284]\n",
      "Similarity: [0.8571428656578064]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.0]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.4793306291103363, 0.7108617424964905, 0.5649368166923523, 0.4807581901550293, 0.5531333684921265, 0.6997630000114441, 0.5001839399337769, 0.6685623526573181]\n",
      "Shaped reward: [0.5864734649658203, 0.8180046081542969, 0.6720796823501587, 0.5879010558128357, 0.6602762341499329, 0.8069058656692505, 0.6073268055915833, 0.7757052183151245]\n",
      "Norm reward: [-1.0546221733093262, 1.3192474842071533, -0.1769086718559265, -1.039985179901123, -0.29792845249176025, 1.2054530382156372, -0.8408145308494568, 0.8855553865432739]\n",
      "----------------------\n",
      "CE: 0.9039, RL: 0.0232, KL: 0.0000, TOTAL: 0.5516\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: Stop running toward things, wanting things. Once you do that, the things that you are running after will run after you. yoga peace\n",
      "[0] OUT: Stop running toward things, wanting things. Once you do that, the things that you are running after will run after you. yoga peace\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.20871411263942719, 0.2680443525314331, 0.16238820552825928, 0.29109522700309753, 0.2298518866300583, 0.1998041272163391, 0.2298787385225296, 0.2670729458332062]\n",
      "Subjectivity reward: [0.9130434989929199, 0.8181818127632141, 0.9090908765792847, 0.9090908765792847, 0.8400000333786011, 0.8695652484893799, 0.949999988079071, 0.8235294222831726]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.5608788132667542, 0.543113112449646, 0.535739541053772, 0.6000930666923523, 0.5349259376525879, 0.5346846580505371, 0.5899393558502197, 0.5453011989593506]\n",
      "Shaped reward: [0.6858788728713989, 0.668113112449646, 0.660739541053772, 0.7250931262969971, 0.6599259376525879, 0.6596846580505371, 0.7149393558502197, 0.6703011989593506]\n",
      "Norm reward: [0.20407000184059143, -0.4807046055793762, -0.764916181564331, 1.7155687808990479, -0.7962762117385864, -0.8055762052536011, 1.3241955041885376, -0.39636561274528503]\n",
      "----------------------\n",
      "CE: 0.8439, RL: 0.0017, KL: 0.0000, TOTAL: 0.5070\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: Good thing this climate change stuff was all made up by Al Gore.\n",
      "[0] OUT: Good thing this climate change stuff was all made up by Al Gore.\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.1918378323316574, 0.15194648504257202, 0.16332443058490753, 0.31647470593452454, 0.29492253065109253, 0.188691183924675, 0.3917333781719208, 0.24442794919013977]\n",
      "Subjectivity reward: [0.9230769276618958, 0.625, 1.0, 0.8181818127632141, 0.875, 0.9166666865348816, 0.8823529481887817, 0.9259259104728699]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.5574573874473572, 0.388473242521286, 0.5816622376441956, 0.5673282742500305, 0.5849612951278687, 0.5526789426803589, 0.6370431780815125, 0.585176944732666]\n",
      "Shaped reward: [0.682457447052002, 0.5134732723236084, 0.7066622972488403, 0.6923283338546753, 0.7099612951278687, 0.6776789426803589, 0.7620432376861572, 0.710176944732666]\n",
      "Norm reward: [0.008372506126761436, -2.311936140060425, 0.34072744846343994, 0.14390888810157776, 0.38602572679519653, -0.057240769267082214, 1.101158857345581, 0.3889867961406708]\n",
      "----------------------\n",
      "CE: 0.6989, RL: -0.0737, KL: 0.0000, TOTAL: 0.3899\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: I love how snuff &amp; cigar companies aim all their advertising at men. I mean it's not like women dip, chew, or smoke cigars.\n",
      "[0] OUT: I love how snuff &amp; cigar companies aim all their advertising at men. I mean it's not like women dip, chew, or smoke cigars. off\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.20713604986667633, 0.5023590922355652, 0.23308086395263672, 0.8852272033691406, 0.2533411979675293, 0.8722839951515198, 0.1323147714138031, 0.6919385194778442]\n",
      "Subjectivity reward: [0.8799999952316284, 0.9285714030265808, 0.8571428656578064, 0.95652174949646, 0.3333333134651184, 0.9411764740943909, 0.8999999761581421, 1.0]\n",
      "Similarity: [0.978723406791687]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.028723403811454773]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.5435680150985718, 0.715465247631073, 0.545111894607544, 0.9208744764328003, 0.29333725571632385, 0.9067302346229553, 0.5161573886871338, 0.8459692597389221]\n",
      "Shaped reward: [0.6728233695030212, 0.8447206020355225, 0.6743672490119934, 1.050129771232605, 0.4225925803184509, 1.0359855890274048, 0.6454127430915833, 0.9752246141433716]\n",
      "Norm reward: [-0.5083572864532471, 0.31162765622138977, -0.5009927153587341, 1.052342176437378, -1.7020093202590942, 1.052342176437378, -0.6391115784645081, 0.9341585040092468]\n",
      "----------------------\n",
      "CE: 0.6656, RL: -0.1176, KL: 0.0000, TOTAL: 0.3523\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: I think he agrees with you .\n",
      "[0] OUT: I think he agrees with you .\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.5375908017158508, 0.24710871279239655, 0.14837247133255005, 0.18982595205307007, 0.20276577770709991, 0.20167315006256104, 0.14609335362911224, 0.15987980365753174]\n",
      "Subjectivity reward: [0.8571428656578064, 0.5833333730697632, 0.9230769276618958, 0.9166666865348816, 0.8571428656578064, 0.5714285373687744, 0.800000011920929, 0.699999988079071]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.6973668336868286, 0.41522103548049927, 0.5357246994972229, 0.5532463192939758, 0.5299543142318726, 0.3865508437156677, 0.4730466902256012, 0.4299398958683014]\n",
      "Shaped reward: [0.8223668336868286, 0.540221095085144, 0.6607247591018677, 0.6782463788986206, 0.6549543142318726, 0.5115509033203125, 0.5980467200279236, 0.5549399256706238]\n",
      "Norm reward: [1.9527463912963867, -0.8765228986740112, 0.33185020089149475, 0.5075515508651733, 0.27398598194122314, -1.1640186309814453, -0.29666557908058167, -0.7289270162582397]\n",
      "----------------------\n",
      "CE: 0.7253, RL: 0.0217, KL: 0.0000, TOTAL: 0.4438\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: obviously the solution to the wdbj incident is more guns. more guns stops gun violence. stoptheviolence\n",
      "[0] OUT: obviously the solution to the wdbj incident is more guns. more guns stops gun violence. stoptheviolence obviously\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.1827201098203659, 0.18687285482883453, 0.8726195693016052, 0.6129995584487915, 0.465564101934433, 0.1868600696325302, 0.21398955583572388, 0.5935627222061157]\n",
      "Subjectivity reward: [0.8823529481887817, 0.8333333134651184, 0.75, 0.875, 0.8666666746139526, 0.75, 1.0, 0.8888888955116272]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.532536506652832, 0.5101031064987183, 0.811309814453125, 0.7439997792243958, 0.666115403175354, 0.4684300422668457, 0.6069947481155396, 0.7412258386611938]\n",
      "Shaped reward: [0.657536506652832, 0.6351031064987183, 0.936309814453125, 0.8689998388290405, 0.791115403175354, 0.5934300422668457, 0.7319947481155396, 0.8662258386611938]\n",
      "Norm reward: [-0.8185970187187195, -0.997664749622345, 1.406625509262085, 0.8693442344665527, 0.24765558540821075, -1.3303072452545166, -0.22425693273544312, 0.8472016453742981]\n",
      "----------------------\n",
      "CE: 0.6889, RL: -0.0930, KL: 0.0000, TOTAL: 0.3761\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: literally everyone but you is sitting... sorryhad2 respecttheprez\n",
      "[0] OUT: literally everyone but you is sitting... sorryhad2 respecttheprezDOWN\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.199581116437912, 0.3474062383174896, 0.2632400691509247, 0.4731849730014801, 0.8124876022338867, 0.15928317606449127, 0.13734585046768188, 0.23972339928150177]\n",
      "Subjectivity reward: [1.0, 1.0, 0.761904776096344, 0.9473684430122375, 0.9545454382896423, 0.875, 0.9090908765792847, 0.9444444179534912]\n",
      "Similarity: [0.875]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.0]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.5997905731201172, 0.673703134059906, 0.5125724077224731, 0.71027672290802, 0.8835165500640869, 0.517141580581665, 0.5232183933258057, 0.5920839309692383]\n",
      "Shaped reward: [0.7122905850410461, 0.786203145980835, 0.6250724196434021, 0.822776734828949, 0.9960165619850159, 0.629641592502594, 0.6357184052467346, 0.7045839428901672]\n",
      "Norm reward: [-0.2106812298297882, 0.3715069591999054, -0.8976738452911377, 0.6595866680145264, 2.0241475105285645, -0.8616837859153748, -0.8138184547424316, -0.27138423919677734]\n",
      "----------------------\n",
      "CE: 0.4893, RL: -0.0098, KL: 0.0000, TOTAL: 0.2897\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: do you also read tea leaves and get your palm read? LOL\n",
      "[0] OUT: do you also read tea leaves and get your palm read? LOL nothing all\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.2079673558473587, 0.9006393551826477, 0.40654274821281433, 0.5830552577972412, 0.5433562994003296, 0.8659427165985107, 0.18303322792053223, 0.2616530954837799]\n",
      "Subjectivity reward: [1.0, 0.7333333492279053, 0.8095238208770752, 0.6666666269302368, 0.8666666746139526, 0.8500000238418579, 0.9375, 0.8888888955116272]\n",
      "Similarity: [0.9230769276618958]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.0]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.6039837002754211, 0.8169863224029541, 0.608033299446106, 0.624860942363739, 0.7050114870071411, 0.8579713702201843, 0.5602666139602661, 0.5752710103988647]\n",
      "Shaped reward: [0.7309067845344543, 0.9439094066619873, 0.7349563837051392, 0.7517840266227722, 0.8319345712661743, 0.9848944544792175, 0.6871896982192993, 0.702194094657898]\n",
      "Norm reward: [-0.5756616592407227, 1.3088942766189575, -0.5398325324058533, -0.3909487724304199, 0.31818887591362, 1.6715123653411865, -0.962451696395874, -0.8296992182731628]\n",
      "----------------------\n",
      "CE: 0.4100, RL: -0.0232, KL: 0.0000, TOTAL: 0.2367\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: This sun is just too much!! I wish I was back in freezing cold Blighty!\n",
      "[0] OUT: This sun is just too much!! I wish I was back in freezing cold Blighty!\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.1927032768726349, 0.21182909607887268, 0.24673175811767578, 0.8385714888572693, 0.19116081297397614, 0.23678305745124817, 0.8004404306411743, 0.3462848663330078]\n",
      "Subjectivity reward: [0.6000000238418579, 0.8333333134651184, 0.6363636255264282, 1.0, 0.9230769276618958, 0.9047619104385376, 1.0, 1.0]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.3963516354560852, 0.5225812196731567, 0.441547691822052, 0.919285774230957, 0.5571188926696777, 0.5707724690437317, 0.9002202153205872, 0.6731424331665039]\n",
      "Shaped reward: [0.52135169506073, 0.6475812196731567, 0.5665477514266968, 1.044285774230957, 0.6821188926696777, 0.6957725286483765, 1.025220274925232, 0.7981424331665039]\n",
      "Norm reward: [-1.1995025873184204, -0.5036329627037048, -0.9503488540649414, 1.4391577243804932, -0.31323596835136414, -0.2379671335220337, 1.4391577243804932, 0.32637080550193787]\n",
      "----------------------\n",
      "CE: 0.4711, RL: -0.0382, KL: 0.0000, TOTAL: 0.2674\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: Did you know in the giant brontosaurus on the way to Palm Springs is a Creationist museum? gofigure mindblown funfact\n",
      "[0] OUT: Did you know in the giant brontosaurus on the way to Palm Springs is a Creationist museum? gofigure mindblown funfact\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.22247743606567383, 0.3432246446609497, 0.24835842847824097, 0.4073980748653412, 0.8587211966514587, 0.3446808457374573, 0.1596754789352417, 0.6418110728263855]\n",
      "Subjectivity reward: [0.8999999761581421, 0.9090908765792847, 0.9230769276618958, 0.7142857313156128, 0.8888888955116272, 0.6666666269302368, 0.800000011920929, 0.800000011920929]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.561238706111908, 0.6261577606201172, 0.5857176780700684, 0.5608419179916382, 0.873805046081543, 0.5056737661361694, 0.4798377454280853, 0.7209055423736572]\n",
      "Shaped reward: [0.6862387657165527, 0.7511577606201172, 0.7107176780700684, 0.6858419179916382, 0.998805046081543, 0.6306737661361694, 0.6048377752304077, 0.8459055423736572]\n",
      "Norm reward: [-0.41333791613578796, 0.09263408929109573, -0.22255177795886993, -0.41643089056015015, 2.0227715969085693, -0.8464058041572571, -1.0477688312530518, 0.8310886025428772]\n",
      "----------------------\n",
      "CE: 0.4100, RL: -0.0520, KL: 0.0000, TOTAL: 0.2252\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: Hadn't seen this before. RT \"FIRST gopdebate HIGHLIGHTS: 2015\"  A Bad Lip Reading p2 tlot gop dems\n",
      "[0] OUT: Hadn't seen this before. RT \"FIRST gopdebate HIGHLIGHTS: 2015\"  A Bad Lip Reading p2 tlot gop dems either\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.2227780967950821, 0.19242867827415466, 0.163505420088768, 0.15612715482711792, 0.13524027168750763, 0.43877682089805603, 0.6976327300071716, 0.2549355626106262]\n",
      "Subjectivity reward: [0.9473684430122375, 0.9375, 0.8571428656578064, 1.0, 0.8260869383811951, 0.9090908765792847, 0.9411764740943909, 0.7857142686843872]\n",
      "Similarity: [0.9729729890823364]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.022972973063588142]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.5850732922554016, 0.5649643540382385, 0.5103241205215454, 0.5780636072158813, 0.48066359758377075, 0.6739338636398315, 0.8194046020507812, 0.5203249454498291]\n",
      "Shaped reward: [0.7154787182807922, 0.6953697800636292, 0.640729546546936, 0.708469033241272, 0.6110690236091614, 0.8043392896652222, 0.9498100280761719, 0.6507303714752197]\n",
      "Norm reward: [-0.059662122279405594, -0.24365051090717316, -0.7435858249664307, -0.12379781156778336, -1.0149672031402588, 0.7533749938011169, 2.084371566772461, -0.6520824432373047]\n",
      "----------------------\n",
      "CE: 0.3370, RL: -0.0540, KL: 0.0000, TOTAL: 0.1806\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: Santorum: always the most substance on all issues: economy, life, foreign policy, religious liberty gopdebate\n",
      "[0] OUT: Santorum: always the most substance on all issues: economy, life, foreign policy, religious liberty gopdebate\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.2673777639865875, 0.3898293673992157, 0.2452736347913742, 0.33508437871932983, 0.19959408044815063, 0.6790532469749451, 0.27782174944877625, 0.6813253164291382]\n",
      "Subjectivity reward: [0.8666666746139526, 0.9230769276618958, 0.800000011920929, 0.9200000166893005, 0.699999988079071, 0.8888888955116272, 0.9047619104385376, 0.8888888955116272]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.5670222043991089, 0.6564531326293945, 0.5226368308067322, 0.6275421977043152, 0.44979703426361084, 0.7839710712432861, 0.5912918448448181, 0.7851071357727051]\n",
      "Shaped reward: [0.6920222043991089, 0.7814531326293945, 0.647636890411377, 0.75254225730896, 0.5747970342636108, 0.9089710712432861, 0.7162919044494629, 0.9101071357727051]\n",
      "Norm reward: [-0.4739810526371002, 0.28356000781059265, -0.8499549627304077, 0.03866513818502426, -1.4669582843780518, 1.3637239933013916, -0.268400102853775, 1.373347282409668]\n",
      "----------------------\n",
      "CE: 0.2640, RL: -0.0509, KL: 0.0000, TOTAL: 0.1380\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: OMG r u serious ??? Whoa if true.\n",
      "[0] OUT: OMG r u serious ??? Whoa if true.\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.24531570076942444, 0.9105848073959351, 0.38999971747398376, 0.22709901630878448, 0.3440278470516205, 0.23681902885437012, 0.2851017415523529, 0.43452632427215576]\n",
      "Subjectivity reward: [0.75, 1.0, 0.8999999761581421, 0.875, 0.8181818127632141, 0.8947368264198303, 0.9130434989929199, 1.0]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.497657835483551, 0.9552924036979675, 0.6449998617172241, 0.551049530506134, 0.5811048150062561, 0.5657778978347778, 0.5990726351737976, 0.7172631621360779]\n",
      "Shaped reward: [0.6226578950881958, 1.0802924633026123, 0.7699998617172241, 0.6760495901107788, 0.7061048746109009, 0.6907778978347778, 0.7240726947784424, 0.8422632217407227]\n",
      "Norm reward: [-1.1038286685943604, 2.067662239074707, 0.1345534771680832, -0.6550812721252441, -0.4024721384048462, -0.5312925577163696, -0.2514559030532837, 0.7419137358665466]\n",
      "----------------------\n",
      "CE: 0.2418, RL: 0.0010, KL: 0.0000, TOTAL: 0.1455\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: what's the over/under on how many bills players leave the game early due to injury? fullpanicmode\n",
      "[0] OUT: what's the over/under on how many bills players leave the game early due to injury? fullpanicmode\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.2783542275428772, 0.15042993426322937, 0.18672125041484833, 0.2712536156177521, 0.7226683497428894, 0.308488667011261, 0.2305351346731186, 0.1783309131860733]\n",
      "Subjectivity reward: [0.875, 0.75, 0.8999999761581421, 0.7368420958518982, 0.8181818127632141, 0.7727272510528564, 0.875, 0.699999988079071]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.5766770839691162, 0.4502149820327759, 0.5433605909347534, 0.5040478706359863, 0.7704250812530518, 0.5406079292297363, 0.5527675747871399, 0.4391654431819916]\n",
      "Shaped reward: [0.7016770839691162, 0.5752149820327759, 0.6683605909347534, 0.6290478706359863, 0.8954250812530518, 0.6656079292297363, 0.6777676343917847, 0.564165472984314]\n",
      "Norm reward: [0.2876010537147522, -0.9445197582244873, -0.03700168430805206, -0.42002567648887634, 2.175288677215576, -0.06382088363170624, 0.054651178419589996, -1.0521751642227173]\n",
      "----------------------\n",
      "CE: 0.1812, RL: -0.0312, KL: 0.0000, TOTAL: 0.0962\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: wait a minute, you have selfie pictures for real?\n",
      "[0] OUT: wait a minute, you have selfie pictures for real?\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.5461273193359375, 0.2781045734882355, 0.6083634495735168, 0.5498631596565247, 0.18047887086868286, 0.31492140889167786, 0.25381502509117126, 0.15135356783866882]\n",
      "Subjectivity reward: [0.8888888955116272, 0.875, 0.8333333134651184, 1.0, 0.8823529481887817, 0.9411764740943909, 0.800000011920929, 0.9444444179534912]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.71750807762146, 0.5765522718429565, 0.7208483815193176, 0.7749315500259399, 0.5314159393310547, 0.6280489563941956, 0.5269075036048889, 0.5478990077972412]\n",
      "Shaped reward: [0.84250807762146, 0.7015522718429565, 0.8458484411239624, 0.8999315500259399, 0.6564159393310547, 0.7530490159988403, 0.6519075632095337, 0.6728990077972412]\n",
      "Norm reward: [0.9158088564872742, -0.5266165137290955, 0.9499914050102234, 1.503433346748352, -0.9885045289993286, 0.0003586475213523954, -1.0346394777297974, -0.8198304176330566]\n",
      "----------------------\n",
      "CE: 0.2135, RL: -0.0263, KL: 0.0000, TOTAL: 0.1176\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: Darryl Dawkins, NBA Dunk Legend, Dead at 58: Darryl Dawkins, the 14-year NBA vet whose powerful dunks a... peace\n",
      "[0] OUT: Darryl Dawkins, NBA Dunk Legend, Dead at 58: Darryl Dawkins, the 14-year NBA vet whose powerful dunks a... peace\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.610882580280304, 0.3710327744483948, 0.14920933544635773, 0.6825280785560608, 0.26890724897384644, 0.3997204005718231, 0.27665287256240845, 0.3049975633621216]\n",
      "Subjectivity reward: [0.8421052694320679, 0.800000011920929, 0.9090908765792847, 0.8888888955116272, 0.8571428656578064, 0.5, 1.0, 0.9047619104385376]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.7264939546585083, 0.5855163931846619, 0.529150128364563, 0.785708487033844, 0.5630250573158264, 0.44986021518707275, 0.6383264064788818, 0.6048797369003296]\n",
      "Shaped reward: [0.8514939546585083, 0.7105164527893066, 0.654150128364563, 0.9107085466384888, 0.6880251169204712, 0.5748602151870728, 0.7633264064788818, 0.7298797369003296]\n",
      "Norm reward: [1.084381341934204, -0.2320869415998459, -0.7584438323974609, 1.6373357772827148, -0.44211429357528687, -1.4988645315170288, 0.26106002926826477, -0.05126979947090149]\n",
      "----------------------\n",
      "CE: 0.2001, RL: -0.0050, KL: 0.0000, TOTAL: 0.1181\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: it started w a whisper just came on\n",
      "[0] OUT: it started w a whisper just came on all\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.2597014307975769, 0.16629968583583832, 0.3078455328941345, 0.4610830545425415, 0.18442463874816895, 0.1531279981136322, 0.1674126237630844, 0.4331963062286377]\n",
      "Subjectivity reward: [0.8888888955116272, 1.0, 1.0, 1.0, 0.699999988079071, 0.8333333134651184, 0.8148148059844971, 0.5]\n",
      "Similarity: [0.9411764740943909]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.0]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.574295163154602, 0.5831498503684998, 0.6539227962493896, 0.7305415272712708, 0.44221231341362, 0.4932306408882141, 0.49111372232437134, 0.46659815311431885]\n",
      "Shaped reward: [0.7066481113433838, 0.7155027985572815, 0.7862757444381714, 0.8628944754600525, 0.5745652914047241, 0.6255835890769958, 0.6234666705131531, 0.5989511013031006]\n",
      "Norm reward: [0.19882453978061676, 0.2872394621372223, 0.9939143657684326, 1.7589600086212158, -1.1200355291366577, -0.6106127500534058, -0.6317504048347473, -0.8765407800674438]\n",
      "----------------------\n",
      "CE: 0.2194, RL: -0.0154, KL: 0.0000, TOTAL: 0.1255\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: \"Censorship with a smile. The best kind.\" fsn15\n",
      "[0] OUT: \"Censorship with a smile. The best kind.\" fsn15 please\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.7684801816940308, 0.5667392611503601, 0.38893383741378784, 0.3434450328350067, 0.22623710334300995, 0.2900935709476471, 0.6385529041290283, 0.8686597347259521]\n",
      "Subjectivity reward: [0.5555555820465088, 0.8571428656578064, 0.9090908765792847, 0.9333333373069763, 1.0, 0.75, 0.8235294222831726, 0.9230769276618958]\n",
      "Similarity: [0.9411764740943909]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.0]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.6620178818702698, 0.7119410634040833, 0.6490123271942139, 0.6383891701698303, 0.6131185293197632, 0.5200467705726624, 0.7310411930084229, 0.8958683013916016]\n",
      "Shaped reward: [0.7943708300590515, 0.844294011592865, 0.7813652753829956, 0.7707421183586121, 0.7454714775085449, 0.6523997187614441, 0.8633941411972046, 1.0282212495803833]\n",
      "Norm reward: [-0.11993259936571121, 0.3735153377056122, -0.2484813779592514, -0.353482186794281, -0.60326087474823, -1.5231956243515015, 0.5623037815093994, 1.9125357866287231]\n",
      "----------------------\n",
      "CE: 0.2551, RL: -0.0598, KL: 0.0000, TOTAL: 0.1291\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: color me surprised\n",
      "[0] OUT: color me surprised coloricoloriagetered\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.22674420475959778, 0.9408732056617737, 0.18548192083835602, 0.2538483440876007, 0.3541078269481659, 0.1614740490913391, 0.4466804563999176, 0.3065987527370453]\n",
      "Subjectivity reward: [1.0, 1.0, 0.75, 0.9166666865348816, 0.8823529481887817, 0.9333333373069763, 0.8125, 0.8235294222831726]\n",
      "Similarity: [0.8571428656578064]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.0]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.6133720874786377, 0.9704365730285645, 0.4677409529685974, 0.5852575302124023, 0.618230402469635, 0.5474036931991577, 0.6295902132987976, 0.5650640726089478]\n",
      "Shaped reward: [0.7205149531364441, 1.0775794982910156, 0.5748838186264038, 0.6924003958702087, 0.7253732681274414, 0.6545465588569641, 0.736733078956604, 0.6722069382667542]\n",
      "Norm reward: [-0.01267301756888628, 2.247034788131714, -1.1901378631591797, -0.2399863749742508, 0.02660769782960415, -0.5460442900657654, 0.11845466494560242, -0.4032556116580963]\n",
      "----------------------\n",
      "CE: 0.1627, RL: -0.0352, KL: 0.0000, TOTAL: 0.0835\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: this is right? is the biggest clownshow at circus\n",
      "[0] OUT: this is right? is the biggest clownshow at circus\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.24271613359451294, 0.17057572305202484, 0.8817901611328125, 0.2528722286224365, 0.4540160000324249, 0.2168959230184555, 0.8458012938499451, 0.2805040180683136]\n",
      "Subjectivity reward: [0.8888888955116272, 0.75, 1.0, 0.3333333134651184, 1.0, 0.800000011920929, 1.0, 1.0]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.5658025145530701, 0.460287868976593, 0.9408950805664062, 0.29310277104377747, 0.7270079851150513, 0.5084479451179504, 0.9229006767272949, 0.6402519941329956]\n",
      "Shaped reward: [0.6908025741577148, 0.5852879285812378, 1.0658950805664062, 0.41810277104377747, 0.8520079851150513, 0.6334480047225952, 1.047900676727295, 0.7652519941329956]\n",
      "Norm reward: [-0.2571514844894409, -0.7758513689041138, 1.2628333568572998, -1.597717523574829, 0.5353188514709473, -0.5391010046005249, 1.2628333568572998, 0.10883472859859467]\n",
      "----------------------\n",
      "CE: 0.1435, RL: 0.0048, KL: 0.0000, TOTAL: 0.0880\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: Lika a blue in the sky flowers pipi tembem late post this pict\n",
      "[0] OUT: Lika a blue in the sky flowers pipi tembem late post this pict\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.2102433294057846, 0.25036144256591797, 0.1652592122554779, 0.15168042480945587, 0.17961104214191437, 0.43482381105422974, 0.35784783959388733, 0.33027827739715576]\n",
      "Subjectivity reward: [1.0, 0.8333333134651184, 0.8461538553237915, 0.75, 0.8571428656578064, 0.9375, 0.9333333373069763, 0.800000011920929]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.6051216721534729, 0.5418473482131958, 0.5057065486907959, 0.45084020495414734, 0.5183769464492798, 0.6861618757247925, 0.645590603351593, 0.5651391744613647]\n",
      "Shaped reward: [0.7301217317581177, 0.6668473482131958, 0.6307065486907959, 0.5758402347564697, 0.6433769464492798, 0.8111618757247925, 0.7705906629562378, 0.6901391744613647]\n",
      "Norm reward: [0.519600510597229, -0.2967493534088135, -0.7630286812782288, -1.4708998203277588, -0.5995584726333618, 1.5651596784591675, 1.0417202711105347, 0.0037558148615062237]\n",
      "----------------------\n",
      "CE: 0.0961, RL: 0.0064, KL: 0.0000, TOTAL: 0.0603\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: Yes yes I know all women are into clothes and trends. That is ALL we care about.\n",
      "[0] OUT: Yes yes I know all women are into clothes and trends. That is ALL we care about.\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.2318604737520218, 0.22059042751789093, 0.8191503286361694, 0.137846440076828, 0.17610497772693634, 0.15490785241127014, 0.16829444468021393, 0.30691343545913696]\n",
      "Subjectivity reward: [0.7647058963775635, 1.0, 0.9599999785423279, 0.8235294222831726, 1.0, 0.9166666865348816, 0.8333333134651184, 0.615384578704834]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.49828317761421204, 0.6102952361106873, 0.8895751237869263, 0.4806879162788391, 0.58805251121521, 0.5357872843742371, 0.500813901424408, 0.4611490070819855]\n",
      "Shaped reward: [0.6232832074165344, 0.735295295715332, 1.0145751237869263, 0.6056879758834839, 0.71305251121521, 0.6607873439788818, 0.6258139610290527, 0.5861490368843079]\n",
      "Norm reward: [-0.5254871249198914, 0.30971086025238037, 2.2834339141845703, -0.6566827893257141, 0.1438615322113037, -0.2458442896604538, -0.50661700963974, -0.8023713827133179]\n",
      "----------------------\n",
      "CE: 0.1061, RL: -0.0227, KL: 0.0000, TOTAL: 0.0546\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: Pitbulls are so vicious...  peoplearestupid\n",
      "[0] OUT: Pitbulls are so vicious...  peoplearestupid...\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.17715777456760406, 0.21103745698928833, 0.3264707326889038, 0.8232113718986511, 0.9191093444824219, 0.18320655822753906, 0.6329538226127625, 0.17548711597919464]\n",
      "Subjectivity reward: [0.6000000238418579, 0.699999988079071, 0.8888888955116272, 1.0, 1.0, 0.75, 0.699999988079071, 0.8799999952316284]\n",
      "Similarity: [0.7272727489471436]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.0]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.3885788917541504, 0.4555187225341797, 0.6076798439025879, 0.911605715751648, 0.9595546722412109, 0.46660327911376953, 0.6664769053459167, 0.5277435779571533]\n",
      "Shaped reward: [0.4567607045173645, 0.5237005352973938, 0.675861656665802, 0.9797875285148621, 1.0277365446090698, 0.5347850918769836, 0.7346587181091309, 0.5959253907203674]\n",
      "Norm reward: [-1.1206085681915283, -0.7957688570022583, -0.05737476423382759, 1.417490005493164, 1.5155752897262573, -0.7419787049293518, 0.22795043885707855, -0.4452824592590332]\n",
      "----------------------\n",
      "CE: 0.0725, RL: 0.0026, KL: 0.0000, TOTAL: 0.0446\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: The 3 of them can sleep in that room. It REALLY is fine with me.\n",
      "[0] OUT: The 3 of them can sleep in that room. It REALLY is fine with me.\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.5344682335853577, 0.19011607766151428, 0.2904309630393982, 0.20590941607952118, 0.24862739443778992, 0.22297970950603485, 0.3181678056716919, 0.20183821022510529]\n",
      "Subjectivity reward: [0.8666666746139526, 0.9166666865348816, 0.75, 1.0, 1.0, 0.8823529481887817, 0.7777777910232544, 0.8571428656578064]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.7005674839019775, 0.5533913969993591, 0.5202155113220215, 0.6029546856880188, 0.6243137121200562, 0.5526663064956665, 0.5479727983474731, 0.5294905304908752]\n",
      "Shaped reward: [0.8255674839019775, 0.6783914566040039, 0.6452155113220215, 0.7279547452926636, 0.7493137121200562, 0.6776663064956665, 0.6729727983474731, 0.65449059009552]\n",
      "Norm reward: [2.0098214149475098, -0.4223061800003052, -0.9705485105514526, 0.3967418968677521, 0.7497051954269409, -0.43428951501846313, -0.5118511319160461, -0.8172751069068909]\n",
      "----------------------\n",
      "CE: 0.0541, RL: 0.0058, KL: 0.0000, TOTAL: 0.0348\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: Tons of weekend activities for the kids here. littleonesfp activites education learning development\n",
      "[0] OUT: Tons of weekend activities for the kids here. littleonesfp activit education learning development\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.7977869510650635, 0.3282248377799988, 0.8173057436943054, 0.23330502212047577, 0.28870221972465515, 0.16139233112335205, 0.20391879975795746, 0.3151196837425232]\n",
      "Subjectivity reward: [0.9230769276618958, 0.8571428656578064, 1.0, 0.7142857313156128, 1.0, 0.7857142686843872, 0.7894736528396606, 0.7333333492279053]\n",
      "Similarity: [0.9230769276618958]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.0]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.8604319095611572, 0.5926838517189026, 0.9086529016494751, 0.4737953841686249, 0.6443511247634888, 0.47355329990386963, 0.49669623374938965, 0.5242265462875366]\n",
      "Shaped reward: [0.9873549938201904, 0.7196069359779358, 1.0355759859085083, 0.6007184982299805, 0.771274209022522, 0.6004763841629028, 0.6236193180084229, 0.6511496305465698]\n",
      "Norm reward: [1.4740427732467651, -0.14958754181861877, 1.550722360610962, -0.8705297112464905, 0.16372397541999817, -0.8719978928565979, -0.7316586375236511, -0.5647141933441162]\n",
      "----------------------\n",
      "CE: 0.0749, RL: -0.0193, KL: 0.0000, TOTAL: 0.0372\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: cracked 5 Subtle Ways Hollywood Taught You To Be A Worse Person humor\n",
      "[0] OUT: cracked 5 Subtle Ways Hollywood Taught You To Be A Worse Person humor\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.19016212224960327, 0.5236937999725342, 0.25751107931137085, 0.16327764093875885, 0.9272119402885437, 0.17247362434864044, 0.3152754008769989, 0.21323777735233307]\n",
      "Subjectivity reward: [0.7692307829856873, 0.9333333373069763, 0.8571428656578064, 0.8095238208770752, 1.0, 0.9545454382896423, 1.0, 0.8181818127632141]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.47969645261764526, 0.7285135984420776, 0.5573269724845886, 0.4864007234573364, 0.9636059999465942, 0.5635095238685608, 0.6576377153396606, 0.5157098174095154]\n",
      "Shaped reward: [0.60469651222229, 0.8535135984420776, 0.6823270320892334, 0.6114007234573364, 1.0886059999465942, 0.6885095834732056, 0.7826377153396606, 0.6407098770141602]\n",
      "Norm reward: [-0.9319984912872314, 0.8757731914520264, -0.36797672510147095, -0.8832892775535583, 1.9400649070739746, -0.323057621717453, 0.3608269691467285, -0.6703446507453918]\n",
      "----------------------\n",
      "CE: 0.0448, RL: -0.0070, KL: 0.0000, TOTAL: 0.0241\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: out here spreading the love peace\n",
      "[0] OUT: out here spreading the love peace\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.3185693025588989, 0.6607088446617126, 0.551219642162323, 0.3402879238128662, 0.8140732645988464, 0.15235111117362976, 0.33467790484428406, 0.14721393585205078]\n",
      "Subjectivity reward: [0.6666666269302368, 0.8947368264198303, 0.7727272510528564, 0.9090908765792847, 0.9090908765792847, 0.6666666269302368, 0.8823529481887817, 0.8571428656578064]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.49261796474456787, 0.7777228355407715, 0.6619734764099121, 0.6246894001960754, 0.8615820407867432, 0.4095088839530945, 0.6085154414176941, 0.502178430557251]\n",
      "Shaped reward: [0.6176179647445679, 0.9027228355407715, 0.7869734764099121, 0.7496894598007202, 0.9865820407867432, 0.5345089435577393, 0.7335155010223389, 0.627178430557251]\n",
      "Norm reward: [-0.8262206315994263, 1.0623259544372559, 0.2955973744392395, 0.048626482486724854, 1.6178128719329834, -1.3767383098602295, -0.05851049721240997, -0.7628917098045349]\n",
      "----------------------\n",
      "CE: 0.0525, RL: -0.0062, KL: 0.0000, TOTAL: 0.0290\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: Plus how it can still help anti-war/pro-peace activists today peace endwar worldpeace\n",
      "[0] OUT: Plus how it can still help anti-war/pro-peace activists today peace endwar worldpeace\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.22363455593585968, 0.3425128757953644, 0.33547186851501465, 0.5451240539550781, 0.6963313817977905, 0.3569799065589905, 0.2104232907295227, 0.3554733395576477]\n",
      "Subjectivity reward: [0.75, 0.6000000238418579, 0.6666666269302368, 0.875, 1.0, 1.0, 0.7777777910232544, 0.9473684430122375]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.48681727051734924, 0.47125643491744995, 0.5010692477226257, 0.7100620269775391, 0.8481656908988953, 0.6784899234771729, 0.49410054087638855, 0.6514208912849426]\n",
      "Shaped reward: [0.6118173003196716, 0.5962564945220947, 0.6260693073272705, 0.8350620269775391, 0.97316575050354, 0.8034899234771729, 0.6191005706787109, 0.7764209508895874]\n",
      "Norm reward: [-0.8596206307411194, -0.9726393818855286, -0.756107747554779, 0.7618144154548645, 1.7648670673370361, 0.5325049757957458, -0.8067219853401184, 0.33590200543403625]\n",
      "----------------------\n",
      "CE: 0.0884, RL: -0.0273, KL: 0.0000, TOTAL: 0.0421\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: so now you can be a beach body coach, when you don't got a beach bod?\n",
      "[0] OUT: so now you can be a beach body coach, when you don't got a beach bod?\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.2603268623352051, 0.37579426169395447, 0.30188485980033875, 0.20969180762767792, 0.7943491339683533, 0.4474896788597107, 0.6609837412834167, 0.2250227928161621]\n",
      "Subjectivity reward: [0.9375, 0.8999999761581421, 0.8823529481887817, 0.7692307829856873, 0.7368420958518982, 0.75, 1.0, 0.6666666269302368]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.5989134311676025, 0.6378971338272095, 0.5921189188957214, 0.4894613027572632, 0.7655956149101257, 0.5987448692321777, 0.8304919004440308, 0.44584470987319946]\n",
      "Shaped reward: [0.7239134311676025, 0.7628971338272095, 0.7171189785003662, 0.6144613027572632, 0.8905956745147705, 0.7237448692321777, 0.9554919004440308, 0.5708447694778442]\n",
      "Norm reward: [-0.1636868417263031, 0.14060914516448975, -0.21672247350215912, -1.0180399417877197, 1.1373885869979858, -0.16500259935855865, 1.6439505815505981, -1.358498454093933]\n",
      "----------------------\n",
      "CE: 0.0495, RL: 0.0061, KL: 0.0000, TOTAL: 0.0321\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: It's nice to have it if you're on Twitter.\n",
      "[0] OUT: It's nice to have it if you're on Twitter.\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.4544718861579895, 0.9362861514091492, 0.32490676641464233, 0.2632325291633606, 0.1596313714981079, 0.3226597011089325, 0.807517945766449, 0.30611395835876465]\n",
      "Subjectivity reward: [0.8888888955116272, 0.9285714030265808, 0.8571428656578064, 0.6363636255264282, 0.6875, 0.800000011920929, 0.7142857313156128, 0.8888888955116272]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.6716803908348083, 0.932428777217865, 0.5910248160362244, 0.4497980773448944, 0.42356568574905396, 0.5613298416137695, 0.7609018087387085, 0.5975013971328735]\n",
      "Shaped reward: [0.7966804504394531, 1.0574288368225098, 0.7160248756408691, 0.5747981071472168, 0.5485657453536987, 0.6863298416137695, 0.8859018087387085, 0.7225013971328735]\n",
      "Norm reward: [0.3661957383155823, 1.7118408679962158, -0.16761311888694763, -1.1023049354553223, -1.2759205102920532, -0.36414599418640137, 0.9566962122917175, -0.1247490718960762]\n",
      "----------------------\n",
      "CE: 0.0450, RL: -0.0091, KL: 0.0000, TOTAL: 0.0234\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: Mom details sons attack in 911 call NEWS &gt;&gt; news post press\n",
      "[0] OUT: Mom details sons attack in 911 call NEWS &gt;&gt; news post press\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.916616678237915, 0.22208832204341888, 0.16930821537971497, 0.2979034185409546, 0.6459828615188599, 0.896601676940918, 0.6003735065460205, 0.21089953184127808]\n",
      "Subjectivity reward: [0.8333333134651184, 0.9333333373069763, 0.6842105388641357, 0.75, 0.8461538553237915, 1.0, 0.8947368264198303, 0.782608687877655]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.8749749660491943, 0.5777108073234558, 0.42675936222076416, 0.5239517092704773, 0.7460683584213257, 0.948300838470459, 0.747555136680603, 0.49675410985946655]\n",
      "Shaped reward: [0.9999749660491943, 0.7027108669281006, 0.5517593622207642, 0.6489517688751221, 0.8710683584213257, 1.073300838470459, 0.872555136680603, 0.6217541694641113]\n",
      "Norm reward: [1.2379555702209473, -0.4627699553966522, -1.3264029026031494, -0.7703397870063782, 0.5004472136497498, 1.2380987405776978, 0.5089534521102905, -0.9259443283081055]\n",
      "----------------------\n",
      "CE: 0.0237, RL: -0.0088, KL: 0.0000, TOTAL: 0.0107\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: It worries me that Benzema has so many medicals a week .  arsenal benzema\n",
      "[0] OUT: It worries me that Benzema has so many medicals a week .  arsenal benzema\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.28998443484306335, 0.256987065076828, 0.3534228801727295, 0.30468448996543884, 0.5290831923484802, 0.197506844997406, 0.28688278794288635, 0.18916720151901245]\n",
      "Subjectivity reward: [0.8571428656578064, 0.8636363744735718, 0.8999999761581421, 1.0, 0.9166666865348816, 1.0, 0.8823529481887817, 0.7142857313156128]\n",
      "Similarity: [0.9629629850387573]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.012962962500751019]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.5735636353492737, 0.5603117346763611, 0.6267114281654358, 0.6523422598838806, 0.7228749394416809, 0.5987534523010254, 0.5846178531646729, 0.4517264664173126]\n",
      "Shaped reward: [0.7059710621833801, 0.6927191615104675, 0.7591188549995422, 0.7847496867179871, 0.8552823662757874, 0.7311608791351318, 0.7170252799987793, 0.5841339230537415]\n",
      "Norm reward: [-0.29104843735694885, -0.4602195620536804, 0.3874256908893585, 0.7146238088607788, 1.6150299310684204, 0.030519740656018257, -0.14993250370025635, -1.846397042274475]\n",
      "----------------------\n",
      "CE: 0.0265, RL: 0.0033, KL: 0.0000, TOTAL: 0.0172\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: Obviously, if I don't want to eat there today, I don't want to eat at all.\n",
      "[0] OUT: IObviously, if I don't want to eat there today, I don't want to eat at all.\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.2274279147386551, 0.527810275554657, 0.14455240964889526, 0.7148647308349609, 0.5607537627220154, 0.23845021426677704, 0.18439249694347382, 0.8970118165016174]\n",
      "Subjectivity reward: [0.875, 1.0, 0.6470588445663452, 0.949999988079071, 0.9090908765792847, 0.875, 0.8461538553237915, 0.8333333134651184]\n",
      "Similarity: [0.9090909361839294]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.0]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.5512139797210693, 0.7639051675796509, 0.39580562710762024, 0.8324323892593384, 0.7349222898483276, 0.5567250847816467, 0.5152731537818909, 0.8651725649833679]\n",
      "Shaped reward: [0.6739412546157837, 0.8866324424743652, 0.518532931804657, 0.9551596641540527, 0.857649564743042, 0.6794523596763611, 0.6380004286766052, 0.9878998398780823]\n",
      "Norm reward: [-0.594388484954834, 0.6608197689056396, -1.5115388631820679, 1.0652368068695068, 0.48977577686309814, -0.56186443567276, -0.806495189666748, 1.25845468044281]\n",
      "----------------------\n",
      "CE: 0.0286, RL: 0.0029, KL: 0.0000, TOTAL: 0.0183\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: Those people who speaks nothing but the 'truth' &amp; totally 'can' handle criticism well.\n",
      "[0] OUT: Those people who speaks nothing but the 'truth' &amp; totally 'can' handle criticism well.\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.1861104518175125, 0.17197975516319275, 0.23034285008907318, 0.4108983278274536, 0.550184428691864, 0.20909622311592102, 0.3404428958892822, 0.2609892785549164]\n",
      "Subjectivity reward: [0.7857142686843872, 0.8571428656578064, 1.0, 0.9411764740943909, 1.0, 1.0, 0.8333333134651184, 0.699999988079071]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.48591235280036926, 0.5145612955093384, 0.6151714324951172, 0.6760374307632446, 0.7750922441482544, 0.6045480966567993, 0.5868880748748779, 0.4804946184158325]\n",
      "Shaped reward: [0.6109123826026917, 0.6395612955093384, 0.7401714324951172, 0.8010374307632446, 0.9000922441482544, 0.7295480966567993, 0.7118880748748779, 0.6054946184158325]\n",
      "Norm reward: [-1.0567107200622559, -0.7722533345222473, 0.22671300172805786, 0.8310564756393433, 1.8145798444747925, 0.1212330237030983, -0.05411478504538536, -1.110504150390625]\n",
      "----------------------\n",
      "CE: 0.0280, RL: -0.0024, KL: 0.0000, TOTAL: 0.0159\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: Mr. Gentleman, why can't come out happy in my pictures? comicstrip humor comedy gentleman vieta comedia\n",
      "[0] OUT: Mr. Gentleman, why can't come out happy in my pictures? comicstrip humor comedy gentleman vieta comedia\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.23808521032333374, 0.44126173853874207, 0.16097524762153625, 0.8296118974685669, 0.14836998283863068, 0.20032019913196564, 0.17276878654956818, 0.2551368176937103]\n",
      "Subjectivity reward: [0.8125, 0.8421052694320679, 0.8333333134651184, 0.8999999761581421, 0.7142857313156128, 0.8947368264198303, 1.0, 0.9090908765792847]\n",
      "Similarity: [0.9375]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.0]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.5252926349639893, 0.6416835188865662, 0.4971542954444885, 0.8648059368133545, 0.43132784962654114, 0.5475285053253174, 0.5863844156265259, 0.5821138620376587]\n",
      "Shaped reward: [0.6565426588058472, 0.7729335427284241, 0.6284043192863464, 0.9960559606552124, 0.5625778436660767, 0.6787785291671753, 0.7176344394683838, 0.7133638858795166]\n",
      "Norm reward: [-0.4566248953342438, 0.4404652714729309, -0.6735029220581055, 2.1601955890655518, -1.180864691734314, -0.28524050116539, 0.014243901707231998, -0.018671663478016853]\n",
      "----------------------\n",
      "CE: 0.0427, RL: 0.0028, KL: 0.0000, TOTAL: 0.0268\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: a banksy fan account re-tweeting corporate sponsored adverts. don't ya think?\n",
      "[0] OUT: a banksy fan account re-tweeting corporate sponsored adverts. don't ya think?\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.35622021555900574, 0.5432073473930359, 0.3464667797088623, 0.15395008027553558, 0.675466001033783, 0.2608885169029236, 0.8426980376243591, 0.2279312014579773]\n",
      "Subjectivity reward: [0.9090908765792847, 0.9230769276618958, 0.8888888955116272, 1.0, 0.8636363744735718, 0.9230769276618958, 0.9583333134651184, 0.9090908765792847]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.6326555609703064, 0.7331421375274658, 0.6176778078079224, 0.5769750475883484, 0.769551157951355, 0.5919827222824097, 0.9005156755447388, 0.5685110092163086]\n",
      "Shaped reward: [0.7576556205749512, 0.8581421375274658, 0.7426778078079224, 0.7019751071929932, 0.894551157951355, 0.7169827222824097, 1.0255156755447388, 0.6935110092163086]\n",
      "Norm reward: [-0.3449126183986664, 0.5664165019989014, -0.4807489216327667, -0.8498885631561279, 0.8966160416603088, -0.7137819528579712, 1.8529492616653442, -0.926650881767273]\n",
      "----------------------\n",
      "CE: 0.0157, RL: 0.0023, KL: 0.0000, TOTAL: 0.0104\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: Israeli settlers killing a Palastinian 'Baby' and they want WEST to stop to pursue a 'Nuke'. ''Pakistan should support ''\n",
      "[0] OUT: Israel settlers killing a Palastinian 'Baby' and they want WEST to stop to pursue a 'Nuke'. ''Pakistan should support ''\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.5233680009841919, 0.5824213624000549, 0.5207711458206177, 0.23351426422595978, 0.6766725182533264, 0.7509482502937317, 0.3148738145828247, 0.16994625329971313]\n",
      "Subjectivity reward: [0.800000011920929, 0.9333333373069763, 0.800000011920929, 1.0, 0.9333333373069763, 1.0, 0.6666666269302368, 0.8181818127632141]\n",
      "Similarity: [0.9444444179534912]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.0]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.6616840362548828, 0.7578773498535156, 0.6603856086730957, 0.6167571544647217, 0.8050029277801514, 0.8754740953445435, 0.49077022075653076, 0.4940640330314636]\n",
      "Shaped reward: [0.7950173616409302, 0.891210675239563, 0.7937189340591431, 0.750090479850769, 0.9383362531661987, 1.0088074207305908, 0.6241035461425781, 0.627397358417511]\n",
      "Norm reward: [-0.054642610251903534, 0.649296760559082, -0.06414445489645004, -0.38341599702835083, 0.9941601157188416, 1.44541335105896, -1.3053840398788452, -1.2812800407409668]\n",
      "----------------------\n",
      "CE: 0.0218, RL: 0.0005, KL: 0.0000, TOTAL: 0.0133\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: You mean mass bot posting and retweeting to build a brand and identify isn't worth reading?\n",
      "[0] OUT: You mean mass bot posting and retweeting to build a brand and identify isn't worth reading?\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.19352252781391144, 0.6071959733963013, 0.7658529281616211, 0.15023180842399597, 0.2589585483074188, 0.27710822224617004, 0.20666620135307312, 0.13269750773906708]\n",
      "Subjectivity reward: [0.875, 0.75, 0.8333333134651184, 0.75, 0.8461538553237915, 0.9473684430122375, 0.6666666269302368, 0.7647058963775635]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.5342612862586975, 0.6785979866981506, 0.7995930910110474, 0.4501159191131592, 0.5525562167167664, 0.612238347530365, 0.43666642904281616, 0.4487017095088959]\n",
      "Shaped reward: [0.6592613458633423, 0.8035980463027954, 0.9245930910110474, 0.5751159191131592, 0.6775562763214111, 0.7372384071350098, 0.5616664886474609, 0.5737017393112183]\n",
      "Norm reward: [-0.23326487839221954, 0.8954189419746399, 1.8415757417678833, -0.8912650942802429, -0.0902022123336792, 0.3764999806880951, -0.9964369535446167, -0.9023237228393555]\n",
      "----------------------\n",
      "CE: 0.0218, RL: -0.0017, KL: 0.0000, TOTAL: 0.0124\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: shoulda sold high on Travis bluejays\n",
      "[0] OUT: Shoulda sold high on Travis bluejays\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.21466602385044098, 0.25979259610176086, 0.7696473598480225, 0.18649645149707794, 0.21503642201423645, 0.18345707654953003, 0.2647808790206909, 0.27532559633255005]\n",
      "Subjectivity reward: [0.8333333134651184, 0.6666666269302368, 0.8823529481887817, 0.9230769276618958, 0.8695652484893799, 0.875, 0.7894736528396606, 0.9090908765792847]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.5239996910095215, 0.46322959661483765, 0.8260001540184021, 0.5547866821289062, 0.542300820350647, 0.5292285680770874, 0.5271272659301758, 0.5922082662582397]\n",
      "Shaped reward: [0.6489996910095215, 0.5882296562194824, 0.9510002136230469, 0.6797866821289062, 0.667300820350647, 0.6542285680770874, 0.6521272659301758, 0.7172082662582397]\n",
      "Norm reward: [-0.4184987246990204, -0.9730543494224548, 2.337400436401367, -0.13755269348621368, -0.2514921724796295, -0.3707827031612396, -0.38995811343193054, 0.20393715798854828]\n",
      "----------------------\n",
      "CE: 0.0181, RL: -0.0019, KL: 0.0000, TOTAL: 0.0101\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: where would you be without them though?\n",
      "[0] OUT: where would you be without them though\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.3436726927757263, 0.14751490950584412, 0.15337489545345306, 0.3804876506328583, 0.16862601041793823, 0.7500755786895752, 0.2628270983695984, 0.7941743731498718]\n",
      "Subjectivity reward: [0.7142857313156128, 0.5714285373687744, 0.8333333134651184, 0.8999999761581421, 0.8333333134651184, 0.9411764740943909, 1.0, 1.0]\n",
      "Similarity: [0.8571428656578064]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.0]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.5289791822433472, 0.35947173833847046, 0.49335411190986633, 0.6402438282966614, 0.5009796619415283, 0.8456259965896606, 0.6314135789871216, 0.8970872163772583]\n",
      "Shaped reward: [0.6361220479011536, 0.46661460399627686, 0.6004970073699951, 0.7473866939544678, 0.6081225275993347, 0.952768862247467, 0.738556444644928, 1.00423002243042]\n",
      "Norm reward: [-0.4541856646537781, -1.3858305215835571, -0.6499875783920288, 0.15734583139419556, -0.608076274394989, 1.2861649990081787, 0.10881310701370239, 1.545756220817566]\n",
      "----------------------\n",
      "CE: 0.0196, RL: 0.0076, KL: 0.0000, TOTAL: 0.0148\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: ITCore 3. Last meeting! late\n",
      "[0] OUT: ITCore 3. Last meeting! late\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.6606847047805786, 0.14030776917934418, 0.16863355040550232, 0.4722263216972351, 0.7328498363494873, 0.1559540033340454, 0.4005948007106781, 0.3539031147956848]\n",
      "Subjectivity reward: [1.0, 0.8888888955116272, 0.7647058963775635, 1.0, 0.9333333373069763, 0.8333333134651184, 0.6470588445663452, 0.9444444179534912]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.8303423523902893, 0.5145983099937439, 0.4666697382926941, 0.7361131906509399, 0.8330916166305542, 0.4946436583995819, 0.5238268375396729, 0.6491737365722656]\n",
      "Shaped reward: [0.9553424119949341, 0.6395983695983887, 0.5916697978973389, 0.8611131906509399, 0.9580916166305542, 0.6196436882019043, 0.6488268375396729, 0.7741737365722656]\n",
      "Norm reward: [1.3075181245803833, -0.7640934586524963, -1.0785551071166992, 0.6892757415771484, 1.3255558013916016, -0.8950170874595642, -0.7035450339317322, 0.11886186897754669]\n",
      "----------------------\n",
      "CE: 0.0134, RL: -0.0000, KL: 0.0000, TOTAL: 0.0080\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: : People Are Actually smarter, sexier And More successful\n",
      "[0] OUT: : People Are Actually smarter, sexier And More successful\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.8440893292427063, 0.14785172045230865, 0.9269039034843445, 0.2010670006275177, 0.4606036841869354, 0.2691769599914551, 0.14792627096176147, 0.1939375400543213]\n",
      "Subjectivity reward: [0.6666666269302368, 0.6363636255264282, 0.9333333373069763, 0.9166666865348816, 0.9230769276618958, 0.8333333134651184, 0.6666666269302368, 0.75]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.755378007888794, 0.39210766553878784, 0.9301186203956604, 0.5588668584823608, 0.6918402910232544, 0.5512551069259644, 0.40729644894599915, 0.47196877002716064]\n",
      "Shaped reward: [0.880378007888794, 0.5171077251434326, 1.0551186800003052, 0.6838668584823608, 0.8168402910232544, 0.6762551069259644, 0.5322964787483215, 0.5969687700271606]\n",
      "Norm reward: [0.9714778661727905, -1.1365262269973755, 1.6656266450881958, -0.1688474714756012, 0.6027778387069702, -0.21301734447479248, -1.048388123512268, -0.6731042861938477]\n",
      "----------------------\n",
      "CE: 0.0185, RL: -0.0002, KL: 0.0000, TOTAL: 0.0110\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: claimed I never tweet about her. So this is a tweet. About her. So. Yea. becauseican\n",
      "[0] OUT: @ I never tweet about her. So this is a tweet. About her. So. Yea. becauseican\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.2834658622741699, 0.19495026767253876, 0.3661764860153198, 0.5397179126739502, 0.2126765251159668, 0.843212366104126, 0.4537215232849121, 0.4765402674674988]\n",
      "Subjectivity reward: [0.875, 0.6000000238418579, 0.8333333134651184, 0.9047619104385376, 0.75, 1.0, 0.9473684430122375, 1.0]\n",
      "Similarity: [0.9285714030265808]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.0]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.579232931137085, 0.39747515320777893, 0.5997549295425415, 0.7222399115562439, 0.4813382625579834, 0.921606183052063, 0.7005449533462524, 0.7382701635360718]\n",
      "Shaped reward: [0.7078043222427368, 0.5260465741157532, 0.7283263206481934, 0.8508113622665405, 0.6099096536636353, 1.0501775741577148, 0.8291163444519043, 0.8668415546417236]\n",
      "Norm reward: [-0.3740396797657013, -1.5656498670578003, -0.23949675261974335, 0.5635194182395935, -1.0158405303955078, 1.5416053533554077, 0.42128613591194153, 0.6686139106750488]\n",
      "----------------------\n",
      "CE: 0.0168, RL: 0.0027, KL: 0.0000, TOTAL: 0.0112\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: Bills D. at 90. I want to feel how that guy does right now. Let me tell you he feels AWESOME. drugs\n",
      "[0] OUT: Bills D. at 90.... I want to feel how that guy does right now. Let me tell you he feels AWESOME. drugs\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.2645205557346344, 0.2598302662372589, 0.2626226842403412, 0.40238574147224426, 0.5074681639671326, 0.15237101912498474, 0.2871289849281311, 0.1814213991165161]\n",
      "Subjectivity reward: [0.7727272510528564, 0.6666666269302368, 0.7777777910232544, 0.5, 0.8695652484893799, 0.7222222089767456, 0.7647058963775635, 1.0]\n",
      "Similarity: [0.9545454382896423]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.004545454401522875]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.5186238884925842, 0.46324843168258667, 0.520200252532959, 0.45119285583496094, 0.6885167360305786, 0.43729662895202637, 0.5259174108505249, 0.5907106995582581]\n",
      "Shaped reward: [0.6527147889137268, 0.5973393321037292, 0.6542911529541016, 0.5852837562561035, 0.8226076364517212, 0.571387529373169, 0.6600083112716675, 0.7248015999794006]\n",
      "Norm reward: [-0.07045875489711761, -0.7386097311973572, -0.05143860727548599, -0.8840702772140503, 1.9794398546218872, -1.0517398118972778, 0.017543667927384377, 0.799328625202179]\n",
      "----------------------\n",
      "CE: 0.0165, RL: 0.0007, KL: 0.0000, TOTAL: 0.0102\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: When nature has a sense of humor: A pot of gold within in building?\n",
      "[0] OUT: When nature has a sense of humor: A pot of gold within in building?\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.17646199464797974, 0.6345824599266052, 0.1545260101556778, 0.17022311687469482, 0.8009002804756165, 0.2632834315299988, 0.21495649218559265, 0.48165133595466614]\n",
      "Subjectivity reward: [0.7142857313156128, 1.0, 0.625, 0.8571428656578064, 0.8666666746139526, 1.0, 0.7272727489471436, 0.9090908765792847]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.44537386298179626, 0.817291259765625, 0.3897629976272583, 0.5136829614639282, 0.8337835073471069, 0.6316417455673218, 0.4711146354675293, 0.6953710913658142]\n",
      "Shaped reward: [0.5703738927841187, 0.942291259765625, 0.5147629976272583, 0.6386829614639282, 0.9587835073471069, 0.7566417455673218, 0.5961146354675293, 0.820371150970459]\n",
      "Norm reward: [-0.9039618372917175, 1.2737913131713867, -1.2295900583267212, -0.5039797425270081, 1.3703612089157104, 0.18672503530979156, -0.7532375454902649, 0.5598910450935364]\n",
      "----------------------\n",
      "CE: 0.0127, RL: 0.0011, KL: 0.0000, TOTAL: 0.0081\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: Is he gay? I can't tell. via\n",
      "[0] OUT: Is he gay? I can't tell. via\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.6527398824691772, 0.20831412076950073, 0.8468956351280212, 0.4186401665210724, 0.8849714398384094, 0.9308063983917236, 0.2532672882080078, 0.8470162153244019]\n",
      "Subjectivity reward: [1.0, 0.8333333134651184, 1.0, 0.8888888955116272, 0.75, 0.8333333134651184, 1.0, 0.8125]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.8263699412345886, 0.5208237171173096, 0.923447847366333, 0.653764545917511, 0.8174856901168823, 0.8820698261260986, 0.6266336441040039, 0.8297581076622009]\n",
      "Shaped reward: [0.9513700008392334, 0.6458237171173096, 1.048447847366333, 0.7787646055221558, 0.9424856901168823, 1.0070698261260986, 0.7516336441040039, 0.9547581672668457]\n",
      "Norm reward: [0.548396110534668, -1.7386341094970703, 0.9123942852020264, -0.7435644865036011, 0.4818965792655945, 0.9123942852020264, -0.946641206741333, 0.573756754398346]\n",
      "----------------------\n",
      "CE: 0.0170, RL: -0.0016, KL: 0.0000, TOTAL: 0.0096\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: How do you Americanize an African name?\n",
      "[0] OUT: You do you americanize an African name?\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.26293572783470154, 0.45628926157951355, 0.5418487191200256, 0.5171773433685303, 0.24136297404766083, 0.4406896233558655, 0.3343304395675659, 0.1934443861246109]\n",
      "Subjectivity reward: [1.0, 0.875, 0.800000011920929, 0.9090908765792847, 0.7894736528396606, 0.8500000238418579, 0.949999988079071, 0.6666666269302368]\n",
      "Similarity: [0.9230769276618958]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.0]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.631467878818512, 0.665644645690918, 0.6709243655204773, 0.7131341099739075, 0.515418291091919, 0.6453448534011841, 0.6421651840209961, 0.43005549907684326]\n",
      "Shaped reward: [0.7583909630775452, 0.7925677299499512, 0.7978474497795105, 0.8400571942329407, 0.6423413753509521, 0.7722679376602173, 0.7690882682800293, 0.5569785833358765]\n",
      "Norm reward: [0.18366122245788574, 0.5486317276954651, 0.605013370513916, 1.055767297744751, -1.0556215047836304, 0.33185213804244995, 0.29789674282073975, -1.96720290184021]\n",
      "----------------------\n",
      "CE: 0.0138, RL: -0.0001, KL: 0.0000, TOTAL: 0.0083\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: Yes Yes Yes!!!!! This is so me this morning  happywednesday newday newstart humor\n",
      "[0] OUT: Yes Yes!!!!!!!!!! This is so me this morning  happywednesday newday newstart humor\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.21499836444854736, 0.16126424074172974, 0.29693105816841125, 0.21679188311100006, 0.23594070971012115, 0.2882344722747803, 0.5135787725448608, 0.311483234167099]\n",
      "Subjectivity reward: [0.6666666269302368, 0.8235294222831726, 0.9090908765792847, 1.0, 1.0, 0.8888888955116272, 0.8235294222831726, 0.7857142686843872]\n",
      "Similarity: [0.8695651888847351]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.0]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.4408324956893921, 0.49239683151245117, 0.6030109524726868, 0.6083959341049194, 0.61797034740448, 0.5885616540908813, 0.6685540676116943, 0.5485987663269043]\n",
      "Shaped reward: [0.551702082157135, 0.6032664179801941, 0.7138805389404297, 0.7192655205726624, 0.7288399338722229, 0.6994312405586243, 0.7794236540794373, 0.6594683527946472]\n",
      "Norm reward: [-1.7651721239089966, -1.0661351680755615, 0.43341609835624695, 0.5064181089401245, 0.6362146139144897, 0.23753276467323303, 1.3219578266143799, -0.3042280673980713]\n",
      "----------------------\n",
      "CE: 0.0179, RL: -0.0020, KL: 0.0000, TOTAL: 0.0100\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: Next gopdebate is Wednesday. Fiorina will be on stage w/ the top candidates this time.\n",
      "[0] OUT: Next gopdebate is Wednesday. Fiorina will be on stage w/ the top candidates this time.\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.8955314755439758, 0.17907188832759857, 0.34481218457221985, 0.1907169371843338, 0.9342229962348938, 0.15300968289375305, 0.24333441257476807, 0.15219922363758087]\n",
      "Subjectivity reward: [0.8666666746139526, 0.7272727489471436, 0.8571428656578064, 0.8500000238418579, 0.9545454382896423, 0.9375, 0.8947368264198303, 0.8461538553237915]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.8810991048812866, 0.45317232608795166, 0.6009775400161743, 0.5203585028648376, 0.9443842172622681, 0.5452548265457153, 0.5690356492996216, 0.4991765320301056]\n",
      "Shaped reward: [1.0060991048812866, 0.5781723260879517, 0.7259775400161743, 0.6453585624694824, 1.069384217262268, 0.6702548265457153, 0.6940356492996216, 0.624176561832428]\n",
      "Norm reward: [1.5607407093048096, -0.9935014843940735, -0.09851441532373428, -0.5866767764091492, 1.5607407093048096, -0.4359254539012909, -0.2919282913208008, -0.7149376273155212]\n",
      "----------------------\n",
      "CE: 0.0124, RL: -0.0014, KL: 0.0000, TOTAL: 0.0069\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: Today educatenoteradicate is used to promote Confederate Flag, previously used for 'Save the Wolves' &amp; educating on mental illness\n",
      "[0] OUT: Today educatenoteradicate is used to promote Confederate Flag, previously used for 'Save the Wolves' &amp; educating on mental illness\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.3507399559020996, 0.19748428463935852, 0.3916719853878021, 0.16257192194461823, 0.3435675799846649, 0.44630199670791626, 0.47878044843673706, 0.1821887344121933]\n",
      "Subjectivity reward: [0.9473684430122375, 0.7894736528396606, 0.800000011920929, 0.800000011920929, 0.9090908765792847, 0.9166666865348816, 1.0, 0.75]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.6490541696548462, 0.4934789538383484, 0.5958359837532043, 0.481285959482193, 0.626329243183136, 0.6814843416213989, 0.7393902540206909, 0.46609437465667725]\n",
      "Shaped reward: [0.7740541696548462, 0.6184790134429932, 0.7208360433578491, 0.6062859892845154, 0.7513293027877808, 0.8064843416213989, 0.8643902540206909, 0.5910943746566772]\n",
      "Norm reward: [0.566281795501709, -0.9676142930984497, 0.04157669469714165, -1.0878316164016724, 0.3422255516052246, 0.8860276341438293, 1.4569519758224487, -1.2376136779785156]\n",
      "----------------------\n",
      "CE: 0.0178, RL: -0.0024, KL: 0.0000, TOTAL: 0.0097\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: Carlson forgot how great this wine is takingonefortheteam\n",
      "[0] OUT: Carlson forgot how great this wine is takingonefortheteam\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.29538342356681824, 0.21647770702838898, 0.5115736126899719, 0.1493101269006729, 0.2833721935749054, 0.42724859714508057, 0.18352727591991425, 0.24977533519268036]\n",
      "Subjectivity reward: [0.875, 0.8636363744735718, 0.7333333492279053, 0.8235294222831726, 0.9047619104385376, 0.8333333134651184, 1.0, 0.8571428656578064]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.5851917266845703, 0.5400570631027222, 0.6224534511566162, 0.48641976714134216, 0.5940670371055603, 0.6302909851074219, 0.5917636156082153, 0.553459107875824]\n",
      "Shaped reward: [0.7101917266845703, 0.6650570631027222, 0.7474534511566162, 0.6114197969436646, 0.7190670967102051, 0.7552909851074219, 0.7167636156082153, 0.6784591674804688]\n",
      "Norm reward: [0.2061285823583603, -0.7501497268676758, 0.9956011772155762, -1.886574625968933, 0.3941730558872223, 1.1616567373275757, 0.3453686833381653, -0.46619632840156555]\n",
      "----------------------\n",
      "CE: 0.0096, RL: -0.0049, KL: 0.0000, TOTAL: 0.0038\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: us elections John Kerry meets with Iran's foreign minister: The top U.S. and Iranian diplomats met... politics\n",
      "[0] OUT: us elections John Kerry meets with Iran's foreign minister The top U.S. and Iranian diplomats met...\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.9024308323860168, 0.2626663148403168, 0.8861560225486755, 0.5169799327850342, 0.6041995286941528, 0.40052056312561035, 0.36595046520233154, 0.21823208034038544]\n",
      "Subjectivity reward: [0.875, 0.8947368264198303, 1.0, 0.8333333134651184, 0.615384578704834, 1.0, 0.949999988079071, 0.9090908765792847]\n",
      "Similarity: [0.9090909361839294]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.0]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.888715386390686, 0.5787015557289124, 0.9430780410766602, 0.6751565933227539, 0.6097920536994934, 0.7002602815628052, 0.6579751968383789, 0.5636614561080933]\n",
      "Shaped reward: [1.0114426612854004, 0.7014288306236267, 1.0658053159713745, 0.7978838682174683, 0.7325193285942078, 0.8229875564575195, 0.7807024717330933, 0.6863887310028076]\n",
      "Norm reward: [1.499722957611084, -0.9238067865371704, 1.499722957611084, -0.14087234437465668, -0.6714423298835754, 0.06289661675691605, -0.28033530712127686, -1.0458886623382568]\n",
      "----------------------\n",
      "CE: 0.0206, RL: 0.0036, KL: 0.0000, TOTAL: 0.0138\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: Good thing I bought a new Skyhawk card like three days ago because I found the \"missing\" one today.\n",
      "[0] OUT: Good thing I bought a new Skyhawk card like three days ago because I found the \"missing\" one today.\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.4746864140033722, 0.16812492907047272, 0.22568926215171814, 0.14875108003616333, 0.22787301242351532, 0.18868348002433777, 0.16117936372756958, 0.9158921837806702]\n",
      "Subjectivity reward: [0.8947368264198303, 0.75, 0.7368420958518982, 0.75, 0.75, 0.8235294222831726, 0.8500000238418579, 0.9375]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.6847116351127625, 0.45906245708465576, 0.481265664100647, 0.44937554001808167, 0.48893651366233826, 0.506106436252594, 0.5055897235870361, 0.9266960620880127]\n",
      "Shaped reward: [0.8097116947174072, 0.5840624570846558, 0.606265664100647, 0.574375569820404, 0.6139365434646606, 0.6311064958572388, 0.6305897235870361, 1.0516960620880127]\n",
      "Norm reward: [0.8654108047485352, -0.6547964811325073, -0.5052126049995422, -0.7200573682785034, -0.4535336494445801, -0.33785903453826904, -0.3413405418395996, 2.147390127182007]\n",
      "----------------------\n",
      "CE: 0.0094, RL: 0.0026, KL: 0.0000, TOTAL: 0.0067\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: Wow. What a waste of time. I'm so glad I did that. (rolling my eyes)\n",
      "[0] OUT: Wow. What a waste of time I I'm so glad I did that. (Ting my eyes)\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.23389798402786255, 0.3223987817764282, 0.3947714567184448, 0.35632258653640747, 0.21434295177459717, 0.19527772068977356, 0.16130228340625763, 0.3087846636772156]\n",
      "Subjectivity reward: [0.75, 0.8823529481887817, 1.0, 0.75, 0.9130434989929199, 0.8333333134651184, 0.8999999761581421, 0.8666666746139526]\n",
      "Similarity: [0.8666666746139526]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.0]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.4919489920139313, 0.602375864982605, 0.6973857283592224, 0.5531612634658813, 0.5636932253837585, 0.5143055319786072, 0.5306511521339417, 0.5877256393432617]\n",
      "Shaped reward: [0.6019489765167236, 0.7123758792877197, 0.8073857426643372, 0.6631612777709961, 0.6736932396888733, 0.6243055462837219, 0.6406511664390564, 0.6977256536483765]\n",
      "Norm reward: [-1.1839954853057861, 0.5429917573928833, 2.0288689136505127, -0.22668468952178955, -0.06197334825992584, -0.8343568444252014, -0.5787245631217957, 0.31387412548065186]\n",
      "----------------------\n",
      "CE: 0.0148, RL: -0.0008, KL: 0.0000, TOTAL: 0.0086\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: Multi-millionaire connected to F1 complains about high prices.\n",
      "[0] OUT: Multi-millionaire connected to F1 complains about high prices. all\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.836798906326294, 0.5785224437713623, 0.2011968046426773, 0.5105865001678467, 0.9331340193748474, 0.24913711845874786, 0.4304213523864746, 0.28046801686286926]\n",
      "Subjectivity reward: [0.8888888955116272, 0.800000011920929, 0.6000000238418579, 0.800000011920929, 0.9090908765792847, 0.875, 0.8947368264198303, 0.8333333134651184]\n",
      "Similarity: [0.9411764740943909]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.0]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.8628438711166382, 0.6892611980438232, 0.400598406791687, 0.6552932262420654, 0.9211124181747437, 0.5620685815811157, 0.6625790596008301, 0.556900680065155]\n",
      "Shaped reward: [0.9951968193054199, 0.821614146232605, 0.5329513549804688, 0.7876461744308472, 1.0534653663635254, 0.6944215297698975, 0.7949320077896118, 0.6892536282539368]\n",
      "Norm reward: [1.309888482093811, 0.20449405908584595, -1.6337437629699707, -0.011817888356745243, 1.3404756784439087, -0.6054831147193909, 0.03457913547754288, -0.6383929252624512]\n",
      "----------------------\n",
      "CE: 0.0145, RL: 0.0004, KL: 0.0000, TOTAL: 0.0088\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: Fun fact from tonight - made it to Europa L on fair play. Had a player sent off in every game so far!\n",
      "[0] OUT: Fun fact from tonight - made it to Europa L on fair play. Had a player sent off in every game so far!\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.33148619532585144, 0.15916907787322998, 0.2504841089248657, 0.8475720882415771, 0.1752384454011917, 0.186958447098732, 0.25182509422302246, 0.15615813434123993]\n",
      "Subjectivity reward: [0.739130437374115, 0.8947368264198303, 0.75, 0.8235294222831726, 0.9473684430122375, 0.8333333134651184, 0.6000000238418579, 0.8888888955116272]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.535308301448822, 0.5269529819488525, 0.5002420544624329, 0.8355507850646973, 0.561303436756134, 0.510145902633667, 0.4259125590324402, 0.5225235223770142]\n",
      "Shaped reward: [0.6603083610534668, 0.6519529819488525, 0.6252421140670776, 0.9605507850646973, 0.6863034963607788, 0.635145902633667, 0.550912618637085, 0.6475235223770142]\n",
      "Norm reward: [-0.13986895978450775, -0.20888110995292664, -0.42950233817100525, 2.340015172958374, 0.07484060525894165, -0.34770095348358154, -1.043434739112854, -0.2454666942358017]\n",
      "----------------------\n",
      "CE: 0.0055, RL: 0.0013, KL: 0.0000, TOTAL: 0.0038\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: Cracked my first rotten egg ever last night. That was a delight.\n",
      "[0] OUT: Cracked my first rotten egg ever last night. That was a delight.\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.30224794149398804, 0.19870996475219727, 0.2715625762939453, 0.21750624477863312, 0.1426098495721817, 0.2589838206768036, 0.19164805114269257, 0.16606245934963226]\n",
      "Subjectivity reward: [0.8333333134651184, 0.7333333492279053, 0.8125, 0.800000011920929, 0.9444444179534912, 0.7777777910232544, 0.75, 0.9333333373069763]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.5677906274795532, 0.46602165699005127, 0.5420312881469727, 0.5087531208992004, 0.5435271263122559, 0.5183808207511902, 0.4708240330219269, 0.5496978759765625]\n",
      "Shaped reward: [0.6927906274795532, 0.5910216569900513, 0.6670312881469727, 0.6337531805038452, 0.6685271263122559, 0.643380880355835, 0.5958240628242493, 0.6746978759765625]\n",
      "Norm reward: [1.2625176906585693, -1.476318120956421, 0.56927490234375, -0.32631510496139526, 0.6095313429832458, -0.06721168011426926, -1.3470743894577026, 0.7756003141403198]\n",
      "----------------------\n",
      "CE: 0.0082, RL: -0.0005, KL: 0.0000, TOTAL: 0.0047\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: Stuff that makes my morning better: being the only one in my 8am class\n",
      "[0] OUT: Stuff that makes my morning better: being the only one in my 8am class\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.33261221647262573, 0.24963775277137756, 0.28115183115005493, 0.9110008478164673, 0.3698975443840027, 0.21062462031841278, 0.1808067262172699, 0.30275478959083557]\n",
      "Subjectivity reward: [0.9285714030265808, 0.8571428656578064, 0.800000011920929, 0.875, 0.8888888955116272, 0.9285714030265808, 0.8421052694320679, 0.800000011920929]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.6305918097496033, 0.5533903241157532, 0.5405759215354919, 0.8930004239082336, 0.6293932199478149, 0.5695980191230774, 0.5114560127258301, 0.5513774156570435]\n",
      "Shaped reward: [0.755591869354248, 0.678390383720398, 0.6655759811401367, 1.0180004835128784, 0.7543932199478149, 0.6945980787277222, 0.6364560127258301, 0.6763774156570435]\n",
      "Norm reward: [0.19798767566680908, -0.46892374753952026, -0.5796220302581787, 2.3093276023864746, 0.18763303756713867, -0.32891222834587097, -0.831177294254303, -0.4863129258155823]\n",
      "----------------------\n",
      "CE: 0.0143, RL: -0.0019, KL: 0.0000, TOTAL: 0.0078\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: absolutely love you 2 dorks shilling for fantasy football.\n",
      "[0] OUT: absolutely love you 2 dorks shilling for fantasy football.\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.15733376145362854, 0.323034405708313, 0.42867815494537354, 0.2252550721168518, 0.2340361624956131, 0.17980749905109406, 0.1721380203962326, 0.18666301667690277]\n",
      "Subjectivity reward: [0.6666666269302368, 0.8421052694320679, 0.3333333134651184, 0.75, 1.0, 0.6666666269302368, 1.0, 0.6875]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.4120001792907715, 0.5825698375701904, 0.38100573420524597, 0.4876275360584259, 0.6170181035995483, 0.42323705554008484, 0.5860689878463745, 0.437081515789032]\n",
      "Shaped reward: [0.5370001792907715, 0.7075698375701904, 0.5060057640075684, 0.6126275658607483, 0.7420181035995483, 0.5482370853424072, 0.7110689878463745, 0.5620815753936768]\n",
      "Norm reward: [-0.8573942184448242, 0.9979013800621033, -1.1945221424102783, -0.03479095920920372, 1.372597098350525, -0.7351697683334351, 1.0359618663787842, -0.584582507610321]\n",
      "----------------------\n",
      "CE: 0.0156, RL: -0.0007, KL: 0.0000, TOTAL: 0.0091\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: Meeting a foreigner and speaking with him is easier than meeting a Polish guy with whom you've been writing for a couple of years ?\n",
      "[0] OUT: Meeting a foreigner and speaking with him is easier than meeting a Polish guy with whom you've been writing for a couple of years .\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.37621283531188965, 0.5791298747062683, 0.3869956433773041, 0.22229023277759552, 0.7933840155601501, 0.1632530391216278, 0.80292809009552, 0.3135738670825958]\n",
      "Subjectivity reward: [0.9599999785423279, 0.8571428656578064, 1.0, 0.8999999761581421, 0.8571428656578064, 0.615384578704834, 0.8999999761581421, 0.8571428656578064]\n",
      "Similarity: [0.9523809552192688]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.0023809524718672037]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.6681064367294312, 0.7181363701820374, 0.6934978365898132, 0.5611451268196106, 0.8252634406089783, 0.3893188238143921, 0.851464033126831, 0.5853583812713623]\n",
      "Shaped reward: [0.8026302456855774, 0.8526601791381836, 0.8280216455459595, 0.6956689357757568, 0.9597872495651245, 0.5238426327705383, 0.9859878420829773, 0.7198821902275085]\n",
      "Norm reward: [0.04384951665997505, 0.37775132060050964, 0.21331274509429932, -0.6700145602226257, 1.092721700668335, -1.816790223121643, 1.2675855159759521, -0.5084143280982971]\n",
      "----------------------\n",
      "CE: 0.0239, RL: -0.0007, KL: 0.0000, TOTAL: 0.0141\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: Another beautiful summers day!!  bring on las vegas/carribean! 3 weeks and counting \n",
      "[0] OUT: Another beautiful summers day!!  bring on las vegas/carribean! 3 weeks and counting \n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.2704084813594818, 0.41757139563560486, 0.3716430962085724, 0.45363280177116394, 0.20462948083877563, 0.29030025005340576, 0.2971455752849579, 0.2535966634750366]\n",
      "Subjectivity reward: [0.9166666865348816, 0.7058823108673096, 0.6000000238418579, 1.0, 0.7857142686843872, 0.8571428656578064, 0.875, 0.6363636255264282]\n",
      "Similarity: [0.9230769276618958]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.0]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.5935375690460205, 0.5617268681526184, 0.48582154512405396, 0.7268164157867432, 0.4951718747615814, 0.5737215280532837, 0.5860728025436401, 0.4449801445007324]\n",
      "Shaped reward: [0.7204606533050537, 0.6886499524116516, 0.6127446293830872, 0.8537395000457764, 0.622094988822937, 0.7006446123123169, 0.7129958868026733, 0.5719032287597656]\n",
      "Norm reward: [0.4038296043872833, 0.03738934174180031, -0.8369945883750916, 1.939122200012207, -0.7292840480804443, 0.17556065320968628, 0.31783995032310486, -1.3074631690979004]\n",
      "----------------------\n",
      "CE: 0.0084, RL: -0.0000, KL: 0.0000, TOTAL: 0.0050\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: You worship Hanuman and call it monkeygod but ridicule someone by calling him monkey/bandar\n",
      "[0] OUT: You worship Hanuman and call it monkeygod but ridicule someone by calling him monkey/bar\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.17282108962535858, 0.6953880190849304, 0.4638252556324005, 0.26852113008499146, 0.1680436134338379, 0.856593132019043, 0.22979851067066193, 0.5396736264228821]\n",
      "Subjectivity reward: [0.8571428656578064, 0.8333333134651184, 0.9444444179534912, 0.800000011920929, 0.8999999761581421, 1.0, 1.0, 0.949999988079071]\n",
      "Similarity: [0.9285714030265808]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.0]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.5149819850921631, 0.7643606662750244, 0.7041348218917847, 0.5342605710029602, 0.53402179479599, 0.9282965660095215, 0.6148992776870728, 0.7448368072509766]\n",
      "Shaped reward: [0.6435533761978149, 0.8929320573806763, 0.8327062129974365, 0.6628320217132568, 0.6625932455062866, 1.0568679571151733, 0.7434706687927246, 0.8734081983566284]\n",
      "Norm reward: [-1.112702488899231, 0.7959328889846802, 0.33499062061309814, -0.9651521444320679, -0.9669796228408813, 1.6153841018676758, -0.34797921776771545, 0.6465058326721191]\n",
      "----------------------\n",
      "CE: 0.0121, RL: -0.0001, KL: 0.0000, TOTAL: 0.0073\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: Oh look, another caution. Lets bring Tony in for 4 tires Chad!\n",
      "[0] OUT: Oh look, another caution. Lets bring Tony in for 4 tires Chad!\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.365283340215683, 0.4870247542858124, 0.3346186876296997, 0.756247341632843, 0.287899911403656, 0.7815226912498474, 0.2466036081314087, 0.24682946503162384]\n",
      "Subjectivity reward: [0.8333333134651184, 0.9285714030265808, 0.8181818127632141, 1.0, 0.5714285373687744, 0.6470588445663452, 0.875, 0.9166666865348816]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.5993083119392395, 0.7077980637550354, 0.5764002799987793, 0.8781236410140991, 0.4296642243862152, 0.7142907381057739, 0.5608018040657043, 0.5817480683326721]\n",
      "Shaped reward: [0.7243083715438843, 0.8327981233596802, 0.7014002799987793, 1.0031236410140991, 0.5546642541885376, 0.8392907381057739, 0.6858018636703491, 0.7067481279373169]\n",
      "Norm reward: [-0.235056072473526, 0.5792078971862793, -0.40699151158332825, 1.8341325521469116, -1.5083107948303223, 0.6279378533363342, -0.5240646004676819, -0.3668535351753235]\n",
      "----------------------\n",
      "CE: 0.0076, RL: 0.0008, KL: 0.0000, TOTAL: 0.0049\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: Had to love Aaron's comment about God being on the Packers side tonight.\n",
      "[0] OUT: Had to love Aaron's comment about God being on the Packers side tonight.\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.4422673285007477, 0.2951265573501587, 0.15861409902572632, 0.2501393258571625, 0.14814862608909607, 0.43362000584602356, 0.894845724105835, 0.19251693785190582]\n",
      "Subjectivity reward: [0.8461538553237915, 0.8999999761581421, 0.800000011920929, 1.0, 1.0, 1.0, 0.9375, 0.8888888955116272]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.6442105770111084, 0.5975632667541504, 0.47930705547332764, 0.6250696778297424, 0.5740743279457092, 0.7168099880218506, 0.9161728620529175, 0.5407029390335083]\n",
      "Shaped reward: [0.7692105770111084, 0.7225632667541504, 0.6043070554733276, 0.7500697374343872, 0.699074387550354, 0.8418099880218506, 1.0411728620529175, 0.6657029390335083]\n",
      "Norm reward: [0.10420268774032593, -0.28101351857185364, -1.257580280303955, -0.05386348441243172, -0.47498607635498047, 0.7037328481674194, 2.010075330734253, -0.7505694627761841]\n",
      "----------------------\n",
      "CE: 0.0152, RL: -0.0002, KL: 0.0000, TOTAL: 0.0090\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: It's odd that the experience on the iPhone is a big cut above the experience. Apple and Microsoft being my faves?\n",
      "[0] OUT: It's odd that the experience on the iPhone is a big cut above the experience. Apple and Microsoft being my faves?\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.1813587099313736, 0.2316228747367859, 0.750664472579956, 0.2609688937664032, 0.5464146733283997, 0.18055382370948792, 0.3993719518184662, 0.28105729818344116]\n",
      "Subjectivity reward: [0.8095238208770752, 0.8333333134651184, 0.9411764740943909, 0.9333333373069763, 0.9411764740943909, 0.8636363744735718, 0.3333333134651184, 0.875]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.4954412579536438, 0.5324780941009521, 0.8459204435348511, 0.5971511006355286, 0.7437955737113953, 0.5220950841903687, 0.3663526177406311, 0.578028678894043]\n",
      "Shaped reward: [0.6204413175582886, 0.6574780941009521, 0.9709204435348511, 0.7221511602401733, 0.86879563331604, 0.6470950841903687, 0.4913526475429535, 0.703028678894043]\n",
      "Norm reward: [-0.6008721590042114, -0.35282015800476074, 1.7464442253112793, 0.0803244560956955, 1.0624685287475586, -0.4223598539829254, -1.4654370546340942, -0.0477474108338356]\n",
      "----------------------\n",
      "CE: 0.0092, RL: 0.0001, KL: 0.0000, TOTAL: 0.0055\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: Did you just jinx home field?\n",
      "[0] OUT: Did you just jinx home field?\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.20454731583595276, 0.1373278647661209, 0.3886108100414276, 0.1577141135931015, 0.2785267233848572, 0.18321184813976288, 0.2537716031074524, 0.4442952573299408]\n",
      "Subjectivity reward: [0.8333333134651184, 0.8571428656578064, 0.7647058963775635, 0.8333333134651184, 0.8181818127632141, 0.9230769276618958, 0.9047619104385376, 0.8571428656578064]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.5189403295516968, 0.49723535776138306, 0.5766583681106567, 0.49552372097969055, 0.5483542680740356, 0.5531443953514099, 0.5792667865753174, 0.6507190465927124]\n",
      "Shaped reward: [0.6439403295516968, 0.6222354173660278, 0.7016583681106567, 0.6205237507820129, 0.6733542680740356, 0.6781444549560547, 0.7042667865753174, 0.7757190465927124]\n",
      "Norm reward: [-0.6542146801948547, -1.0775798559188843, 0.47160452604293823, -1.1109668016433716, -0.08048109710216522, 0.012953894212841988, 0.5224830508232117, 1.9161951541900635]\n",
      "----------------------\n",
      "CE: 0.0210, RL: -0.0004, KL: 0.0000, TOTAL: 0.0124\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: peace and quiet on my birthday stellamcartney by...\n",
      "[0] OUT: peace and quiet on my birthday stellamcartney by...\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.31555691361427307, 0.7447454929351807, 0.36522579193115234, 0.9014849662780762, 0.5644702911376953, 0.317287802696228, 0.27140921354293823, 0.947161853313446]\n",
      "Subjectivity reward: [0.75, 0.9333333373069763, 1.0, 1.0, 0.5714285373687744, 1.0, 1.0, 1.0]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.5327784419059753, 0.8390394449234009, 0.6826128959655762, 0.9507424831390381, 0.5679494142532349, 0.658643901348114, 0.6357046365737915, 0.9735809564590454]\n",
      "Shaped reward: [0.6577785015106201, 0.9640394449234009, 0.8076128959655762, 1.075742483139038, 0.6929494142532349, 0.7836439609527588, 0.7607046365737915, 1.0985809564590454]\n",
      "Norm reward: [-1.280463457107544, 0.9532463550567627, -0.1876484453678131, 1.2155241966247559, -1.0239448547363281, -0.3624655306339264, -0.5297731757164001, 1.2155241966247559]\n",
      "----------------------\n",
      "CE: 0.0094, RL: 0.0010, KL: 0.0000, TOTAL: 0.0060\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: Remember Davis was a democrat. Oh the\n",
      "[0] OUT: Remember Davis was a democrat. Oh,\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.5163465142250061, 0.1781705766916275, 0.13009299337863922, 0.875569224357605, 0.5414959192276001, 0.40754979848861694, 0.18977423012256622, 0.26952818036079407]\n",
      "Subjectivity reward: [0.8333333134651184, 0.75, 1.0, 0.9285714030265808, 0.8999999761581421, 0.7777777910232544, 0.8999999761581421, 0.9166666865348816]\n",
      "Similarity: [0.7692307829856873]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.0]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.6748399138450623, 0.46408528089523315, 0.565046489238739, 0.9020702838897705, 0.7207479476928711, 0.5926637649536133, 0.544887125492096, 0.593097448348999]\n",
      "Shaped reward: [0.7556091547012329, 0.5448545217514038, 0.6458157300949097, 0.9828395247459412, 0.8015171885490417, 0.6734330058097839, 0.6256563663482666, 0.6738666892051697]\n",
      "Norm reward: [0.3179754912853241, -1.252925157546997, -0.5003911852836609, 2.011681318283081, 0.6601599454879761, -0.2945404350757599, -0.6506529450416565, -0.29130789637565613]\n",
      "----------------------\n",
      "CE: 0.0365, RL: 0.0004, KL: 0.0000, TOTAL: 0.0220\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: It always nice to get compared to white slave owner when you recommend owning a stock. You can't make this shit up! stocks\n",
      "[0] OUT: It always nice to get compared to white slave owner when you recommend owning a stock. You can't make this shit up! stocks\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.14268513023853302, 0.20997416973114014, 0.16270814836025238, 0.334332138299942, 0.15273365378379822, 0.279669851064682, 0.44022271037101746, 0.9210245013237]\n",
      "Subjectivity reward: [0.8695652484893799, 0.8571428656578064, 1.0, 1.0, 0.8235294222831726, 0.5714285373687744, 1.0, 1.0]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.5061252117156982, 0.5335584878921509, 0.5813540816307068, 0.6671660542488098, 0.4881315231323242, 0.4255492091178894, 0.7201113700866699, 0.9605122804641724]\n",
      "Shaped reward: [0.6311252117156982, 0.6585584878921509, 0.7063541412353516, 0.7921661138534546, 0.6131315231323242, 0.5505492687225342, 0.8451113700866699, 1.0855122804641724]\n",
      "Norm reward: [-0.6363827586174011, -0.44966408610343933, -0.1243533343076706, 0.45970726013183594, -0.7588528990745544, -1.1848053932189941, 0.8200676441192627, 1.8742835521697998]\n",
      "----------------------\n",
      "CE: 0.0134, RL: -0.0016, KL: 0.0000, TOTAL: 0.0074\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: that new tv/online commercial is pretty funny\n",
      "[0] OUT: that new tv/online commercial is pretty funny\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.2726835012435913, 0.8356537222862244, 0.3814537823200226, 0.6386853456497192, 0.19935721158981323, 0.1890101432800293, 0.9064396619796753, 0.15104182064533234]\n",
      "Subjectivity reward: [0.7142857313156128, 1.0, 1.0, 1.0, 0.95652174949646, 0.8999999761581421, 0.8888888955116272, 0.615384578704834]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.49348461627960205, 0.9178268909454346, 0.6907268762588501, 0.8193426728248596, 0.577939510345459, 0.5445050597190857, 0.8976643085479736, 0.38321319222450256]\n",
      "Shaped reward: [0.618484616279602, 1.0428268909454346, 0.8157268762588501, 0.9443427324295044, 0.702939510345459, 0.6695051193237305, 1.0226643085479736, 0.508213222026825]\n",
      "Norm reward: [-0.8794679045677185, 1.1674866676330566, 0.17880119383335114, 0.8688672184944153, -0.4263398349285126, -0.6057262420654297, 1.1674866676330566, -1.4711099863052368]\n",
      "----------------------\n",
      "CE: 0.0063, RL: 0.0011, KL: 0.0000, TOTAL: 0.0042\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: My luck with phones is fucking PHENOMENAL\n",
      "[0] OUT: My luck with phones is fucking PHENOMENAL\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.1796073317527771, 0.23912113904953003, 0.17833395302295685, 0.26294535398483276, 0.4143461585044861, 0.4473913311958313, 0.7729067802429199, 0.22541503608226776]\n",
      "Subjectivity reward: [0.7142857313156128, 0.800000011920929, 0.800000011920929, 0.8333333134651184, 0.8947368264198303, 0.9047619104385376, 0.9333333373069763, 0.3333333134651184]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.44694653153419495, 0.5195605754852295, 0.4891669750213623, 0.5481393337249756, 0.6545414924621582, 0.6760766506195068, 0.8531200885772705, 0.2793741822242737]\n",
      "Shaped reward: [0.5719465613365173, 0.6445605754852295, 0.6141669750213623, 0.6731393337249756, 0.7795414924621582, 0.8010766506195068, 0.9781200885772705, 0.4043741822242737]\n",
      "Norm reward: [-0.648457407951355, -0.22584514319896698, -0.4027353525161743, -0.059517282992601395, 0.5597413778305054, 0.6850756406784058, 1.7154653072357178, -1.6237256526947021]\n",
      "----------------------\n",
      "CE: 0.0187, RL: -0.0065, KL: 0.0000, TOTAL: 0.0086\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: Of course the catch rule comes to question when the Cowboys are playing smh\n",
      "[0] OUT: Of course the catch rule comes to question when the Cowboys are playing smh\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.20814889669418335, 0.6995782852172852, 0.37109827995300293, 0.17088305950164795, 0.3559889793395996, 0.7362385392189026, 0.5315358638763428, 0.2179013192653656]\n",
      "Subjectivity reward: [1.0, 1.0, 1.0, 0.7894736528396606, 1.0, 0.9166666865348816, 1.0, 0.8333333134651184]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.6040744781494141, 0.8497891426086426, 0.6855491399765015, 0.4801783561706543, 0.6779944896697998, 0.8264526128768921, 0.7657679319381714, 0.5256173014640808]\n",
      "Shaped reward: [0.7290744781494141, 0.9747891426086426, 0.8105491399765015, 0.6051783561706543, 0.8029944896697998, 0.9514526128768921, 0.8907679319381714, 0.6506173610687256]\n",
      "Norm reward: [-0.5405809283256531, 1.282650113105774, 0.0639704167842865, -1.459904432296753, 0.007914039306342602, 1.1094903945922852, 0.6592031121253967, -1.1227418184280396]\n",
      "----------------------\n",
      "CE: 0.0085, RL: -0.0033, KL: 0.0000, TOTAL: 0.0038\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: i didn't sleep currently a zombie at work i'mreallyattractivewithshoppingbagsundermyeyes\n",
      "[0] OUT: i didn't sleep currently a zombie at work i'mreallyattractivewithshoppingbagsundermyeyes\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.25977590680122375, 0.2061275839805603, 0.1896413266658783, 0.19224973022937775, 0.22053611278533936, 0.18341557681560516, 0.9436513185501099, 0.26715993881225586]\n",
      "Subjectivity reward: [1.0, 0.8333333134651184, 0.8461538553237915, 0.8235294222831726, 0.875, 1.0, 0.8571428656578064, 0.7142857313156128]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.6298879384994507, 0.5197304487228394, 0.5178976058959961, 0.5078895688056946, 0.5477680563926697, 0.5917077660560608, 0.9003970623016357, 0.4907228350639343]\n",
      "Shaped reward: [0.7548879384994507, 0.6447304487228394, 0.6428976058959961, 0.6328896284103394, 0.6727681159973145, 0.7167078256607056, 1.0253970623016357, 0.6157228946685791]\n",
      "Norm reward: [0.3556077778339386, -0.5185447931289673, -0.533089280128479, -0.6125074028968811, -0.2960524559020996, 0.0526302345097065, 2.300689697265625, -0.7487336993217468]\n",
      "----------------------\n",
      "CE: 0.0067, RL: -0.0016, KL: 0.0000, TOTAL: 0.0034\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: MFW we're going shopping meme humor\n",
      "[0] OUT: MFW we're going shopping meme humor\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.1865701824426651, 0.2978322207927704, 0.9371225833892822, 0.6393439173698425, 0.16440406441688538, 0.2381129115819931, 0.2229960709810257, 0.181051105260849]\n",
      "Subjectivity reward: [0.8333333134651184, 0.7777777910232544, 1.0, 0.6666666269302368, 0.75, 0.8999999761581421, 0.9166666865348816, 0.8947368264198303]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.5099517703056335, 0.5378050208091736, 0.9685612916946411, 0.6530052423477173, 0.4572020173072815, 0.5690564513206482, 0.569831371307373, 0.5378939509391785]\n",
      "Shaped reward: [0.6349518299102783, 0.6628050804138184, 1.0935612916946411, 0.7780052423477173, 0.5822020769119263, 0.694056510925293, 0.694831371307373, 0.6628940105438232]\n",
      "Norm reward: [-0.6131237149238586, -0.39631205797195435, 2.2284369468688965, 0.5004141330718994, -1.0237315893173218, -0.15304872393608093, -0.14701715111732483, -0.39561983942985535]\n",
      "----------------------\n",
      "CE: 0.0121, RL: -0.0003, KL: 0.0000, TOTAL: 0.0072\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: oh that's ok then my mistake,that proves it Islam truly is peaceful\n",
      "[0] OUT: oh that's ok then my mistake,that proves it Islam truly is peaceful\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.25006693601608276, 0.49246492981910706, 0.22473272681236267, 0.20113423466682434, 0.6765137910842896, 0.3182271122932434, 0.28516966104507446, 0.6019907593727112]\n",
      "Subjectivity reward: [0.75, 0.8461538553237915, 0.9375, 0.8333333134651184, 0.9166666865348816, 1.0, 1.0, 0.7368420958518982]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.5000334978103638, 0.6693093776702881, 0.5811163783073425, 0.5172337889671326, 0.7965902090072632, 0.6591135263442993, 0.6425848007202148, 0.6694164276123047]\n",
      "Shaped reward: [0.6250334978103638, 0.7943093776702881, 0.7061164379119873, 0.6422338485717773, 0.9215902090072632, 0.7841135263442993, 0.7675848007202148, 0.7944164276123047]\n",
      "Norm reward: [-1.3546355962753296, 0.4175640344619751, -0.5057541131973267, -1.1745599508285522, 1.7501050233840942, 0.31082063913345337, 0.1377764791250229, 0.41868478059768677]\n",
      "----------------------\n",
      "CE: 0.0131, RL: 0.0002, KL: 0.0000, TOTAL: 0.0079\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: clearly. And Mark Zuckerberg is a Jew. much ?\n",
      "[0] OUT: clearly. And Mark Zuckerberg is a Jew. much ?\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.16390390694141388, 0.1807756870985031, 0.281172513961792, 0.7366399168968201, 0.3175524175167084, 0.9434249401092529, 0.13710440695285797, 0.4443609118461609]\n",
      "Subjectivity reward: [0.7777777910232544, 0.7777777910232544, 0.8888888955116272, 0.9444444179534912, 1.0, 1.0, 0.7777777910232544, 0.8260869383811951]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.47084084153175354, 0.47927674651145935, 0.5850306749343872, 0.840542197227478, 0.6587762236595154, 0.9717124700546265, 0.4574410915374756, 0.635223925113678]\n",
      "Shaped reward: [0.5958408713340759, 0.6042767763137817, 0.7100306749343872, 0.965542197227478, 0.7837762832641602, 1.0967124700546265, 0.5824410915374756, 0.7602239847183228]\n",
      "Norm reward: [-0.9501673579216003, -0.8982619643211365, -0.24756747484207153, 1.3245725631713867, 0.2061828076839447, 1.536588430404663, -1.032615065574646, 0.06126758083701134]\n",
      "----------------------\n",
      "CE: 0.0046, RL: -0.0011, KL: 0.0000, TOTAL: 0.0023\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: rumors of a rotator cuff injury to Brees. That would make the Luke McCown commercials so much better.\n",
      "[0] OUT: rumors of a rotator cuff injury to Brees. That would make the Luke McCown commercials so much better.\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.5143753886222839, 0.6325017809867859, 0.42839452624320984, 0.2576504051685333, 0.21110309660434723, 0.16764865815639496, 0.2546573877334595, 0.26869046688079834]\n",
      "Subjectivity reward: [0.6666666269302368, 0.9090908765792847, 0.8888888955116272, 0.8125, 0.8636363744735718, 0.7777777910232544, 0.875, 1.0]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.590520977973938, 0.7707962989807129, 0.6586416959762573, 0.5350751876831055, 0.5373697280883789, 0.4727132320404053, 0.5648286938667297, 0.6343452334403992]\n",
      "Shaped reward: [0.715520977973938, 0.8957962989807129, 0.7836416959762573, 0.6600751876831055, 0.6623697280883789, 0.5977132320404053, 0.6898287534713745, 0.759345293045044]\n",
      "Norm reward: [-0.05448713153600693, 1.9040048122406006, 0.6855691075325012, -0.6568443179130554, -0.6319167017936707, -1.3343379497528076, -0.33360472321510315, 0.4216155707836151]\n",
      "----------------------\n",
      "CE: 0.0136, RL: -0.0007, KL: 0.0000, TOTAL: 0.0079\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: prescription drugs - Signs and risks or abuse and addiction:\n",
      "[0] OUT: prescription drugs - Signs and risks or abuse and addiction:\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.7610945701599121, 0.46972331404685974, 0.20782199501991272, 0.38640424609184265, 0.8859050273895264, 0.32512614130973816, 0.19122768938541412, 0.564130425453186]\n",
      "Subjectivity reward: [0.800000011920929, 0.9090908765792847, 0.9047619104385376, 0.9333333373069763, 0.875, 0.9285714030265808, 0.8571428656578064, 0.8571428656578064]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.7805472612380981, 0.6894071102142334, 0.556291937828064, 0.6598687767982483, 0.8804525136947632, 0.6268487572669983, 0.524185299873352, 0.7106366157531738]\n",
      "Shaped reward: [0.9055472612380981, 0.8144071102142334, 0.681291937828064, 0.7848688364028931, 1.0054525136947632, 0.7518488168716431, 0.649185299873352, 0.8356366157531738]\n",
      "Norm reward: [0.8960500955581665, 0.10085142403841019, -1.0605796575546265, -0.1568702906370163, 1.7201511859893799, -0.4449702799320221, -1.3407104015350342, 0.286079078912735]\n",
      "----------------------\n",
      "CE: 0.0210, RL: 0.0012, KL: 0.0000, TOTAL: 0.0131\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: \"Conservative\" talk radio owns the trump \"phenomenon\". When he loses, they will lose all credibility. gopdebate gop2016\n",
      "[0] OUT: \"Conservative\" talk radio owns the trump \"phenomenon\". When he loses, they will lose all credibility. gopdebate gop2016\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.2497042864561081, 0.17453156411647797, 0.26700809597969055, 0.2189306914806366, 0.2661397457122803, 0.2903491258621216, 0.2064255028963089, 0.3631637394428253]\n",
      "Subjectivity reward: [0.8235294222831726, 0.6399999856948853, 0.7777777910232544, 0.9230769276618958, 0.875, 0.625, 0.5555555820465088, 0.75]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.536616861820221, 0.4072657823562622, 0.5223929286003113, 0.571003794670105, 0.5705698728561401, 0.4576745629310608, 0.38099053502082825, 0.5565818548202515]\n",
      "Shaped reward: [0.6616169214248657, 0.5322657823562622, 0.647392988204956, 0.696003794670105, 0.6955698728561401, 0.5826746225357056, 0.5059905648231506, 0.6815818548202515]\n",
      "Norm reward: [0.48151376843452454, -1.2376309633255005, 0.29247018694877625, 0.938533365726471, 0.932766318321228, -0.5676709413528442, -1.58684241771698, 0.7468582391738892]\n",
      "----------------------\n",
      "CE: 0.0054, RL: -0.0005, KL: 0.0000, TOTAL: 0.0030\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: Head lines autocorrect copywriting proofreading saudiarabia subediting typo un\n",
      "[0] OUT: Head lines autocorrect copywriting proofreading saudiarabia subediting typo un\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.3728583753108978, 0.23031537234783173, 0.19679056107997894, 0.24133390188217163, 0.2264287918806076, 0.33890947699546814, 0.18944808840751648, 0.24463461339473724]\n",
      "Subjectivity reward: [1.0, 0.8181818127632141, 0.8125, 0.8888888955116272, 0.7058823108673096, 0.7333333492279053, 0.7692307829856873, 0.9375]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.6864292025566101, 0.5242486000061035, 0.5046452879905701, 0.5651113986968994, 0.4661555588245392, 0.5361214280128479, 0.4793394207954407, 0.5910673141479492]\n",
      "Shaped reward: [0.8114292621612549, 0.6492486000061035, 0.6296453475952148, 0.6901113986968994, 0.5911555886268616, 0.6611214876174927, 0.6043394804000854, 0.7160673141479492]\n",
      "Norm reward: [2.006591796875, -0.280509352684021, -0.5569579601287842, 0.29574525356292725, -1.0997475385665894, -0.11307573318481445, -0.9138259291648865, 0.6617802977561951]\n",
      "----------------------\n",
      "CE: 0.0090, RL: 0.0015, KL: 0.0000, TOTAL: 0.0060\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: how dare they attempt to wait and make an informed decision when reporters have deadlines to meet.\n",
      "[0] OUT: how dare they attempt to wait and make an informed decision when reporters have deadlines to meet.\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.17525967955589294, 0.2828689515590668, 0.15914177894592285, 0.27974721789360046, 0.19745947420597076, 0.3337875306606293, 0.34254926443099976, 0.17855186760425568]\n",
      "Subjectivity reward: [0.9411764740943909, 0.75, 0.7647058963775635, 0.9375, 0.8333333134651184, 0.8888888955116272, 1.0, 0.8181818127632141]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.5582180619239807, 0.5164344906806946, 0.46192383766174316, 0.6086236238479614, 0.5153964161872864, 0.611338198184967, 0.6712746620178223, 0.4983668327331543]\n",
      "Shaped reward: [0.6832181215286255, 0.6414345502853394, 0.5869238376617432, 0.7336236238479614, 0.6403964757919312, 0.7363382577896118, 0.7962746620178223, 0.6233668327331543]\n",
      "Norm reward: [0.04302315041422844, -0.5520196557044983, -1.3283106088638306, 0.7608515024185181, -0.566802978515625, 0.7995108366012573, 1.6530694961547852, -0.8093233108520508]\n",
      "----------------------\n",
      "CE: 0.0046, RL: 0.0003, KL: 0.0000, TOTAL: 0.0029\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: Still lolling that I can't remember that persons name that had a go at me for not remembering them iconic claimtolame\n",
      "[0] OUT: Still lolling that I can't remember that persons name that had a go at me for not remembering them iconic claimtolame\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.16867244243621826, 0.23868651688098907, 0.2305351346731186, 0.32547152042388916, 0.1569635272026062, 0.16343744099140167, 0.3786180913448334, 0.1643495112657547]\n",
      "Subjectivity reward: [0.9523809552192688, 0.5714285373687744, 0.875, 0.8333333134651184, 0.75, 0.75, 0.800000011920929, 0.9473684430122375]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.5605267286300659, 0.40505751967430115, 0.5527675747871399, 0.5794024467468262, 0.4534817636013031, 0.45671871304512024, 0.58930903673172, 0.5558589696884155]\n",
      "Shaped reward: [0.6855267286300659, 0.5300575494766235, 0.6777676343917847, 0.7044024467468262, 0.5784817934036255, 0.5817187428474426, 0.7143090963363647, 0.6808589696884155]\n",
      "Norm reward: [0.594018280506134, -1.6374300718307495, 0.48265206813812256, 0.864941418170929, -0.9423971176147461, -0.8959372043609619, 1.0071314573287964, 0.5270220041275024]\n",
      "----------------------\n",
      "CE: 0.0098, RL: -0.0005, KL: 0.0000, TOTAL: 0.0057\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: AWWW FUCK CELTIC HAVE ONLY GOT THREE STRIKERS !! FUCCCCCKK AW NAW ! SOMEBODY HELP US !!\n",
      "[0] OUT: AWWW FUCK CELTIC HAVE ONLY GOT THREE STRIKERS !! FUCCCCCKK AW NAW!! SOMEBODY HELP US !!\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.14854076504707336, 0.3882324993610382, 0.7765929102897644, 0.19280458986759186, 0.20628923177719116, 0.15203848481178284, 0.3163056969642639, 0.2618192434310913]\n",
      "Subjectivity reward: [0.9375, 1.0, 0.9444444179534912, 0.8125, 0.7894736528396606, 1.0, 0.7142857313156128, 0.7142857313156128]\n",
      "Similarity: [0.9032257795333862]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.0]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.5430203676223755, 0.6941162347793579, 0.8605186939239502, 0.5026522874832153, 0.4978814423084259, 0.5760192275047302, 0.5152957439422607, 0.48805248737335205]\n",
      "Shaped reward: [0.6639881134033203, 0.8150839805603027, 0.981486439704895, 0.6236200332641602, 0.6188491582870483, 0.696986973285675, 0.6362634897232056, 0.6090202331542969]\n",
      "Norm reward: [-0.3205632269382477, 0.84168541431427, 2.1216742992401123, -0.6310796141624451, -0.6677778363227844, -0.06673179566860199, -0.5338245630264282, -0.7433831691741943]\n",
      "----------------------\n",
      "CE: 0.0102, RL: -0.0024, KL: 0.0000, TOTAL: 0.0052\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: Guess that means tonight going to be great...\n",
      "[0] OUT: Tonight that means tonight going to be great...\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.23492664098739624, 0.9372598528862, 0.2646077573299408, 0.18074098229408264, 0.3502698540687561, 0.7649993300437927, 0.7462053894996643, 0.3869483768939972]\n",
      "Subjectivity reward: [0.875, 1.0, 0.75, 0.9473684430122375, 0.8421052694320679, 1.0, 0.9285714030265808, 0.8999999761581421]\n",
      "Similarity: [0.9333333373069763]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.0]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.5549633502960205, 0.9686299562454224, 0.5073038935661316, 0.5640547275543213, 0.5961875915527344, 0.8824996948242188, 0.8373883962631226, 0.6434741616249084]\n",
      "Shaped reward: [0.6849633455276489, 1.0986299514770508, 0.63730388879776, 0.6940547227859497, 0.7261875867843628, 1.0124996900558472, 0.967388391494751, 0.7734741568565369]\n",
      "Norm reward: [-0.8190186619758606, 1.2376099824905396, -1.1301500797271729, -0.7596681118011475, -0.5498977303504944, 1.2376099824905396, 1.0247142314910889, -0.24120056629180908]\n",
      "----------------------\n",
      "CE: 0.0085, RL: -0.0014, KL: 0.0000, TOTAL: 0.0045\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: I shared 's recent video on fb&amp;my mom told me she wants me to marry him.It cracked me up bc he's a stranger online. :p\n",
      "[0] OUT: I shared 's recent video on fb&amp;my mom told me she wants me to marry him.It cracked me up bc he's a stranger online. :p\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.46849432587623596, 0.3426735997200012, 0.4502267837524414, 0.2923462986946106, 0.7076703310012817, 0.28292104601860046, 0.5680423378944397, 0.26316750049591064]\n",
      "Subjectivity reward: [0.9599999785423279, 0.875, 0.8695652484893799, 1.0, 0.3333333134651184, 0.9230769276618958, 0.8125, 0.739130437374115]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.7142471671104431, 0.6088367700576782, 0.6598960161209106, 0.6461731195449829, 0.5205018520355225, 0.6029989719390869, 0.6902711391448975, 0.5011489391326904]\n",
      "Shaped reward: [0.8392472267150879, 0.7338367700576782, 0.7848960161209106, 0.7711731195449829, 0.6455018520355225, 0.7279989719390869, 0.8152711391448975, 0.6261489391326904]\n",
      "Norm reward: [1.2649855613708496, -0.12056656181812286, 0.5505741238594055, 0.37019556760787964, -1.2816717624664307, -0.19730062782764435, 0.949835479259491, -1.5360532999038696]\n",
      "----------------------\n",
      "CE: 0.0069, RL: 0.0001, KL: 0.0000, TOTAL: 0.0041\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: and her coolforthesummervma video are the hottest things I've seen all summer... ?\n",
      "[0] OUT: and her coolforthesummervma video are the hottest things I've seen all summer... ?\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.21777310967445374, 0.691495418548584, 0.219519704580307, 0.9081489443778992, 0.38267943263053894, 0.1894768476486206, 0.6279087066650391, 0.18048731982707977]\n",
      "Subjectivity reward: [1.0, 0.8235294222831726, 0.9090908765792847, 0.7272727489471436, 0.625, 0.75, 0.8888888955116272, 0.8333333134651184]\n",
      "Similarity: [0.9230769276618958]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.0]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.6088865399360657, 0.7575124502182007, 0.564305305480957, 0.8177108764648438, 0.5038397312164307, 0.4697384238243103, 0.7583987712860107, 0.5069103240966797]\n",
      "Shaped reward: [0.7358096241950989, 0.8844355344772339, 0.6912283897399902, 0.944633960723877, 0.6307628154754639, 0.5966615080833435, 0.885321855545044, 0.6338334083557129]\n",
      "Norm reward: [-0.10693763196468353, 0.9871947169303894, -0.43512919545173645, 1.4303545951843262, -0.8802557587623596, -1.1312977075576782, 0.9937195181846619, -0.8576511144638062]\n",
      "----------------------\n",
      "CE: 0.0082, RL: -0.0024, KL: 0.0000, TOTAL: 0.0040\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: but it's okay because they were all\n",
      "[0] OUT: but it's okay because they were all\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.4176967144012451, 0.16874445974826813, 0.732755720615387, 0.26118698716163635, 0.1781875640153885, 0.2597973048686981, 0.9228802919387817, 0.916946291923523]\n",
      "Subjectivity reward: [0.8571428656578064, 0.800000011920929, 0.9090908765792847, 0.692307710647583, 0.6666666269302368, 1.0, 1.0, 0.875]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.6374198198318481, 0.48437222838401794, 0.8209233283996582, 0.4767473340034485, 0.42242708802223206, 0.6298986673355103, 0.9614401459693909, 0.8959731459617615]\n",
      "Shaped reward: [0.7624198198318481, 0.6093722581863403, 0.9459233283996582, 0.6017473936080933, 0.5474271178245544, 0.7548986673355103, 1.0864402055740356, 1.0209732055664062]\n",
      "Norm reward: [-0.0826096460223198, -0.9087614417076111, 0.9079435467720032, -0.9499205350875854, -1.243141770362854, -0.12320888042449951, 1.1998498439788818, 1.1998498439788818]\n",
      "----------------------\n",
      "CE: 0.0367, RL: -0.0010, KL: 0.0000, TOTAL: 0.0216\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: it's pretty that the human race has spent forever developing a world based on certainty when the only thing certain is change.\n",
      "[0] OUT: it's pretty that the human race has spent forever developing a world based on certainty when the only thing certain is change.\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.16319964826107025, 0.5925695896148682, 0.22159472107887268, 0.47745460271835327, 0.3704027831554413, 0.3187691271305084, 0.262730211019516, 0.2884661555290222]\n",
      "Subjectivity reward: [0.9090908765792847, 0.9285714030265808, 0.7142857313156128, 0.8888888955116272, 1.0, 0.9090908765792847, 0.8999999761581421, 0.800000011920929]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.5361452698707581, 0.7605705261230469, 0.46794021129608154, 0.6831717491149902, 0.6852014064788818, 0.6139299869537354, 0.5813651084899902, 0.5442330837249756]\n",
      "Shaped reward: [0.6611453294754028, 0.8855705261230469, 0.5929402112960815, 0.8081717491149902, 0.8102014064788818, 0.7389299869537354, 0.7063651084899902, 0.6692330837249756]\n",
      "Norm reward: [-0.7597965598106384, 1.5784817934036255, -1.4704233407974243, 0.7720666527748108, 0.7932135462760925, 0.050639111548662186, -0.2886532247066498, -0.6755304932594299]\n",
      "----------------------\n",
      "CE: 0.0070, RL: 0.0004, KL: 0.0000, TOTAL: 0.0043\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: no need when they have ray ray\n",
      "[0] OUT: no need when they have ray ray\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.3875330686569214, 0.3944573700428009, 0.267522931098938, 0.34889861941337585, 0.14089690148830414, 0.14373649656772614, 0.8814725875854492, 0.21499940752983093]\n",
      "Subjectivity reward: [0.8571428656578064, 0.7647058963775635, 0.7368420958518982, 0.6000000238418579, 0.8823529481887817, 0.6818181872367859, 1.0, 0.6363636255264282]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.6223379373550415, 0.579581618309021, 0.5021824836730957, 0.4744493365287781, 0.5116249322891235, 0.4127773344516754, 0.9407362937927246, 0.42568153142929077]\n",
      "Shaped reward: [0.7473379373550415, 0.704581618309021, 0.6271824836730957, 0.5994493961334229, 0.6366249322891235, 0.5377773642539978, 1.0657362937927246, 0.5506815910339355]\n",
      "Norm reward: [0.4822891056537628, 0.1954236924648285, -0.3238711357116699, -0.509941041469574, -0.26051878929138184, -0.9237177968025208, 2.1774773597717285, -0.837139368057251]\n",
      "----------------------\n",
      "CE: 0.0085, RL: -0.0028, KL: 0.0000, TOTAL: 0.0040\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: I believe having the right beliefs gets you into heaven so therefore what we do doesn't matter. freeheavenpass\n",
      "[0] OUT: I believe having the right beliefs gets you into heaven so therefore what we do doesn't matter. freeheavenpass\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.2356545329093933, 0.9056758880615234, 0.3108324110507965, 0.2362365871667862, 0.22977863252162933, 0.9159656763076782, 0.39358705282211304, 0.18920549750328064]\n",
      "Subjectivity reward: [0.6111111044883728, 0.8571428656578064, 0.7647058963775635, 0.9090908765792847, 0.8999999761581421, 0.9473684430122375, 0.7777777910232544, 0.5625]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.42338281869888306, 0.8814094066619873, 0.5377691388130188, 0.5726637244224548, 0.5648893117904663, 0.9316670894622803, 0.5856823921203613, 0.3758527636528015]\n",
      "Shaped reward: [0.5483828783035278, 1.0064094066619873, 0.6627691984176636, 0.6976637840270996, 0.6898893117904663, 1.0566670894622803, 0.7106823921203613, 0.5008528232574463]\n",
      "Norm reward: [-0.9632540941238403, 1.482102870941162, -0.34389010071754456, -0.15494750440120697, -0.19704368710517883, 1.482102870941162, -0.08445605635643005, -1.2206135988235474]\n",
      "----------------------\n",
      "CE: 0.0061, RL: 0.0001, KL: 0.0000, TOTAL: 0.0037\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: No I genuinely have low morals,allegedly.Think it's called libertarian\n",
      "[0] OUT: No I genuinely have low morals,allegedly.Think it's called libertarian\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.16969606280326843, 0.32804548740386963, 0.40710803866386414, 0.2305891066789627, 0.2178160697221756, 0.5239704251289368, 0.24775603413581848, 0.463119238615036]\n",
      "Subjectivity reward: [0.8888888955116272, 0.7777777910232544, 0.8888888955116272, 0.75, 0.5, 0.9166666865348816, 0.6428571343421936, 0.8666666746139526]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.5292924642562866, 0.552911639213562, 0.6479984521865845, 0.49029454588890076, 0.3589080274105072, 0.7203185558319092, 0.44530659914016724, 0.6648929715156555]\n",
      "Shaped reward: [0.6542924642562866, 0.677911639213562, 0.7729984521865845, 0.6152945756912231, 0.4839080274105072, 0.8453185558319092, 0.570306658744812, 0.7898930311203003]\n",
      "Norm reward: [-0.18067650496959686, 0.013757333159446716, 0.7965151071548462, -0.5017083883285522, -1.5832868814468384, 1.3918566703796387, -0.8720504641532898, 0.9355918765068054]\n",
      "----------------------\n",
      "CE: 0.0111, RL: 0.0015, KL: 0.0000, TOTAL: 0.0073\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: Stonehenge upstaged by a new Stonehenge 2 miles away? mystonesbiggerthanyourstone\n",
      "[0] OUT: Stonehenge upstaged by a new Stonehenge 2 miles away? mystonesbiggerthanyourstone\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.2242756187915802, 0.2874920666217804, 0.2275938093662262, 0.3544066250324249, 0.7543383836746216, 0.19479139149188995, 0.21865276992321014, 0.5603892803192139]\n",
      "Subjectivity reward: [1.0, 0.9166666865348816, 0.8947368264198303, 0.800000011920929, 0.9473684430122375, 0.6666666269302368, 0.699999988079071, 0.75]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.6121377944946289, 0.6020793914794922, 0.5611653327941895, 0.5772033333778381, 0.850853443145752, 0.4307290017604828, 0.4593263864517212, 0.6551946401596069]\n",
      "Shaped reward: [0.7371377944946289, 0.7270793914794922, 0.6861653327941895, 0.7022033929824829, 0.975853443145752, 0.5557290315628052, 0.5843263864517212, 0.7801946401596069]\n",
      "Norm reward: [0.14393667876720428, 0.06589654088020325, -0.2515433728694916, -0.1271088868379593, 1.9960598945617676, -1.2635595798492432, -1.0416812896728516, 0.47800183296203613]\n",
      "----------------------\n",
      "CE: 0.0086, RL: -0.0009, KL: 0.0000, TOTAL: 0.0048\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: Internet browsers list internet education inspirational lol #\n",
      "[0] OUT: Internet browsers list internet education inspirational lol #...\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.2305351346731186, 0.22888237237930298, 0.28951460123062134, 0.15992872416973114, 0.2239251583814621, 0.9388768076896667, 0.6958683729171753, 0.27596232295036316]\n",
      "Subjectivity reward: [0.875, 0.7058823108673096, 0.8888888955116272, 0.6666666269302368, 0.8571428656578064, 1.0, 0.8461538553237915, 0.9599999785423279]\n",
      "Similarity: [0.8571428656578064]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.0]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.5527675747871399, 0.4673823416233063, 0.5892017483711243, 0.4132976830005646, 0.5405340194702148, 0.9694384336471558, 0.7710111141204834, 0.6179811358451843]\n",
      "Shaped reward: [0.6599104404449463, 0.5745252370834351, 0.6963446140289307, 0.5204405784606934, 0.6476768851280212, 1.0765812397003174, 0.8781539797782898, 0.7251240015029907]\n",
      "Norm reward: [-0.33583498001098633, -0.8782959580421448, -0.10436493903398514, -1.221901297569275, -0.4135560095310211, 1.8247888088226318, 1.050688624382019, 0.07847345620393753]\n",
      "----------------------\n",
      "CE: 0.0055, RL: -0.0010, KL: 0.0000, TOTAL: 0.0029\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: Top Blogger templates blogger education inspirational lol fu\n",
      "[0] OUT: Top Blogger templates blogger education inspirational lol fu...\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.2132807970046997, 0.2850334346294403, 0.5067111849784851, 0.3419402837753296, 0.7319815158843994, 0.2327568531036377, 0.45627185702323914, 0.268421471118927]\n",
      "Subjectivity reward: [0.75, 0.8260869383811951, 0.75, 0.6666666269302368, 0.8333333134651184, 0.9375, 0.9473684430122375, 0.8235294222831726]\n",
      "Similarity: [0.8571428656578064]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.0]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.48164039850234985, 0.5555601716041565, 0.6283556222915649, 0.5043034553527832, 0.7826573848724365, 0.5851284265518188, 0.7018201351165771, 0.5459754467010498]\n",
      "Shaped reward: [0.5887832641601562, 0.6627030372619629, 0.7354984879493713, 0.6114463210105896, 0.8898002505302429, 0.6922712922096252, 0.8089630007743835, 0.6531183123588562]\n",
      "Norm reward: [-1.1427314281463623, -0.41791030764579773, 0.29588624835014343, -0.9205085635185242, 1.8088937997817993, -0.12797847390174866, 1.0162433385849, -0.5118933916091919]\n",
      "----------------------\n",
      "CE: 0.0130, RL: 0.0007, KL: 0.0000, TOTAL: 0.0081\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: peace comes from within\n",
      "[0] OUT: peace comes from within\n",
      "=====================\n",
      "\n",
      "---- REWARD DEBUG ----\n",
      "Classifier reward: [0.518125057220459, 0.18941684067249298, 0.4659055173397064, 0.14077527821063995, 0.2886196970939636, 0.15237469971179962, 0.8816906809806824, 0.21384291350841522]\n",
      "Subjectivity reward: [0.75, 1.0, 0.9130434989929199, 0.9230769276618958, 0.5, 0.7777777910232544, 0.7142857313156128, 0.800000011920929]\n",
      "Similarity: [1.0]\n",
      "Empty penalty: [0.0]\n",
      "Copy penalty: [0.05000000074505806]\n",
      "Halluc penalty: [0.0]\n",
      "Base reward: [0.6340625286102295, 0.5947084426879883, 0.6894745230674744, 0.5319260954856873, 0.3943098485469818, 0.4650762379169464, 0.7979881763458252, 0.5069214701652527]\n",
      "Shaped reward: [0.7590625286102295, 0.7197084426879883, 0.8144745826721191, 0.656926155090332, 0.5193098783493042, 0.5900762677192688, 0.9229881763458252, 0.6319215297698975]\n",
      "Norm reward: [0.44112667441368103, 0.13791441917419434, 0.8680610656738281, -0.3458055555820465, -1.4061005115509033, -0.8608652949333191, 1.7041280269622803, -0.5384592413902283]\n",
      "----------------------\n",
      "CE: 0.0163, RL: 0.0018, KL: 0.0000, TOTAL: 0.0105\n",
      "\n",
      "==== DEBUG SAMPLE ====\n",
      "[0] SRC: Friends skipped out of work to go to the farmer's market. So now I get to stay late to complete the project. Yay.\n",
      "[0] OUT: Friends skipped out of work to go to the farmer's market. So now I get to stay late to complete the project. Yay.\n",
      "=====================\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-758728629.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# @title training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2325\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2326\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2672\u001b[0m                     )\n\u001b[1;32m   2673\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2674\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2676\u001b[0m                     if (\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   4018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4019\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4020\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4022\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-587777770.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m                 \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtoks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m                 \u001b[0mclf_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1075\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1077\u001b[0;31m         outputs = self.deberta(\n\u001b[0m\u001b[1;32m   1078\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1079\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    782\u001b[0m         )\n\u001b[1;32m    783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    785\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[0mrel_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_rel_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_module\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m             output_states, attn_weights = layer_module(\n\u001b[0m\u001b[1;32m    658\u001b[0m                 \u001b[0mnext_kv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n\u001b[0;32m--> 436\u001b[0;31m         attention_output, att_matrix = self.attention(\n\u001b[0m\u001b[1;32m    437\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0mrel_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n\u001b[0;32m--> 369\u001b[0;31m         self_output, att_matrix = self.self(\n\u001b[0m\u001b[1;32m    370\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelative_attention\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mrel_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_dropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrel_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             rel_att = self.disentangled_attention_bias(\n\u001b[0m\u001b[1;32m    251\u001b[0m                 \u001b[0mquery_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelative_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_factor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             )\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\u001b[0m in \u001b[0;36mdisentangled_attention_bias\u001b[0;34m(self, query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0mrelative_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelative_pos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mrelative_pos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m             \u001b[0mrelative_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelative_pos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m         \u001b[0;31m# bsz x height x query x key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mrelative_pos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# @title training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKa_NSFO_c2z"
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 937
    },
    "executionInfo": {
     "elapsed": 682,
     "status": "ok",
     "timestamp": 1763352497632,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "SvoI3_60e1MP",
    "outputId": "6b3fe1c9-1feb-4258-a12b-bafb85fabb1b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"print(df_tw[\\\"class\\\"]\",\n  \"rows\": 25,\n  \"fields\": [\n    {\n      \"column\": \"tweets\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 25,\n        \"samples\": [\n          \"Rahm has solved crime, budget &amp; pension issues, thus he's in France's to work on trade...\",\n          \"oh wait are you even in this episode?\",\n          \"no one ever predicted this was going to happen.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"class\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"figurative\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-e729efba-487c-48af-b7c0-9dc9dfb30e2b\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no one ever predicted this was going to happen.</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>its as closely related as Andrews original cla...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I find it ironic when Vegans say they love food</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Quick rt that throwing money vine I've not see...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yep, keep adding me to your devops lists.... j...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>wonderful time with customer service can't add...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>: But ... football.</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>So last month I lost my job and today my dad l...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Rahm has solved crime, budget &amp;amp; pension is...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>And the heel on my shoe just broke. I love whe...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>This girl can't wait to shop on Michigan Ave. ...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>That shitty feeling we all love so much </td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>...says the person typing in shorthand and emoji.</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Crowded hallways are the loneliest places</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>I remember when I lived in Downtown Toronto......</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>I'll go for to say that.</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>oh wait are you even in this episode?</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>You say, via twitter.</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Maybe it's just me, but I'm sure that's not si...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>that I just tweeted about them</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>but moving the coaches from 1st to 3rd and 3rd...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>I just love getting anxiety at random times. I...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>last tweet was hashtag</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>To Liberals who care so much about Corporation...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>it's pretty obvious mad he's not getting credi...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e729efba-487c-48af-b7c0-9dc9dfb30e2b')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-e729efba-487c-48af-b7c0-9dc9dfb30e2b button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-e729efba-487c-48af-b7c0-9dc9dfb30e2b');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-3fe1e6ad-5a13-4d24-9ccd-797c93bffbda\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3fe1e6ad-5a13-4d24-9ccd-797c93bffbda')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-3fe1e6ad-5a13-4d24-9ccd-797c93bffbda button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                               tweets       class\n",
       "0     no one ever predicted this was going to happen.  figurative\n",
       "1   its as closely related as Andrews original cla...  figurative\n",
       "2     I find it ironic when Vegans say they love food  figurative\n",
       "3   Quick rt that throwing money vine I've not see...  figurative\n",
       "4   yep, keep adding me to your devops lists.... j...  figurative\n",
       "5   wonderful time with customer service can't add...  figurative\n",
       "6                                 : But ... football.  figurative\n",
       "7   So last month I lost my job and today my dad l...  figurative\n",
       "8   Rahm has solved crime, budget &amp; pension is...  figurative\n",
       "9   And the heel on my shoe just broke. I love whe...  figurative\n",
       "10  This girl can't wait to shop on Michigan Ave. ...  figurative\n",
       "11          That shitty feeling we all love so much   figurative\n",
       "12  ...says the person typing in shorthand and emoji.  figurative\n",
       "13          Crowded hallways are the loneliest places  figurative\n",
       "14  I remember when I lived in Downtown Toronto......  figurative\n",
       "15                           I'll go for to say that.  figurative\n",
       "16              oh wait are you even in this episode?  figurative\n",
       "17                              You say, via twitter.  figurative\n",
       "18  Maybe it's just me, but I'm sure that's not si...  figurative\n",
       "19                     that I just tweeted about them  figurative\n",
       "20  but moving the coaches from 1st to 3rd and 3rd...  figurative\n",
       "21  I just love getting anxiety at random times. I...  figurative\n",
       "22                             last tweet was hashtag  figurative\n",
       "23  To Liberals who care so much about Corporation...  figurative\n",
       "24  it's pretty obvious mad he's not getting credi...  figurative"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class\n",
      "irony         2110\n",
      "sarcasm       2105\n",
      "figurative    2043\n",
      "regular       1859\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# @title process data\n",
    "# @title load and clean\n",
    "\n",
    "import re\n",
    "\n",
    "LABEL_HASHTAGS = {\n",
    "    \"sarcasm\", \"irony\", \"sarcastic\", \"ironic\",\n",
    "    \"satire\", \"satirical\", \"joke\", \"funny\"\n",
    "}\n",
    "\n",
    "def clean_tweet(text):\n",
    "    # lower-case for consistency\n",
    "    text = text.strip()\n",
    "\n",
    "    # 1. remove @mentions\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "\n",
    "    # 2. remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\.\\S+\", \"\", text)\n",
    "\n",
    "    # 3. process hashtags\n",
    "    def replace_hashtag(match):\n",
    "        tag = match.group(1).lower()\n",
    "        if tag in LABEL_HASHTAGS:\n",
    "            return \"\"              # remove style hashtags\n",
    "        else:\n",
    "            return tag             # keep content hashtags but drop '#'\n",
    "\n",
    "    text = re.sub(r\"#(\\w+)\", replace_hashtag, text)\n",
    "\n",
    "    # 4. collapse repeated spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tweets_path = os.path.join(folder_path, \"data/tweets/test.csv\")\n",
    "df_tw = pd.read_csv(tweets_path)\n",
    "\n",
    "# Keep only non empty tweets\n",
    "df_tw = df_tw.dropna(subset=[\"tweets\"])\n",
    "df_tw = df_tw[df_tw[\"tweets\"].str.len() > 3].reset_index(drop=True)\n",
    "\n",
    "\n",
    "df_tw[\"tweets\"] = df_tw[\"tweets\"].apply(clean_tweet)\n",
    "df_tw = df_tw[df_tw[\"tweets\"].str.len() > 3]\n",
    "\n",
    "display(df_tw.head(25))\n",
    "print(df_tw[\"class\"].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 886,
     "status": "ok",
     "timestamp": 1763352555746,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "0JFkdLrK_wDI",
    "outputId": "d00e5fd3-800f-40c6-a003-78ed35461f0e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 8124/8124 [00:00<00:00, 10264.60it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"display(df_tw\",\n  \"rows\": 25,\n  \"fields\": [\n    {\n      \"column\": \"tweets\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 25,\n        \"samples\": [\n          \"Rahm has solved crime, budget &amp; pension issues, thus he's in France's to work on trade...\",\n          \"oh wait are you even in this episode?\",\n          \"no one ever predicted this was going to happen.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"class\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"figurative\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tokens\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"flags\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"input_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 25,\n        \"samples\": [\n          \"neutralize: Rahm[0] has[0] solved[0] crime[0] ,[0] budget[0] &[0] amp[0] ;[0] pension[0] issues[0] ,[0] thus[1] he[0] 's[0] in[0] France[0] 's[0] to[0] work[0] on[0] trade[0] ...[0]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 25,\n        \"samples\": [\n          \"Rahm has solved crime, budget &amp; pension issues, thus he's in France's to work on trade...\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-62c4eeca-de78-4247-b80f-268f1edf5fe2\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>class</th>\n",
       "      <th>tokens</th>\n",
       "      <th>flags</th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no one ever predicted this was going to happen.</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[no, one, ever, predicted, this, was, going, t...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>neutralize: no[0] one[0] ever[0] predicted[0] ...</td>\n",
       "      <td>no one ever predicted this was going to happen.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>its as closely related as Andrews original cla...</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[its, as, closely, related, as, Andrews, origi...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>neutralize: its[0] as[0] closely[0] related[0]...</td>\n",
       "      <td>its as closely related as Andrews original cla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I find it ironic when Vegans say they love food</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[I, find, it, ironic, when, Vegans, say, they,...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 1, 0]</td>\n",
       "      <td>neutralize: I[0] find[0] it[0] ironic[1] when[...</td>\n",
       "      <td>I find it ironic when Vegans say they love food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Quick rt that throwing money vine I've not see...</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[Quick, rt, that, throwing, money, vine, I, 'v...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>neutralize: Quick[0] rt[0] that[0] throwing[0]...</td>\n",
       "      <td>Quick rt that throwing money vine I've not see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yep, keep adding me to your devops lists.... j...</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[yep, ,, keep, adding, me, to, your, devops, l...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>neutralize: yep[1] ,[0] keep[0] adding[0] me[0...</td>\n",
       "      <td>yep, keep adding me to your devops lists.... j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>wonderful time with customer service can't add...</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[wonderful, time, with, customer, service, ca,...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>neutralize: wonderful[1] time[0] with[0] custo...</td>\n",
       "      <td>wonderful time with customer service can't add...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>: But ... football.</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[:, But, ..., football, .]</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "      <td>neutralize: :[0] But[0] ...[0] football[0] .[0]</td>\n",
       "      <td>: But ... football.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>So last month I lost my job and today my dad l...</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[So, last, month, I, lost, my, job, and, today...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>neutralize: So[0] last[0] month[0] I[0] lost[0...</td>\n",
       "      <td>So last month I lost my job and today my dad l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Rahm has solved crime, budget &amp;amp; pension is...</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[Rahm, has, solved, crime, ,, budget, &amp;, amp, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>neutralize: Rahm[0] has[0] solved[0] crime[0] ...</td>\n",
       "      <td>Rahm has solved crime, budget &amp;amp; pension is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>And the heel on my shoe just broke. I love whe...</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[And, the, heel, on, my, shoe, just, broke, .,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>neutralize: And[0] the[0] heel[0] on[0] my[0] ...</td>\n",
       "      <td>And the heel on my shoe just broke. I love whe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>This girl can't wait to shop on Michigan Ave. ...</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[This, girl, ca, n't, wait, to, shop, on, Mich...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>neutralize: This[0] girl[0] ca[0] n't[0] wait[...</td>\n",
       "      <td>This girl can't wait to shop on Michigan Ave. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>That shitty feeling we all love so much </td>\n",
       "      <td>figurative</td>\n",
       "      <td>[That, shitty, feeling, we, all, love, so, muc...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 1, 0, 0, 0]</td>\n",
       "      <td>neutralize: That[0] shitty[0] feeling[1] we[0]...</td>\n",
       "      <td>That shitty feeling we all love so much </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>...says the person typing in shorthand and emoji.</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[..., says, the, person, typing, in, shorthand...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>neutralize: ...[0] says[0] the[0] person[0] ty...</td>\n",
       "      <td>...says the person typing in shorthand and emoji.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Crowded hallways are the loneliest places</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[Crowded, hallways, are, the, loneliest, places]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>neutralize: Crowded[0] hallways[0] are[0] the[...</td>\n",
       "      <td>Crowded hallways are the loneliest places</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>I remember when I lived in Downtown Toronto......</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[I, remember, when, I, lived, in, Downtown, To...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "      <td>neutralize: I[0] remember[0] when[0] I[0] live...</td>\n",
       "      <td>I remember when I lived in Downtown Toronto......</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>I'll go for to say that.</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[I, 'll, go, for, to, say, that, .]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>neutralize: I[0] 'll[0] go[0] for[0] to[0] say...</td>\n",
       "      <td>I'll go for to say that.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>oh wait are you even in this episode?</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[oh, wait, are, you, even, in, this, episode, ?]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>neutralize: oh[1] wait[0] are[0] you[0] even[0...</td>\n",
       "      <td>oh wait are you even in this episode?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>You say, via twitter.</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[You, say, ,, via, twitter, .]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>neutralize: You[0] say[0] ,[0] via[0] twitter[...</td>\n",
       "      <td>You say, via twitter.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Maybe it's just me, but I'm sure that's not si...</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[Maybe, it, 's, just, me, ,, but, I, 'm, sure,...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>neutralize: Maybe[1] it[0] 's[0] just[0] me[0]...</td>\n",
       "      <td>Maybe it's just me, but I'm sure that's not si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>that I just tweeted about them</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[that, I, just, tweeted, about, them]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>neutralize: that[0] I[0] just[0] tweeted[0] ab...</td>\n",
       "      <td>that I just tweeted about them</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>but moving the coaches from 1st to 3rd and 3rd...</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[but, moving, the, coaches, from, 1st, to, 3rd...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>neutralize: but[0] moving[1] the[0] coaches[0]...</td>\n",
       "      <td>but moving the coaches from 1st to 3rd and 3rd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>I just love getting anxiety at random times. I...</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[I, just, love, getting, anxiety, at, random, ...</td>\n",
       "      <td>[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0]</td>\n",
       "      <td>neutralize: I[0] just[0] love[1] getting[0] an...</td>\n",
       "      <td>I just love getting anxiety at random times. I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>last tweet was hashtag</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[last, tweet, was, hashtag]</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "      <td>neutralize: last[0] tweet[0] was[0] hashtag[0]</td>\n",
       "      <td>last tweet was hashtag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>To Liberals who care so much about Corporation...</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[To, Liberals, who, care, so, much, about, Cor...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>neutralize: To[0] Liberals[0] who[0] care[0] s...</td>\n",
       "      <td>To Liberals who care so much about Corporation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>it's pretty obvious mad he's not getting credi...</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[it, 's, pretty, obvious, mad, he, 's, not, ge...</td>\n",
       "      <td>[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>neutralize: it[0] 's[0] pretty[1] obvious[0] m...</td>\n",
       "      <td>it's pretty obvious mad he's not getting credi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-62c4eeca-de78-4247-b80f-268f1edf5fe2')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-62c4eeca-de78-4247-b80f-268f1edf5fe2 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-62c4eeca-de78-4247-b80f-268f1edf5fe2');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-97c67af3-56d3-429d-ac58-edf8cafb32c1\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-97c67af3-56d3-429d-ac58-edf8cafb32c1')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-97c67af3-56d3-429d-ac58-edf8cafb32c1 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                               tweets       class  \\\n",
       "0     no one ever predicted this was going to happen.  figurative   \n",
       "1   its as closely related as Andrews original cla...  figurative   \n",
       "2     I find it ironic when Vegans say they love food  figurative   \n",
       "3   Quick rt that throwing money vine I've not see...  figurative   \n",
       "4   yep, keep adding me to your devops lists.... j...  figurative   \n",
       "5   wonderful time with customer service can't add...  figurative   \n",
       "6                                 : But ... football.  figurative   \n",
       "7   So last month I lost my job and today my dad l...  figurative   \n",
       "8   Rahm has solved crime, budget &amp; pension is...  figurative   \n",
       "9   And the heel on my shoe just broke. I love whe...  figurative   \n",
       "10  This girl can't wait to shop on Michigan Ave. ...  figurative   \n",
       "11          That shitty feeling we all love so much   figurative   \n",
       "12  ...says the person typing in shorthand and emoji.  figurative   \n",
       "13          Crowded hallways are the loneliest places  figurative   \n",
       "14  I remember when I lived in Downtown Toronto......  figurative   \n",
       "15                           I'll go for to say that.  figurative   \n",
       "16              oh wait are you even in this episode?  figurative   \n",
       "17                              You say, via twitter.  figurative   \n",
       "18  Maybe it's just me, but I'm sure that's not si...  figurative   \n",
       "19                     that I just tweeted about them  figurative   \n",
       "20  but moving the coaches from 1st to 3rd and 3rd...  figurative   \n",
       "21  I just love getting anxiety at random times. I...  figurative   \n",
       "22                             last tweet was hashtag  figurative   \n",
       "23  To Liberals who care so much about Corporation...  figurative   \n",
       "24  it's pretty obvious mad he's not getting credi...  figurative   \n",
       "\n",
       "                                               tokens  \\\n",
       "0   [no, one, ever, predicted, this, was, going, t...   \n",
       "1   [its, as, closely, related, as, Andrews, origi...   \n",
       "2   [I, find, it, ironic, when, Vegans, say, they,...   \n",
       "3   [Quick, rt, that, throwing, money, vine, I, 'v...   \n",
       "4   [yep, ,, keep, adding, me, to, your, devops, l...   \n",
       "5   [wonderful, time, with, customer, service, ca,...   \n",
       "6                          [:, But, ..., football, .]   \n",
       "7   [So, last, month, I, lost, my, job, and, today...   \n",
       "8   [Rahm, has, solved, crime, ,, budget, &, amp, ...   \n",
       "9   [And, the, heel, on, my, shoe, just, broke, .,...   \n",
       "10  [This, girl, ca, n't, wait, to, shop, on, Mich...   \n",
       "11  [That, shitty, feeling, we, all, love, so, muc...   \n",
       "12  [..., says, the, person, typing, in, shorthand...   \n",
       "13   [Crowded, hallways, are, the, loneliest, places]   \n",
       "14  [I, remember, when, I, lived, in, Downtown, To...   \n",
       "15                [I, 'll, go, for, to, say, that, .]   \n",
       "16   [oh, wait, are, you, even, in, this, episode, ?]   \n",
       "17                     [You, say, ,, via, twitter, .]   \n",
       "18  [Maybe, it, 's, just, me, ,, but, I, 'm, sure,...   \n",
       "19              [that, I, just, tweeted, about, them]   \n",
       "20  [but, moving, the, coaches, from, 1st, to, 3rd...   \n",
       "21  [I, just, love, getting, anxiety, at, random, ...   \n",
       "22                        [last, tweet, was, hashtag]   \n",
       "23  [To, Liberals, who, care, so, much, about, Cor...   \n",
       "24  [it, 's, pretty, obvious, mad, he, 's, not, ge...   \n",
       "\n",
       "                                                flags  \\\n",
       "0                      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "1                [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "2                      [0, 0, 0, 1, 0, 0, 0, 0, 1, 0]   \n",
       "3   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4                [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "5   [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...   \n",
       "6                                     [0, 0, 0, 0, 0]   \n",
       "7   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "8   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...   \n",
       "9   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...   \n",
       "10                  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "11                        [0, 0, 1, 0, 0, 1, 0, 0, 0]   \n",
       "12                     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "13                                 [0, 0, 0, 0, 0, 0]   \n",
       "14         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]   \n",
       "15                           [0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "16                        [1, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "17                                 [0, 0, 0, 0, 0, 0]   \n",
       "18  [1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...   \n",
       "19                                 [0, 0, 0, 0, 0, 0]   \n",
       "20  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...   \n",
       "21               [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0]   \n",
       "22                                       [0, 0, 0, 0]   \n",
       "23  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "24      [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "                                           input_text  \\\n",
       "0   neutralize: no[0] one[0] ever[0] predicted[0] ...   \n",
       "1   neutralize: its[0] as[0] closely[0] related[0]...   \n",
       "2   neutralize: I[0] find[0] it[0] ironic[1] when[...   \n",
       "3   neutralize: Quick[0] rt[0] that[0] throwing[0]...   \n",
       "4   neutralize: yep[1] ,[0] keep[0] adding[0] me[0...   \n",
       "5   neutralize: wonderful[1] time[0] with[0] custo...   \n",
       "6     neutralize: :[0] But[0] ...[0] football[0] .[0]   \n",
       "7   neutralize: So[0] last[0] month[0] I[0] lost[0...   \n",
       "8   neutralize: Rahm[0] has[0] solved[0] crime[0] ...   \n",
       "9   neutralize: And[0] the[0] heel[0] on[0] my[0] ...   \n",
       "10  neutralize: This[0] girl[0] ca[0] n't[0] wait[...   \n",
       "11  neutralize: That[0] shitty[0] feeling[1] we[0]...   \n",
       "12  neutralize: ...[0] says[0] the[0] person[0] ty...   \n",
       "13  neutralize: Crowded[0] hallways[0] are[0] the[...   \n",
       "14  neutralize: I[0] remember[0] when[0] I[0] live...   \n",
       "15  neutralize: I[0] 'll[0] go[0] for[0] to[0] say...   \n",
       "16  neutralize: oh[1] wait[0] are[0] you[0] even[0...   \n",
       "17  neutralize: You[0] say[0] ,[0] via[0] twitter[...   \n",
       "18  neutralize: Maybe[1] it[0] 's[0] just[0] me[0]...   \n",
       "19  neutralize: that[0] I[0] just[0] tweeted[0] ab...   \n",
       "20  neutralize: but[0] moving[1] the[0] coaches[0]...   \n",
       "21  neutralize: I[0] just[0] love[1] getting[0] an...   \n",
       "22     neutralize: last[0] tweet[0] was[0] hashtag[0]   \n",
       "23  neutralize: To[0] Liberals[0] who[0] care[0] s...   \n",
       "24  neutralize: it[0] 's[0] pretty[1] obvious[0] m...   \n",
       "\n",
       "                                          target_text  \n",
       "0     no one ever predicted this was going to happen.  \n",
       "1   its as closely related as Andrews original cla...  \n",
       "2     I find it ironic when Vegans say they love food  \n",
       "3   Quick rt that throwing money vine I've not see...  \n",
       "4   yep, keep adding me to your devops lists.... j...  \n",
       "5   wonderful time with customer service can't add...  \n",
       "6                                 : But ... football.  \n",
       "7   So last month I lost my job and today my dad l...  \n",
       "8   Rahm has solved crime, budget &amp; pension is...  \n",
       "9   And the heel on my shoe just broke. I love whe...  \n",
       "10  This girl can't wait to shop on Michigan Ave. ...  \n",
       "11          That shitty feeling we all love so much   \n",
       "12  ...says the person typing in shorthand and emoji.  \n",
       "13          Crowded hallways are the loneliest places  \n",
       "14  I remember when I lived in Downtown Toronto......  \n",
       "15                           I'll go for to say that.  \n",
       "16              oh wait are you even in this episode?  \n",
       "17                              You say, via twitter.  \n",
       "18  Maybe it's just me, but I'm sure that's not si...  \n",
       "19                     that I just tweeted about them  \n",
       "20  but moving the coaches from 1st to 3rd and 3rd...  \n",
       "21  I just love getting anxiety at random times. I...  \n",
       "22                             last tweet was hashtag  \n",
       "23  To Liberals who care so much about Corporation...  \n",
       "24  it's pretty obvious mad he's not getting credi...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @title flag with lexicon\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "df_tw[\"tokens_flags\"] = df_tw[\"tweets\"].progress_apply(\n",
    "    lambda x: tag_subjectivity(x, mpqa_dict)\n",
    ")\n",
    "df_tw[\"tokens\"] = df_tw[\"tokens_flags\"].apply(lambda x: x[0])\n",
    "df_tw[\"flags\"]  = df_tw[\"tokens_flags\"].apply(lambda x: x[1])\n",
    "df_tw = df_tw.drop(columns=[\"tokens_flags\"])\n",
    "\n",
    "df_tw[[\"tweets\", \"tokens\", \"flags\"]].head(25)\n",
    "\n",
    "def format_input(tokens, flags):\n",
    "    combined = \" \".join(f\"{w}[{f}]\" for w, f in zip(tokens, flags))\n",
    "    return \"neutralize: \" + combined\n",
    "\n",
    "df_tw[\"input_text\"] = [\n",
    "    format_input(t, f) for t, f in zip(df_tw[\"tokens\"], df_tw[\"flags\"])\n",
    "]\n",
    "df_tw[\"target_text\"] = df_tw[\"tweets\"]\n",
    "\n",
    "display(df_tw.head(25))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vgX3GPsy_26N"
   },
   "outputs": [],
   "source": [
    "# @title save cleaned flagged dataset\n",
    "all_out = os.path.join(folder_path, \"data/tweets/processed/mpqa_lexicon/flagged_test.csv\")\n",
    "os.makedirs(os.path.dirname(all_out), exist_ok=True)\n",
    "df_tw.to_csv(all_out, index=False)\n",
    "\n",
    "all_out = os.path.join(folder_path, \"data/tweets/processed/mpqa_lexicon/input_target_pair_tweets_test.csv\")\n",
    "os.makedirs(os.path.dirname(all_out), exist_ok=True)\n",
    "df_tw[[\"input_text\", \"target_text\"]].to_csv(all_out, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 454,
     "status": "ok",
     "timestamp": 1763352856236,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "Ufn1DBU-BTyL",
    "outputId": "5b7d5e45-7671-4a30-a1cb-2361386abb02"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @title load model\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "checkpoint_path = os.path.join(folder_path,\n",
    "    \"model_checkpoints/tweets-mpqa-flan_t5-2lossrf-11_16/checkpoint-10000\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "trained_model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_path)\n",
    "trained_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136,
     "referenced_widgets": [
      "ea684ce24bf14bbdade11ca65a3c763a",
      "d6cf00fbc50a499cb815a96408666cb1",
      "cbc84aacf8034928ad05ae622ad146c2",
      "38ce96b9dbe94ff7a8295dffca552ff3",
      "8bd0b3b56c404af9893293abbe91e6ac",
      "8e43a9e9a43d4ae29c509701386fb25d",
      "c731b5a456874971b3136eb4b7b72902",
      "c9bb4c82728743ffbc53ac2534a67582",
      "873469a2c0a54c5fad999326c3ca2392",
      "05c57397d7594f50939907ff0391be38",
      "03402f1e9e3e47fdaecf832ccb7e9c95",
      "b9046cc01eaa42ec8bb44d7c4d76298e",
      "48f23b89cc7e489887193874080d4216",
      "17e12a31f2e64de1a588af29004bcc8f",
      "ebc89632f52746f5bf00a564d55c1fec",
      "697189ee43704e57ba015ff4466432d8",
      "1b97ed39453c48edba49ceaa2ea082f2",
      "6341f62e59794ed49e6de95a585f7738",
      "f5da34916279459bad312762d416c9c4",
      "47f916fd5bf74379829652b1307c3244",
      "dc96283bafc944459fdc2a9185bae7e4",
      "f5ccc9ce200644979bd28f59108e9f62"
     ]
    },
    "executionInfo": {
     "elapsed": 7872,
     "status": "ok",
     "timestamp": 1763395353505,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "sQSbDy1iA_y0",
    "outputId": "5a137932-f13f-42b9-ad6a-d5e3ae0ed2ab"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea684ce24bf14bbdade11ca65a3c763a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9046cc01eaa42ec8bb44d7c4d76298e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8124 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# @title load dataset\n",
    "dataset_test = load_dataset(\"csv\", data_files=os.path.join(folder_path, \"data/tweets/processed/mpqa_lexicon/input_target_pair_tweets_test.csv\"))\n",
    "max_length = 128\n",
    "\n",
    "def preprocess_test(batch):\n",
    "    inputs = tokenizer(batch[\"input_text\"], max_length=max_length, truncation=True, padding=\"max_length\")\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(batch[\"target_text\"], max_length=max_length, truncation=True, padding=\"max_length\")\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "test_dataset = dataset_test.map(preprocess_test, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "executionInfo": {
     "elapsed": 1064842,
     "status": "ok",
     "timestamp": 1763353957188,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "IKpHsCx1A20v",
    "outputId": "3515ca25-16bc-4a6b-aa03-94983bf65ecd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-946400540.py:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @title run predictions on test set\n",
    "\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "# make sure predict_with_generate=True so it uses model.generate()\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./eval_results\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,  # turn off on GPU\n",
    "    report_to=\"none\",\n",
    "    generation_max_length=128,     # allow up to 128 tokens in output\n",
    "    generation_num_beams=4,        # optional, improves quality (beam search)\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=trained_model,\n",
    "    args=args,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "predictions = trainer.predict(test_dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 570
    },
    "executionInfo": {
     "elapsed": 905,
     "status": "ok",
     "timestamp": 1763354182468,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "oygfWtTwGIlT",
    "outputId": "7e0eba7a-c3ba-4cc7-e87c-21963eb528e3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"display(df_results\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"input_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"neutralize: Rahm[0] has[0] solved[0] crime[0] ,[0] budget[0] &[0] amp[0] ;[0] pension[0] issues[0] ,[0] thus[1] he[0] 's[0] in[0] France[0] 's[0] to[0] work[0] on[0] trade[0] ...[0]\",\n          \"neutralize: its[0] as[0] closely[0] related[0] as[0] Andrews[0] original[0] claim[0] that[0] evolution[0] and[0] entropy[0]\",\n          \"neutralize: wonderful[1] time[0] with[0] customer[0] service[0] ca[0] n't[0] add[0] line[0] w[0] loyalty[1] plan[0] .[0] Rep[0] could[0] n't[0] even[0] explain[0] data[0] plan[0] options[0] to[0] me[0]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Rahm has solved crime, budget &amp; pension issues, thus he's in France's to work on trade...\",\n          \"its as closely related as Andrews original claim that evolution and entropy\",\n          \"wonderful time with customer service can't add line w loyalty plan. Rep couldn't even explain data plan options to me\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"predicted_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Rahm has solved crime, budget &amp; pension issues, thus he's in France's to work on trade...\",\n          \"its as closely related as Andrews original claim that evolution and entropy\",\n          \"wonderful time with customer service can't add line w loyalty plan. Rep couldn't even explain data plan options to me\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-2ca6224a-16f7-42f8-9497-3942f7306668\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "      <th>predicted_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutralize: no[0] one[0] ever[0] predicted[0] ...</td>\n",
       "      <td>no one ever predicted this was going to happen.</td>\n",
       "      <td>no one ever predicted this was going to happen.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutralize: its[0] as[0] closely[0] related[0]...</td>\n",
       "      <td>its as closely related as Andrews original cla...</td>\n",
       "      <td>its as closely related as Andrews original cla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutralize: I[0] find[0] it[0] ironic[1] when[...</td>\n",
       "      <td>I find it ironic when Vegans say they love food</td>\n",
       "      <td>I find it ironic when Vegans say they love food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutralize: Quick[0] rt[0] that[0] throwing[0]...</td>\n",
       "      <td>Quick rt that throwing money vine I've not see...</td>\n",
       "      <td>Quick rt that throwing money vine I've not see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutralize: yep[1] ,[0] keep[0] adding[0] me[0...</td>\n",
       "      <td>yep, keep adding me to your devops lists.... j...</td>\n",
       "      <td>yep, keep adding me to your devops lists.... j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>neutralize: wonderful[1] time[0] with[0] custo...</td>\n",
       "      <td>wonderful time with customer service can't add...</td>\n",
       "      <td>wonderful time with customer service can't add...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>neutralize: :[0] But[0] ...[0] football[0] .[0]</td>\n",
       "      <td>: But ... football.</td>\n",
       "      <td>: But... football.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>neutralize: So[0] last[0] month[0] I[0] lost[0...</td>\n",
       "      <td>So last month I lost my job and today my dad l...</td>\n",
       "      <td>So last month I lost my job and today my dad l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>neutralize: Rahm[0] has[0] solved[0] crime[0] ...</td>\n",
       "      <td>Rahm has solved crime, budget &amp;amp; pension is...</td>\n",
       "      <td>Rahm has solved crime, budget &amp;amp; pension is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>neutralize: And[0] the[0] heel[0] on[0] my[0] ...</td>\n",
       "      <td>And the heel on my shoe just broke. I love whe...</td>\n",
       "      <td>And the heel on my shoe just broke. I love whe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2ca6224a-16f7-42f8-9497-3942f7306668')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-2ca6224a-16f7-42f8-9497-3942f7306668 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-2ca6224a-16f7-42f8-9497-3942f7306668');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-5c6903b3-ebd5-453f-a15b-0864bcd2f97d\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5c6903b3-ebd5-453f-a15b-0864bcd2f97d')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-5c6903b3-ebd5-453f-a15b-0864bcd2f97d button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                          input_text  \\\n",
       "0  neutralize: no[0] one[0] ever[0] predicted[0] ...   \n",
       "1  neutralize: its[0] as[0] closely[0] related[0]...   \n",
       "2  neutralize: I[0] find[0] it[0] ironic[1] when[...   \n",
       "3  neutralize: Quick[0] rt[0] that[0] throwing[0]...   \n",
       "4  neutralize: yep[1] ,[0] keep[0] adding[0] me[0...   \n",
       "5  neutralize: wonderful[1] time[0] with[0] custo...   \n",
       "6    neutralize: :[0] But[0] ...[0] football[0] .[0]   \n",
       "7  neutralize: So[0] last[0] month[0] I[0] lost[0...   \n",
       "8  neutralize: Rahm[0] has[0] solved[0] crime[0] ...   \n",
       "9  neutralize: And[0] the[0] heel[0] on[0] my[0] ...   \n",
       "\n",
       "                                         target_text  \\\n",
       "0    no one ever predicted this was going to happen.   \n",
       "1  its as closely related as Andrews original cla...   \n",
       "2    I find it ironic when Vegans say they love food   \n",
       "3  Quick rt that throwing money vine I've not see...   \n",
       "4  yep, keep adding me to your devops lists.... j...   \n",
       "5  wonderful time with customer service can't add...   \n",
       "6                                : But ... football.   \n",
       "7  So last month I lost my job and today my dad l...   \n",
       "8  Rahm has solved crime, budget &amp; pension is...   \n",
       "9  And the heel on my shoe just broke. I love whe...   \n",
       "\n",
       "                                      predicted_text  \n",
       "0    no one ever predicted this was going to happen.  \n",
       "1  its as closely related as Andrews original cla...  \n",
       "2    I find it ironic when Vegans say they love food  \n",
       "3  Quick rt that throwing money vine I've not see...  \n",
       "4  yep, keep adding me to your devops lists.... j...  \n",
       "5  wonderful time with customer service can't add...  \n",
       "6                                 : But... football.  \n",
       "7  So last month I lost my job and today my dad l...  \n",
       "8  Rahm has solved crime, budget &amp; pension is...  \n",
       "9  And the heel on my shoe just broke. I love whe...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Replace potential out-of-vocabulary predicted IDs with pad_token_id\n",
    "predictions.predictions[predictions.predictions >= tokenizer.vocab_size] = tokenizer.pad_token_id\n",
    "predictions.predictions[predictions.predictions < 0] = tokenizer.pad_token_id # Also handle potential negative indices\n",
    "\n",
    "preds=tokenizer.batch_decode(\n",
    "    predictions.predictions, skip_special_tokens=True\n",
    ")\n",
    "labels = tokenizer.batch_decode(\n",
    "    predictions.label_ids, skip_special_tokens=True\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "df_results = pd.DataFrame({\n",
    "    \"input_text\": dataset_test[\"train\"][\"input_text\"],\n",
    "    \"target_text\": labels,\n",
    "    \"predicted_text\": preds\n",
    "})\n",
    "df_results.to_csv(os.path.join(folder_path, \"output/tweets/predictions_testset_tweets-mpqa-flan_t5-2lossrf-11_16.csv\"), index=False)\n",
    "\n",
    "display(df_results.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 193447,
     "status": "ok",
     "timestamp": 1763354733285,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "VScad5wEF0df",
    "outputId": "908d8be7-5e48-405a-a9e0-4e5faf31bb44"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean objectivity score: 0.39062714838647067\n",
      "std: 0.2304217417035224\n",
      "total percent of objective sentences: 0.26809453471196454\n"
     ]
    }
   ],
   "source": [
    "# @title objectivity evaluation\n",
    "import numpy as np\n",
    "clf_name = \"GroNLP/mdebertav3-subjectivity-english\"\n",
    "clf_tok = AutoTokenizer.from_pretrained(clf_name)\n",
    "clf = AutoModelForSequenceClassification.from_pretrained(clf_name).to(trained_model.device).eval()\n",
    "\n",
    "# predicted text\n",
    "path=os.path.join(folder_path, \"output/tweets/predictions_testset_tweets-mpqa-flan_t5-2lossrf-11_16.csv\")\n",
    "df=pd.read_csv(path)\n",
    "\n",
    "# mismatches_df = df[df['predicted_text'] != df['target_text']]\n",
    "# display(mismatches_df)\n",
    "scores = []\n",
    "count_objective=0\n",
    "for text in df[\"predicted_text\"]:\n",
    "    tokens = clf_tok(text, return_tensors=\"pt\", truncation=True).to(trained_model.device)\n",
    "    with torch.no_grad():\n",
    "        probs = clf(**tokens).logits.softmax(dim=-1)\n",
    "        p_objective = probs[0][0].item()  # index 0 = objective\n",
    "        if p_objective>0.5:\n",
    "          count_objective+=1\n",
    "\n",
    "    scores.append(p_objective)\n",
    "\n",
    "print(\"Mean objectivity score:\", np.mean(scores))\n",
    "print(\"std:\", np.std(scores))\n",
    "print(\"total percent of objective sentences:\", count_objective/len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 197775,
     "status": "ok",
     "timestamp": 1763354931058,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "m9XKgns5HsCw",
    "outputId": "9d9acf5f-f32c-4b8b-8475-e09a3967017e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean objectivity score: 0.38940380347121356\n",
      "std: 0.23052298472748756\n",
      "total percent of objective sentences: 0.266494337764648\n"
     ]
    }
   ],
   "source": [
    "# target text\n",
    "path=os.path.join(folder_path, \"output/tweets/predictions_testset_tweets-mpqa-flan_t5-2lossrf-11_16.csv\")\n",
    "df=pd.read_csv(path)\n",
    "\n",
    "scores = []\n",
    "count_objective=0\n",
    "for text in df[\"target_text\"]:\n",
    "    tokens = aclf_tok(text, return_tensors=\"pt\", truncation=True).to(trained_model.device)\n",
    "    with torch.no_grad():\n",
    "        probs = clf(**tokens).logits.softmax(dim=-1)\n",
    "        p_objective = probs[0][0].item()  # index 0 = objective\n",
    "        if p_objective>0.5:\n",
    "          count_objective+=1\n",
    "\n",
    "    scores.append(p_objective)\n",
    "\n",
    "print(\"Mean objectivity score:\", np.mean(scores))\n",
    "print(\"std:\", np.std(scores))\n",
    "print(\"total percent of objective sentences:\", count_objective/len(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDBb5SHCJvM8"
   },
   "source": [
    "## load a different checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1214,
     "status": "ok",
     "timestamp": 1763355147155,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "oetiGaG2JueY",
    "outputId": "7b3cb7c3-884b-4264-d166-a7592baff3c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @title load model\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "checkpoint_path = os.path.join(folder_path,\n",
    "    \"model_checkpoints/tweets-mpqa-flan_t5-2lossrf-11_16/checkpoint-4500\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "trained_model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_path)\n",
    "trained_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "executionInfo": {
     "elapsed": 1037997,
     "status": "ok",
     "timestamp": 1763356207452,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "3iiuuEIDKH0R",
    "outputId": "0806b2e6-c53d-40bb-b931-4b1ffd2c9fb3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-946400540.py:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @title run predictions on test set\n",
    "\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "# make sure predict_with_generate=True so it uses model.generate()\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./eval_results\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,  # turn off on GPU\n",
    "    report_to=\"none\",\n",
    "    generation_max_length=128,     # allow up to 128 tokens in output\n",
    "    generation_num_beams=4,        # optional, improves quality (beam search)\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=trained_model,\n",
    "    args=args,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "predictions = trainer.predict(test_dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 570
    },
    "executionInfo": {
     "elapsed": 686,
     "status": "ok",
     "timestamp": 1763356208142,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "mBdot6LbKMCY",
    "outputId": "b9502456-db84-4bd3-9b45-dad4a4a61b60"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"display(df_results\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"input_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"neutralize: Rahm[0] has[0] solved[0] crime[0] ,[0] budget[0] &[0] amp[0] ;[0] pension[0] issues[0] ,[0] thus[1] he[0] 's[0] in[0] France[0] 's[0] to[0] work[0] on[0] trade[0] ...[0]\",\n          \"neutralize: its[0] as[0] closely[0] related[0] as[0] Andrews[0] original[0] claim[0] that[0] evolution[0] and[0] entropy[0]\",\n          \"neutralize: wonderful[1] time[0] with[0] customer[0] service[0] ca[0] n't[0] add[0] line[0] w[0] loyalty[1] plan[0] .[0] Rep[0] could[0] n't[0] even[0] explain[0] data[0] plan[0] options[0] to[0] me[0]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Rahm has solved crime, budget &amp; pension issues, thus he's in France's to work on trade...\",\n          \"its as closely related as Andrews original claim that evolution and entropy\",\n          \"wonderful time with customer service can't add line w loyalty plan. Rep couldn't even explain data plan options to me\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"predicted_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Rahm has solved crime, budget &amp; pension issues, thus he's in France's to work on trade...\",\n          \"its as closely related as Andrews original claim that evolution and entropy\",\n          \"wonderful time with customer service can't add line w loyalty plan. Rep couldn't even explain data plan options to me\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-71032e1c-9f15-4fd8-85a7-5be6d01a3b9a\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "      <th>predicted_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutralize: no[0] one[0] ever[0] predicted[0] ...</td>\n",
       "      <td>no one ever predicted this was going to happen.</td>\n",
       "      <td>no one ever predicted this was going to happen.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutralize: its[0] as[0] closely[0] related[0]...</td>\n",
       "      <td>its as closely related as Andrews original cla...</td>\n",
       "      <td>its as closely related as Andrews original cla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutralize: I[0] find[0] it[0] ironic[1] when[...</td>\n",
       "      <td>I find it ironic when Vegans say they love food</td>\n",
       "      <td>I find it ironic when Vegans say they love food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutralize: Quick[0] rt[0] that[0] throwing[0]...</td>\n",
       "      <td>Quick rt that throwing money vine I've not see...</td>\n",
       "      <td>Quick rt that throwing money vine I've not see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutralize: yep[1] ,[0] keep[0] adding[0] me[0...</td>\n",
       "      <td>yep, keep adding me to your devops lists.... j...</td>\n",
       "      <td>yep, keep adding me to your devops lists.... j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>neutralize: wonderful[1] time[0] with[0] custo...</td>\n",
       "      <td>wonderful time with customer service can't add...</td>\n",
       "      <td>wonderful time with customer service can't add...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>neutralize: :[0] But[0] ...[0] football[0] .[0]</td>\n",
       "      <td>: But ... football.</td>\n",
       "      <td>: But... football.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>neutralize: So[0] last[0] month[0] I[0] lost[0...</td>\n",
       "      <td>So last month I lost my job and today my dad l...</td>\n",
       "      <td>So last month I lost my job and today my dad l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>neutralize: Rahm[0] has[0] solved[0] crime[0] ...</td>\n",
       "      <td>Rahm has solved crime, budget &amp;amp; pension is...</td>\n",
       "      <td>Rahm has solved crime, budget &amp;amp; pension is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>neutralize: And[0] the[0] heel[0] on[0] my[0] ...</td>\n",
       "      <td>And the heel on my shoe just broke. I love whe...</td>\n",
       "      <td>And the heel on my shoe just broke. I love whe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-71032e1c-9f15-4fd8-85a7-5be6d01a3b9a')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-71032e1c-9f15-4fd8-85a7-5be6d01a3b9a button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-71032e1c-9f15-4fd8-85a7-5be6d01a3b9a');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-79db8f2d-2886-4931-86a9-d84341aea016\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-79db8f2d-2886-4931-86a9-d84341aea016')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-79db8f2d-2886-4931-86a9-d84341aea016 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                          input_text  \\\n",
       "0  neutralize: no[0] one[0] ever[0] predicted[0] ...   \n",
       "1  neutralize: its[0] as[0] closely[0] related[0]...   \n",
       "2  neutralize: I[0] find[0] it[0] ironic[1] when[...   \n",
       "3  neutralize: Quick[0] rt[0] that[0] throwing[0]...   \n",
       "4  neutralize: yep[1] ,[0] keep[0] adding[0] me[0...   \n",
       "5  neutralize: wonderful[1] time[0] with[0] custo...   \n",
       "6    neutralize: :[0] But[0] ...[0] football[0] .[0]   \n",
       "7  neutralize: So[0] last[0] month[0] I[0] lost[0...   \n",
       "8  neutralize: Rahm[0] has[0] solved[0] crime[0] ...   \n",
       "9  neutralize: And[0] the[0] heel[0] on[0] my[0] ...   \n",
       "\n",
       "                                         target_text  \\\n",
       "0    no one ever predicted this was going to happen.   \n",
       "1  its as closely related as Andrews original cla...   \n",
       "2    I find it ironic when Vegans say they love food   \n",
       "3  Quick rt that throwing money vine I've not see...   \n",
       "4  yep, keep adding me to your devops lists.... j...   \n",
       "5  wonderful time with customer service can't add...   \n",
       "6                                : But ... football.   \n",
       "7  So last month I lost my job and today my dad l...   \n",
       "8  Rahm has solved crime, budget &amp; pension is...   \n",
       "9  And the heel on my shoe just broke. I love whe...   \n",
       "\n",
       "                                      predicted_text  \n",
       "0    no one ever predicted this was going to happen.  \n",
       "1  its as closely related as Andrews original cla...  \n",
       "2    I find it ironic when Vegans say they love food  \n",
       "3  Quick rt that throwing money vine I've not see...  \n",
       "4  yep, keep adding me to your devops lists.... j...  \n",
       "5  wonderful time with customer service can't add...  \n",
       "6                                 : But... football.  \n",
       "7  So last month I lost my job and today my dad l...  \n",
       "8  Rahm has solved crime, budget &amp; pension is...  \n",
       "9  And the heel on my shoe just broke. I love whe...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Replace potential out-of-vocabulary predicted IDs with pad_token_id\n",
    "predictions.predictions[predictions.predictions >= tokenizer.vocab_size] = tokenizer.pad_token_id\n",
    "predictions.predictions[predictions.predictions < 0] = tokenizer.pad_token_id # Also handle potential negative indices\n",
    "\n",
    "preds=tokenizer.batch_decode(\n",
    "    predictions.predictions, skip_special_tokens=True\n",
    ")\n",
    "labels = tokenizer.batch_decode(\n",
    "    predictions.label_ids, skip_special_tokens=True\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "df_results = pd.DataFrame({\n",
    "    \"input_text\": dataset_test[\"train\"][\"input_text\"],\n",
    "    \"target_text\": labels,\n",
    "    \"predicted_text\": preds\n",
    "})\n",
    "df_results.to_csv(os.path.join(folder_path, \"output/tweets/predictions_testset_tweets-mpqa-flan_t5-2lossrf-11_16-cp4500.csv\"), index=False)\n",
    "\n",
    "display(df_results.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 202371,
     "status": "ok",
     "timestamp": 1763356491930,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "0AzPFMGAOR7x",
    "outputId": "1fc79220-06ca-4f95-fb7c-3f9c6e299a28"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean objectivity score: 0.39084645934450635\n",
      "std: 0.23076748559242208\n",
      "total percent of objective sentences: 0.26834071885770555\n"
     ]
    }
   ],
   "source": [
    "# @title objectivity evaluation\n",
    "import numpy as np\n",
    "clf_name = \"GroNLP/mdebertav3-subjectivity-english\"\n",
    "clf_tok = AutoTokenizer.from_pretrained(clf_name)\n",
    "clf = AutoModelForSequenceClassification.from_pretrained(clf_name).to(trained_model.device).eval()\n",
    "\n",
    "# predicted text\n",
    "path=os.path.join(folder_path, \"output/tweets/predictions_testset_tweets-mpqa-flan_t5-2lossrf-11_16-cp4500.csv\")\n",
    "df=pd.read_csv(path)\n",
    "\n",
    "# mismatches_df = df[df['predicted_text'] != df['target_text']]\n",
    "# display(mismatches_df)\n",
    "scores = []\n",
    "count_objective=0\n",
    "for text in df[\"predicted_text\"]:\n",
    "    tokens = clf_tok(text, return_tensors=\"pt\", truncation=True).to(trained_model.device)\n",
    "    with torch.no_grad():\n",
    "        probs = clf(**tokens).logits.softmax(dim=-1)\n",
    "        p_objective = probs[0][0].item()  # index 0 = objective\n",
    "        if p_objective>0.5:\n",
    "          count_objective+=1\n",
    "\n",
    "    scores.append(p_objective)\n",
    "\n",
    "print(\"Mean objectivity score:\", np.mean(scores))\n",
    "print(\"std:\", np.std(scores))\n",
    "print(\"total percent of objective sentences:\", count_objective/len(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vGPnIfjRg3wG"
   },
   "source": [
    "## load wnc checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30236,
     "status": "ok",
     "timestamp": 1763395210385,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "UfUbXTVkg3Y5",
    "outputId": "d5f89b2c-8ecf-46cf-febf-b07f102676f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @title load model\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "checkpoint_path = os.path.join(folder_path,\n",
    "    \"model_checkpoints/wnc-mpqa-flan_t5-2loss-11_09/checkpoint-6000\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "trained_model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_path)\n",
    "trained_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "executionInfo": {
     "elapsed": 1171747,
     "status": "ok",
     "timestamp": 1763396548753,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "XqnXoI0hiyHL",
    "outputId": "5cb57dd0-41cd-4bde-bc61-1091a43a61af"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-946400540.py:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @title run predictions on test set\n",
    "\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "# make sure predict_with_generate=True so it uses model.generate()\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./eval_results\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,  # turn off on GPU\n",
    "    report_to=\"none\",\n",
    "    generation_max_length=128,     # allow up to 128 tokens in output\n",
    "    generation_num_beams=4,        # optional, improves quality (beam search)\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=trained_model,\n",
    "    args=args,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "predictions = trainer.predict(test_dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 570
    },
    "executionInfo": {
     "elapsed": 1151,
     "status": "ok",
     "timestamp": 1763396549909,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "EvSuAu2pjR69",
    "outputId": "357edf05-227a-4ef6-d55b-7d99df65d053"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"display(df_results\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"input_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"neutralize: Rahm[0] has[0] solved[0] crime[0] ,[0] budget[0] &[0] amp[0] ;[0] pension[0] issues[0] ,[0] thus[1] he[0] 's[0] in[0] France[0] 's[0] to[0] work[0] on[0] trade[0] ...[0]\",\n          \"neutralize: its[0] as[0] closely[0] related[0] as[0] Andrews[0] original[0] claim[0] that[0] evolution[0] and[0] entropy[0]\",\n          \"neutralize: wonderful[1] time[0] with[0] customer[0] service[0] ca[0] n't[0] add[0] line[0] w[0] loyalty[1] plan[0] .[0] Rep[0] could[0] n't[0] even[0] explain[0] data[0] plan[0] options[0] to[0] me[0]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Rahm has solved crime, budget &amp; pension issues, thus he's in France's to work on trade...\",\n          \"its as closely related as Andrews original claim that evolution and entropy\",\n          \"wonderful time with customer service can't add line w loyalty plan. Rep couldn't even explain data plan options to me\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"predicted_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Rahm has solved crime, budget & amp; pension issues, thus he's in France's to work on trade...\",\n          \"its as closely related as Andrews original claim that evolution and entropy\",\n          \"wonderful time with customer service ca n't add line w loyalty plan. Rep could n't even explain data plan options to me\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-eb9a6ea7-812c-434b-b50b-69e052305bed\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "      <th>predicted_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutralize: no[0] one[0] ever[0] predicted[0] ...</td>\n",
       "      <td>no one ever predicted this was going to happen.</td>\n",
       "      <td>no one ever predicted this was going to happen.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutralize: its[0] as[0] closely[0] related[0]...</td>\n",
       "      <td>its as closely related as Andrews original cla...</td>\n",
       "      <td>its as closely related as Andrews original cla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutralize: I[0] find[0] it[0] ironic[1] when[...</td>\n",
       "      <td>I find it ironic when Vegans say they love food</td>\n",
       "      <td>I find it ironic when vegans say they love food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutralize: Quick[0] rt[0] that[0] throwing[0]...</td>\n",
       "      <td>Quick rt that throwing money vine I've not see...</td>\n",
       "      <td>Quick rt that throwing money vine I've not see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutralize: yep[1] ,[0] keep[0] adding[0] me[0...</td>\n",
       "      <td>yep, keep adding me to your devops lists.... j...</td>\n",
       "      <td>yep, keep adding me to your devops lists.... j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>neutralize: wonderful[1] time[0] with[0] custo...</td>\n",
       "      <td>wonderful time with customer service can't add...</td>\n",
       "      <td>wonderful time with customer service ca n't ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>neutralize: :[0] But[0] ...[0] football[0] .[0]</td>\n",
       "      <td>: But ... football.</td>\n",
       "      <td>: But ... football.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>neutralize: So[0] last[0] month[0] I[0] lost[0...</td>\n",
       "      <td>So last month I lost my job and today my dad l...</td>\n",
       "      <td>So last month I lost my job and today my dad l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>neutralize: Rahm[0] has[0] solved[0] crime[0] ...</td>\n",
       "      <td>Rahm has solved crime, budget &amp;amp; pension is...</td>\n",
       "      <td>Rahm has solved crime, budget &amp; amp; pension i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>neutralize: And[0] the[0] heel[0] on[0] my[0] ...</td>\n",
       "      <td>And the heel on my shoe just broke. I love whe...</td>\n",
       "      <td>And the heel on my shoe just broke. I love whe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eb9a6ea7-812c-434b-b50b-69e052305bed')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-eb9a6ea7-812c-434b-b50b-69e052305bed button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-eb9a6ea7-812c-434b-b50b-69e052305bed');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-b53a0e02-24c8-423b-bcde-8033c4310fbf\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b53a0e02-24c8-423b-bcde-8033c4310fbf')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-b53a0e02-24c8-423b-bcde-8033c4310fbf button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                          input_text  \\\n",
       "0  neutralize: no[0] one[0] ever[0] predicted[0] ...   \n",
       "1  neutralize: its[0] as[0] closely[0] related[0]...   \n",
       "2  neutralize: I[0] find[0] it[0] ironic[1] when[...   \n",
       "3  neutralize: Quick[0] rt[0] that[0] throwing[0]...   \n",
       "4  neutralize: yep[1] ,[0] keep[0] adding[0] me[0...   \n",
       "5  neutralize: wonderful[1] time[0] with[0] custo...   \n",
       "6    neutralize: :[0] But[0] ...[0] football[0] .[0]   \n",
       "7  neutralize: So[0] last[0] month[0] I[0] lost[0...   \n",
       "8  neutralize: Rahm[0] has[0] solved[0] crime[0] ...   \n",
       "9  neutralize: And[0] the[0] heel[0] on[0] my[0] ...   \n",
       "\n",
       "                                         target_text  \\\n",
       "0    no one ever predicted this was going to happen.   \n",
       "1  its as closely related as Andrews original cla...   \n",
       "2    I find it ironic when Vegans say they love food   \n",
       "3  Quick rt that throwing money vine I've not see...   \n",
       "4  yep, keep adding me to your devops lists.... j...   \n",
       "5  wonderful time with customer service can't add...   \n",
       "6                                : But ... football.   \n",
       "7  So last month I lost my job and today my dad l...   \n",
       "8  Rahm has solved crime, budget &amp; pension is...   \n",
       "9  And the heel on my shoe just broke. I love whe...   \n",
       "\n",
       "                                      predicted_text  \n",
       "0    no one ever predicted this was going to happen.  \n",
       "1  its as closely related as Andrews original cla...  \n",
       "2    I find it ironic when vegans say they love food  \n",
       "3  Quick rt that throwing money vine I've not see...  \n",
       "4  yep, keep adding me to your devops lists.... j...  \n",
       "5  wonderful time with customer service ca n't ad...  \n",
       "6                                : But ... football.  \n",
       "7  So last month I lost my job and today my dad l...  \n",
       "8  Rahm has solved crime, budget & amp; pension i...  \n",
       "9  And the heel on my shoe just broke. I love whe...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Replace potential out-of-vocabulary predicted IDs with pad_token_id\n",
    "predictions.predictions[predictions.predictions >= tokenizer.vocab_size] = tokenizer.pad_token_id\n",
    "predictions.predictions[predictions.predictions < 0] = tokenizer.pad_token_id # Also handle potential negative indices\n",
    "\n",
    "preds=tokenizer.batch_decode(\n",
    "    predictions.predictions, skip_special_tokens=True\n",
    ")\n",
    "labels = tokenizer.batch_decode(\n",
    "    predictions.label_ids, skip_special_tokens=True\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "df_results = pd.DataFrame({\n",
    "    \"input_text\": dataset_test[\"train\"][\"input_text\"],\n",
    "    \"target_text\": labels,\n",
    "    \"predicted_text\": preds\n",
    "})\n",
    "df_results.to_csv(os.path.join(folder_path, \"output/tweets/predictions_testset_tweets-mpqa-flan_t5-2lossrf-11_16-wnc6000.csv\"), index=False)\n",
    "\n",
    "display(df_results.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1335
    },
    "executionInfo": {
     "elapsed": 58,
     "status": "ok",
     "timestamp": 1763396734090,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "DK0Md9VqoqPw",
    "outputId": "727bc241-dc7f-4521-e332-5cb38e8ca249"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df_results\",\n  \"rows\": 8124,\n  \"fields\": [\n    {\n      \"column\": \"input_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7927,\n        \"samples\": [\n          \"neutralize: Watch[0] Lady[0] Gaga[1] in[0] Haunting[1] New[0] 'American[0] Horror[1] Story[0] '[0] Trailer[0] :[0] Lady[0] Gaga[1] 's[0] appearance[0] in[0] the[0] trailer[0] ...[0] peace[0]\",\n          \"neutralize: I[0] am[0] shocked[0] ,[0] SHOCKED[0] ,[0] that[0] Swagger[1] ONCE[0] AGAIN[0] lost[0] to[0] Rusev[0] .[0] Who[0] .[0] Would[0] .[0] Have[0] .[0] Thought[0] .[0] smackdown[0] swaggervsrusev[0] wwe[0]\",\n          \"neutralize: Are[0] you[0] sure[1] ?[0] humor[1] realestate[0] hashtag[0]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7923,\n        \"samples\": [\n          \"concentrate on children in the foundational years. Once they are grown beyond 7 years, influence rate is reduced. education\",\n          \"I am shocked, SHOCKED, that Swagger ONCE AGAIN lost to Rusev. Who. Would. Have. Thought. smackdown swaggervsrusev wwe\",\n          \"Bird Fergusen eshumorcom humor gif\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"predicted_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7922,\n        \"samples\": [\n          \"I love being ignored!\",\n          \"I am shocked, SHOCKED, that Swagger ONCE AGAIN lost to Rusev. Who. Would. Have. Thought. smackdown swaggervsrusev\",\n          \"My spidey sense was tingling ... you guessed it, hard brake to avoid left hook. Guess what street? glasgow\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "df_results"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-5cbdc2de-102c-4681-a955-dd252858a2dd\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "      <th>predicted_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutralize: no[0] one[0] ever[0] predicted[0] ...</td>\n",
       "      <td>no one ever predicted this was going to happen.</td>\n",
       "      <td>no one ever predicted this was going to happen.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutralize: its[0] as[0] closely[0] related[0]...</td>\n",
       "      <td>its as closely related as Andrews original cla...</td>\n",
       "      <td>its as closely related as Andrews original cla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutralize: I[0] find[0] it[0] ironic[1] when[...</td>\n",
       "      <td>I find it ironic when Vegans say they love food</td>\n",
       "      <td>I find it ironic when vegans say they love food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutralize: Quick[0] rt[0] that[0] throwing[0]...</td>\n",
       "      <td>Quick rt that throwing money vine I've not see...</td>\n",
       "      <td>Quick rt that throwing money vine I've not see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutralize: yep[1] ,[0] keep[0] adding[0] me[0...</td>\n",
       "      <td>yep, keep adding me to your devops lists.... j...</td>\n",
       "      <td>yep, keep adding me to your devops lists.... j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8119</th>\n",
       "      <td>neutralize: Why[0] yes[1] I[0] will[1] totally...</td>\n",
       "      <td>Why yes I will totally submit my photos to a s...</td>\n",
       "      <td>Why yes I will submit my photos to a shitty on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8120</th>\n",
       "      <td>neutralize: Test[0] on[0] a[0] Saturday[0] ![0...</td>\n",
       "      <td>Test on a Saturday! Thank you uni! @ Griffith ...</td>\n",
       "      <td>Test on a Saturday! Thank you uni! @ Griffith ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8121</th>\n",
       "      <td>neutralize: Listening[0] to[0] 's[0] Misery[1]...</td>\n",
       "      <td>Listening to 's Misery isn't at all disconcert...</td>\n",
       "      <td>Listening to 's Misery is n't at all disconcer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8122</th>\n",
       "      <td>neutralize: There[0] you[0] go[0] being[0] kin...</td>\n",
       "      <td>There you go being kind again standup4kids</td>\n",
       "      <td>There you go being kind again standup4kids</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8123</th>\n",
       "      <td>neutralize: I[0] 'm[0] shocked[0] that[0] thes...</td>\n",
       "      <td>I'm shocked that these refs in the tcu vs minn...</td>\n",
       "      <td>I'm shocked that these refs in the tcu vs minn...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8124 rows  3 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5cbdc2de-102c-4681-a955-dd252858a2dd')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-5cbdc2de-102c-4681-a955-dd252858a2dd button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-5cbdc2de-102c-4681-a955-dd252858a2dd');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-f3d5e678-e94b-4e11-a35a-9e873748bd43\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f3d5e678-e94b-4e11-a35a-9e873748bd43')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-f3d5e678-e94b-4e11-a35a-9e873748bd43 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "  <div id=\"id_83f44862-39d1-4e5b-b279-4daa590f9064\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_results')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_83f44862-39d1-4e5b-b279-4daa590f9064 button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('df_results');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                             input_text  \\\n",
       "0     neutralize: no[0] one[0] ever[0] predicted[0] ...   \n",
       "1     neutralize: its[0] as[0] closely[0] related[0]...   \n",
       "2     neutralize: I[0] find[0] it[0] ironic[1] when[...   \n",
       "3     neutralize: Quick[0] rt[0] that[0] throwing[0]...   \n",
       "4     neutralize: yep[1] ,[0] keep[0] adding[0] me[0...   \n",
       "...                                                 ...   \n",
       "8119  neutralize: Why[0] yes[1] I[0] will[1] totally...   \n",
       "8120  neutralize: Test[0] on[0] a[0] Saturday[0] ![0...   \n",
       "8121  neutralize: Listening[0] to[0] 's[0] Misery[1]...   \n",
       "8122  neutralize: There[0] you[0] go[0] being[0] kin...   \n",
       "8123  neutralize: I[0] 'm[0] shocked[0] that[0] thes...   \n",
       "\n",
       "                                            target_text  \\\n",
       "0       no one ever predicted this was going to happen.   \n",
       "1     its as closely related as Andrews original cla...   \n",
       "2       I find it ironic when Vegans say they love food   \n",
       "3     Quick rt that throwing money vine I've not see...   \n",
       "4     yep, keep adding me to your devops lists.... j...   \n",
       "...                                                 ...   \n",
       "8119  Why yes I will totally submit my photos to a s...   \n",
       "8120  Test on a Saturday! Thank you uni! @ Griffith ...   \n",
       "8121  Listening to 's Misery isn't at all disconcert...   \n",
       "8122         There you go being kind again standup4kids   \n",
       "8123  I'm shocked that these refs in the tcu vs minn...   \n",
       "\n",
       "                                         predicted_text  \n",
       "0       no one ever predicted this was going to happen.  \n",
       "1     its as closely related as Andrews original cla...  \n",
       "2       I find it ironic when vegans say they love food  \n",
       "3     Quick rt that throwing money vine I've not see...  \n",
       "4     yep, keep adding me to your devops lists.... j...  \n",
       "...                                                 ...  \n",
       "8119  Why yes I will submit my photos to a shitty on...  \n",
       "8120  Test on a Saturday! Thank you uni! @ Griffith ...  \n",
       "8121  Listening to 's Misery is n't at all disconcer...  \n",
       "8122         There you go being kind again standup4kids  \n",
       "8123  I'm shocked that these refs in the tcu vs minn...  \n",
       "\n",
       "[8124 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434,
     "referenced_widgets": [
      "a4c6f882b3e24d2d886787165f49da74",
      "7b2dcb0a9ca24b09b5969dc66aff52b1",
      "85f6c2184b2949e083e1ef7b29771720",
      "44bf605cc82242fa9bc81c5d13f44601",
      "ffa2127ddd7e47e4ab92d3fe7345903d",
      "84f7a4a62a094d8697e92877af99ef8e",
      "72fbf7d8c57944668080102dc818ac57",
      "10ac4e23283b419b9a03f3a8f46f0eb2",
      "7e7d4dda1faf4d29b04bfe81536ac13f",
      "bd7862a7c229464cbf46b9994331806a",
      "236655b305e744a38d17b6bc89ac45ef",
      "a1a33862d7ed467180bca4b9afcc5f58",
      "188600cef0944bb0a02e2cdb0eda1f21",
      "8fcead28a2f1431bb923ebbb68115a03",
      "0b6611ef8f804cb4926eacf1e719fe95",
      "e9002886b5254eb786a2265da8590096",
      "50472c15102140c29b55048a0b02ce88",
      "2e6e009b37c84e23a79fb905006519f9",
      "0a9b204adbad4e1d907d5457193caab0",
      "a5c9932f43574514bd7cdb1fdccf67da",
      "b4964a29afd041a6a6e61c71fa9bfee3",
      "e3efd86044c74dcd8edd9932c9f64a2b",
      "ccb9c16435664590b6cb8f56b4047069",
      "3a146b212ed84b51bb3b6b978d2a58bf",
      "8d37e37957ce4891a173bde89e6b16f9",
      "2419285ec414421dbbe2a4964b180c70",
      "5832ed82d90b49c78f54f3c419f1a28a",
      "e82c551cc4b942d9ae77af6ac63559ce",
      "2ea6de672b4a4133bf85f3482842397e",
      "d62709daea2c47bcbe9ae05ef0f3a912",
      "57e09e8d1d85451eacbfe32a426f7099",
      "8eb90bf0892e40cb9b5c47ee11522cda",
      "b334b3c373484d37a28ee82030bd1078",
      "64fbfbae3a0c493da9ca7c7568bc5d1a",
      "bcd507a4c186499c94de78dfb6212696",
      "01920cac1ebe40d98bde90bfec4feba9",
      "9c931e5e49584999b6b2c3db3bf10d6c",
      "2806406def4e4d498f44fb6aee9d95f6",
      "b1798c3e869f4f6a821a9192e0786664",
      "164f378b54af46bd9bd7bc2c48e7f992",
      "bf7ca9bdcb4a40ed8731e2353c66be94",
      "380abc0092eb4ebabbe3a9840c695345",
      "02c2d8eacc5447289fd34a336ddc1bff",
      "3916013e311d4a91b70a0b943062db19",
      "d99a57b6fff049b9b251e2d1268bdf17",
      "dc2aced280584650873c1767466a306d",
      "de5e40fa137049859b884f895c0aea06",
      "623a9670e45c44f0abc2c8342aaaf163",
      "5b75a88a94d64f28a6fc464dd7322b80",
      "f576072157ef4ba799866cd60bc03b02",
      "c8fbb7336c964ac09dc001b25b78d2ce",
      "5c3e40bb909e4b309276b9bc235e1542",
      "8be499a3ae1f42e483aa0b2add2b00a0",
      "8b36f248b3404658b841c9557d923cc2",
      "41523a7b42fc4b8c9d192bebec3c1308",
      "4017097d6cbf4836897a19d7f4aff86f",
      "9ae63a008edf4361bbd8c39a0970bd2c",
      "9cea7cf2f45f4690b66f2779308bce42",
      "5822e219474a4e46a584750bb9a54014",
      "edaac1fa6a894c0eb48cbf808736245d",
      "af84263cf2c64ba2bc9f4fdbc41f453b",
      "b24dcdfb28254872a9d21fbb4225c2c8",
      "005c11b2cc59423fab3e376344613c33",
      "6769ae54a1864f4286cf58be7a192106",
      "4512bc44818f477e9936f43c7f4f03b8",
      "e8dd4ee075934a4c838805c792b8388d",
      "317fb3f5d74149b09cd635fbc1bd6302",
      "fd979112742e49c590519bf88a3700e5",
      "2ff042194cd343ecb1819907ee531967",
      "9777bd95e1514975939e8af269a5d7ba",
      "e848280153794184978daacded1e5327",
      "b49f22c174f34e6f9a2834b42d26ddda",
      "5890eeddb84141e584a051355005f5df",
      "acb48db41dde44bfbdc2f502ece268aa",
      "061d47136ac54e3bbec9825e2790f16d",
      "353fbbb877844d7d8838b1746e407e75",
      "0ef097e84fc2481fb349709e6bdcb04b"
     ]
    },
    "executionInfo": {
     "elapsed": 221162,
     "status": "ok",
     "timestamp": 1763397066186,
     "user": {
      "displayName": "Hannah Huang",
      "userId": "10695030143552154621"
     },
     "user_tz": 300
    },
    "id": "Pzi7vR7no6UX",
    "outputId": "6a26f5c8-d8bb-4fec-82bc-0dab84bc6618"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4c6f882b3e24d2d886787165f49da74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/412 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1a33862d7ed467180bca4b9afcc5f58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb9c16435664590b6cb8f56b4047069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/16.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64fbfbae3a0c493da9ca7c7568bc5d1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/23.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d99a57b6fff049b9b251e2d1268bdf17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/173 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4017097d6cbf4836897a19d7f4aff86f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/881 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "317fb3f5d74149b09cd635fbc1bd6302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean objectivity score: 0.40405376168937296\n",
      "std: 0.24022559221199472\n",
      "total percent of objective sentences: 0.2917282127031019\n"
     ]
    }
   ],
   "source": [
    "# @title objectivity evaluation\n",
    "import numpy as np\n",
    "clf_name = \"GroNLP/mdebertav3-subjectivity-english\"\n",
    "clf_tok = AutoTokenizer.from_pretrained(clf_name)\n",
    "clf = AutoModelForSequenceClassification.from_pretrained(clf_name).to(trained_model.device).eval()\n",
    "\n",
    "# predicted text\n",
    "path=os.path.join(folder_path, \"output/tweets/predictions_testset_tweets-mpqa-flan_t5-2lossrf-11_16-wnc6000.csv\")\n",
    "df=pd.read_csv(path)\n",
    "\n",
    "# mismatches_df = df[df['predicted_text'] != df['target_text']]\n",
    "# display(mismatches_df)\n",
    "scores = []\n",
    "count_objective=0\n",
    "for text in df[\"predicted_text\"]:\n",
    "    tokens = clf_tok(text, return_tensors=\"pt\", truncation=True).to(trained_model.device)\n",
    "    with torch.no_grad():\n",
    "        probs = clf(**tokens).logits.softmax(dim=-1)\n",
    "        p_objective = probs[0][0].item()  # index 0 = objective\n",
    "        if p_objective>0.5:\n",
    "          count_objective+=1\n",
    "\n",
    "    scores.append(p_objective)\n",
    "\n",
    "print(\"Mean objectivity score:\", np.mean(scores))\n",
    "print(\"std:\", np.std(scores))\n",
    "print(\"total percent of objective sentences:\", count_objective/len(scores))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMH6Xw0Rv831qJO7Au0rICu",
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1L-SkA_1RjtNU_47CdDIYtTwB_o7V_Ptk",
     "timestamp": 1763430776646
    },
    {
     "file_id": "1dAVvsVT3r3argz3zcemfHEizj81SBUYE",
     "timestamp": 1763416832867
    },
    {
     "file_id": "1WiRdoNVbcVu6Phlr5O7S-Fh7C5HAPcPM",
     "timestamp": 1763398953826
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "005c11b2cc59423fab3e376344613c33": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "01920cac1ebe40d98bde90bfec4feba9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bf7ca9bdcb4a40ed8731e2353c66be94",
      "max": 23,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_380abc0092eb4ebabbe3a9840c695345",
      "value": 23
     }
    },
    "01950bc3bd0a4de08630aeda16c95f9a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c8e21ff9da4b431da32866a33acd8b16",
      "placeholder": "",
      "style": "IPY_MODEL_82c72b74d13c401694edd910d86db0e4",
      "value": "2.54k/?[00:00&lt;00:00,253kB/s]"
     }
    },
    "02857d4860ae4e55ae0b76b4b514987c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "02c2d8eacc5447289fd34a336ddc1bff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "03402f1e9e3e47fdaecf832ccb7e9c95": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "04aeed22e5db4250a6f5f9065a2c5987": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "05c57397d7594f50939907ff0391be38": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "061d47136ac54e3bbec9825e2790f16d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "06515260f42e40d78b7fe031066b7314": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "06c14377b1624679a17e57248c8be499": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0793cea924de49c99aa39bda433510f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0a079e8f9bdc486eb34001fb90c5c964": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0a9b204adbad4e1d907d5457193caab0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0b6611ef8f804cb4926eacf1e719fe95": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b4964a29afd041a6a6e61c71fa9bfee3",
      "placeholder": "",
      "style": "IPY_MODEL_e3efd86044c74dcd8edd9932c9f64a2b",
      "value": "4.31M/4.31M[00:02&lt;00:00,1.92MB/s]"
     }
    },
    "0b7922fe580849059b8ae91f287981c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0df27d8087984b58883db1bce6bdb24a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c493c31533a14204a6edc757ce2e0c06",
      "placeholder": "",
      "style": "IPY_MODEL_56a48431e4c541d2b7fe0c47f8a03c25",
      "value": "special_tokens_map.json:"
     }
    },
    "0e59e28f31034d9ea5161db8c4b3eee3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0ef097e84fc2481fb349709e6bdcb04b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "100393dabcac4e608d69753bffac2257": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "10ac4e23283b419b9a03f3a8f46f0eb2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "11ea0f8322594bdaa4ef72c03e6ded87": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "14c2b2f465f647b4b5cd912462a67162": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "164f378b54af46bd9bd7bc2c48e7f992": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "16a72e2157084a3d9eb25dc2a3bc0c5b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "16cbf39f36f84d0a994baa4de3dd342b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "17e12a31f2e64de1a588af29004bcc8f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f5da34916279459bad312762d416c9c4",
      "max": 8124,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_47f916fd5bf74379829652b1307c3244",
      "value": 8124
     }
    },
    "188600cef0944bb0a02e2cdb0eda1f21": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_50472c15102140c29b55048a0b02ce88",
      "placeholder": "",
      "style": "IPY_MODEL_2e6e009b37c84e23a79fb905006519f9",
      "value": "spm.model:100%"
     }
    },
    "19722962d598405482928975278cad5b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_16cbf39f36f84d0a994baa4de3dd342b",
      "placeholder": "",
      "style": "IPY_MODEL_a401e116741c4ec097ec55779b795da9",
      "value": "81382/81382[00:16&lt;00:00,5183.68examples/s]"
     }
    },
    "1b5c7716a9844ae0964f9ec264b7c0e0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b97ed39453c48edba49ceaa2ea082f2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1c2e7d47491f4bb99a39a85a0dbdd7f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1c84ae399704403b8308b241687f977b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2c44cb86a4dd4eb19930e6470fe89444",
      "max": 16331564,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_20b7d1d556614dcd9e20055f34a8c778",
      "value": 16331564
     }
    },
    "20b7d1d556614dcd9e20055f34a8c778": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2258518d213342258ea5f78d8e05c1c1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "236655b305e744a38d17b6bc89ac45ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2419285ec414421dbbe2a4964b180c70": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8eb90bf0892e40cb9b5c47ee11522cda",
      "placeholder": "",
      "style": "IPY_MODEL_b334b3c373484d37a28ee82030bd1078",
      "value": "16.3M/16.3M[00:02&lt;00:00,7.68MB/s]"
     }
    },
    "24a7efe1bc0b497380f0df58f1798b54": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "2619875c75d64ea8ab1fecb4db03b4fa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2806406def4e4d498f44fb6aee9d95f6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2a671b26309f4efeb0abe8a3af67621d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9f9b18307012407b8188550ab3ca8189",
      "placeholder": "",
      "style": "IPY_MODEL_41603d97e2f94ba7a984354ad33f9975",
      "value": "2.20k/?[00:00&lt;00:00,225kB/s]"
     }
    },
    "2aae7c78807d46ae97d90f56bd9acd4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2c2a9f2220f1411b8bf2b03e6f851ac2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2c44cb86a4dd4eb19930e6470fe89444": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2c561bc4f4954f988d95960b1d15dd04": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9e25167ed313465fb5b63a3498c18c18",
       "IPY_MODEL_4bbd988667234db8aa5339ea276ae55b",
       "IPY_MODEL_7aa582ee9421467db98487fe678dbbfe"
      ],
      "layout": "IPY_MODEL_4892a74630454f76bcc88f8abb03fe47"
     }
    },
    "2ccf92e203c94c9f887936eead6efac7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_507437e7f77b45b08c1a5bc00e90c79e",
       "IPY_MODEL_9ee76951e8d44cae8ef9b57b8c91a7df",
       "IPY_MODEL_3adcd88a32914d369b7c80b484836fe5"
      ],
      "layout": "IPY_MODEL_959c8398f4c54e6b8b60b83eb8cca2a4"
     }
    },
    "2d22d785764c47568de236732523490b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2d5190bc9a27473f97636e25577d93a5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2ddf0e585b0a49269f7e1c7f2ccd152c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2ddfe90f870c4a12a036ce65041dc360": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_855d8d32957e462da4176ab721965b78",
       "IPY_MODEL_860e72673b2942c3b7995ffc835228dc",
       "IPY_MODEL_2a671b26309f4efeb0abe8a3af67621d"
      ],
      "layout": "IPY_MODEL_91d6fd92c1ca4fa59d4e5db4dcdefa61"
     }
    },
    "2e6316c662bd428f9b733cbfe5aa4b9c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_92845f841c634c8a934513521a8d2ad7",
      "max": 81382,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bf5a5b26b150489db52645048f1a7685",
      "value": 81382
     }
    },
    "2e6e009b37c84e23a79fb905006519f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2ea6de672b4a4133bf85f3482842397e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2f14ce19688f4a608ad348477a092947": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e6033e5ffd4f43dd8906015aabf5657c",
      "placeholder": "",
      "style": "IPY_MODEL_39c203075e1b4b06b389c98b7c88aff2",
      "value": "tokenizer.json:100%"
     }
    },
    "2f7989bb88e14e239521a89cdab46ff3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2f7ec7e46a924843b84f2c7615fd4139": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c35651cd316046f8bd8665f90cbfee76",
      "placeholder": "",
      "style": "IPY_MODEL_78a39a9d1ea24eba88fd0909273100ca",
      "value": "Map:100%"
     }
    },
    "2f912d085af142a1930d92544443ac24": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2ff042194cd343ecb1819907ee531967": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_acb48db41dde44bfbdc2f502ece268aa",
      "max": 1112513736,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_061d47136ac54e3bbec9825e2790f16d",
      "value": 1112513736
     }
    },
    "308ed0c4bd894ed28705d6ba49249263": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "317fb3f5d74149b09cd635fbc1bd6302": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fd979112742e49c590519bf88a3700e5",
       "IPY_MODEL_2ff042194cd343ecb1819907ee531967",
       "IPY_MODEL_9777bd95e1514975939e8af269a5d7ba"
      ],
      "layout": "IPY_MODEL_e848280153794184978daacded1e5327"
     }
    },
    "32f171f700be4dbbb95619e5e9d1e889": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "330565840a77488cb67341b02905d37a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_334b55d7160342298363e9a28f2fcb84",
       "IPY_MODEL_9d07a07ca29c405c95332524cf1571ac",
       "IPY_MODEL_d8c73ca29dca4e3da889198fe0ba7831"
      ],
      "layout": "IPY_MODEL_2c2a9f2220f1411b8bf2b03e6f851ac2"
     }
    },
    "334b55d7160342298363e9a28f2fcb84": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ac9247ed2a5643229573a85a4343c119",
      "placeholder": "",
      "style": "IPY_MODEL_06515260f42e40d78b7fe031066b7314",
      "value": "tokenizer_config.json:100%"
     }
    },
    "34ac89955fc1450c8f321844cb1289ed": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "353fbbb877844d7d8838b1746e407e75": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "35d675dc62ee47f5b6f49ab37c4103ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_63fede34b64c4444a80da131daa85877",
      "max": 53803,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f3f8ea85f0654d33b73de468a9bbef17",
      "value": 53803
     }
    },
    "3640ae7ec2b34e0f8087ad42a9ecf1ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "37024a405a074bc8acf8fa5f5282a7f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_52994d18207b47f68e4d0d27c81551f8",
      "max": 81382,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4cf26fcc9459491b9416ad4ca222556c",
      "value": 81382
     }
    },
    "37a5e6c415f34650bbb0d57e176f1db6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "37b2396a986c41dc993c20ecf484bdff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4a2bd8d846104eb7b8aab304d0f77a73",
      "placeholder": "",
      "style": "IPY_MODEL_adbc2faeb9494f83be702f45ce85c7cc",
      "value": "Map:100%"
     }
    },
    "380abc0092eb4ebabbe3a9840c695345": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "38ce96b9dbe94ff7a8295dffca552ff3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_05c57397d7594f50939907ff0391be38",
      "placeholder": "",
      "style": "IPY_MODEL_03402f1e9e3e47fdaecf832ccb7e9c95",
      "value": "8124/0[00:01&lt;00:00,6147.34examples/s]"
     }
    },
    "3916013e311d4a91b70a0b943062db19": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "393bfefc676c48ad9bc76cb892eb20c3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "39c203075e1b4b06b389c98b7c88aff2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "39c2066f8e6146969c30a22deb18b84d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3a146b212ed84b51bb3b6b978d2a58bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e82c551cc4b942d9ae77af6ac63559ce",
      "placeholder": "",
      "style": "IPY_MODEL_2ea6de672b4a4133bf85f3482842397e",
      "value": "tokenizer.json:100%"
     }
    },
    "3adcd88a32914d369b7c80b484836fe5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_14c2b2f465f647b4b5cd912462a67162",
      "placeholder": "",
      "style": "IPY_MODEL_ec7a896e91c64eb19fb025c51bd52aee",
      "value": "173/173[00:00&lt;00:00,23.0kB/s]"
     }
    },
    "3cbf3e6b7ce548109c383fa2be7d0ece": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0df27d8087984b58883db1bce6bdb24a",
       "IPY_MODEL_c661b5b6e80a4547b9358620141d356b",
       "IPY_MODEL_5c27f5c86a55417dbc599e81a3b8aa31"
      ],
      "layout": "IPY_MODEL_2d5190bc9a27473f97636e25577d93a5"
     }
    },
    "3dfd3f2b375e45b5b0e9f75eb5d7a105": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6c314a90029440bebc7a36aa7f63a2a7",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6f575cc95c3c4724a52a938b409dc4bc",
      "value": 1
     }
    },
    "3e5556b94a6349e1862e6ae568ae7480": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "400bc57d8b8b45e582b19dd974cffb37": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4017097d6cbf4836897a19d7f4aff86f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9ae63a008edf4361bbd8c39a0970bd2c",
       "IPY_MODEL_9cea7cf2f45f4690b66f2779308bce42",
       "IPY_MODEL_5822e219474a4e46a584750bb9a54014"
      ],
      "layout": "IPY_MODEL_edaac1fa6a894c0eb48cbf808736245d"
     }
    },
    "40fddb9cbf1842638691e9294ab42ed6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "41523a7b42fc4b8c9d192bebec3c1308": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "41603d97e2f94ba7a984354ad33f9975": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "41c2ab2ac1074d168301ed892df14ace": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "42ae9d039f0a413c94c7980c330940e7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b3fb94d69b5e4f649fb779c4c2ebad92",
      "placeholder": "",
      "style": "IPY_MODEL_be5051318cba4400ac017516409c2df7",
      "value": "Generatingtrainsplit:"
     }
    },
    "43587b1be55d444e9c39dfc32f07124a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5979af5630e14f7e9d697dfab810aeea",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ef1477585cf8432ea25696a87cf9612c",
      "value": 1
     }
    },
    "44bf605cc82242fa9bc81c5d13f44601": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bd7862a7c229464cbf46b9994331806a",
      "placeholder": "",
      "style": "IPY_MODEL_236655b305e744a38d17b6bc89ac45ef",
      "value": "412/412[00:00&lt;00:00,56.0kB/s]"
     }
    },
    "4512bc44818f477e9936f43c7f4f03b8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "45f4dad1bfb94dcfbd29a69b869c0864": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "47f916fd5bf74379829652b1307c3244": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4892a74630454f76bcc88f8abb03fe47": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "48f23b89cc7e489887193874080d4216": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1b97ed39453c48edba49ceaa2ea082f2",
      "placeholder": "",
      "style": "IPY_MODEL_6341f62e59794ed49e6de95a585f7738",
      "value": "Map:100%"
     }
    },
    "4a10b2a54ae0494b9d68e64277e26cd1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4a2bd8d846104eb7b8aab304d0f77a73": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4bbd988667234db8aa5339ea276ae55b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4a10b2a54ae0494b9d68e64277e26cd1",
      "max": 791656,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a941b6381a14462886456a69b2df1226",
      "value": 791656
     }
    },
    "4ce6e1d7e8df450fbd586387fc19da8d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_58daa4586cdb4208b2f1fde49902f160",
       "IPY_MODEL_37024a405a074bc8acf8fa5f5282a7f7",
       "IPY_MODEL_c4296ab1bfe04b11a6199c387751859f"
      ],
      "layout": "IPY_MODEL_393bfefc676c48ad9bc76cb892eb20c3"
     }
    },
    "4cf26fcc9459491b9416ad4ca222556c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "50472c15102140c29b55048a0b02ce88": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "507437e7f77b45b08c1a5bc00e90c79e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_308ed0c4bd894ed28705d6ba49249263",
      "placeholder": "",
      "style": "IPY_MODEL_cb26e43816114201b9b1a744042cf613",
      "value": "special_tokens_map.json:100%"
     }
    },
    "50fa33f38aa64171aa7584c2835ffdce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "517217c05785432f8edfa4220b2623c7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5193b7782e04421585ab62e6525eec97": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_61562e24cca94e4a9ca4b86c318b0f0b",
      "placeholder": "",
      "style": "IPY_MODEL_45f4dad1bfb94dcfbd29a69b869c0864",
      "value": "tokenizer_config.json:"
     }
    },
    "52994d18207b47f68e4d0d27c81551f8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "52ba9b011df94da08c4b60f37c228b4c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "52eb91254f8746e4808d52fcd57d44f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "530d3effe9514d8c8fda84074f71d674": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "532c763831fb4aad9de2e6b310ceeacc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fb1a8c27574d4fa0ba775aaab5688d98",
      "placeholder": "",
      "style": "IPY_MODEL_3640ae7ec2b34e0f8087ad42a9ecf1ee",
      "value": "16.3M/16.3M[00:00&lt;00:00,20.5MB/s]"
     }
    },
    "53b0af708f214a9fba58ca71bffab48d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_37b2396a986c41dc993c20ecf484bdff",
       "IPY_MODEL_65311a5a4c2b4ebf88971ca3b1ff4c2a",
       "IPY_MODEL_ec2991ffa0b2447ebef695409e3bbb06"
      ],
      "layout": "IPY_MODEL_794f1bef6a2741068c3878dfb9161a39"
     }
    },
    "54ef07bdd9684240be90db973e3439ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "55ac48e549534af08d774986eeab522e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5960c93f7b0d4c6787f965f7107ff93f",
       "IPY_MODEL_35d675dc62ee47f5b6f49ab37c4103ed",
       "IPY_MODEL_afb829b5c7c442528ab2c888ab7c4680"
      ],
      "layout": "IPY_MODEL_af9b3d1bb4eb43d581ba72bc65deb03a"
     }
    },
    "563c21d3975b406ebb20557235a121c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3e5556b94a6349e1862e6ae568ae7480",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2d22d785764c47568de236732523490b",
      "value": 1
     }
    },
    "56a48431e4c541d2b7fe0c47f8a03c25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "56cc9319549f4e4d8d771ce54df19483": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8f33e346105f4250b15b3bf0700e37e5",
       "IPY_MODEL_816123a1d6394df0b25c1e15e223142b",
       "IPY_MODEL_88365c36fd76430aa19bf68b2a4f7fdd"
      ],
      "layout": "IPY_MODEL_6217095e97314cffab56e22676c0bf96"
     }
    },
    "57d5995774f04cd18d4732dfde5c8477": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "57e09e8d1d85451eacbfe32a426f7099": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5822e219474a4e46a584750bb9a54014": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4512bc44818f477e9936f43c7f4f03b8",
      "placeholder": "",
      "style": "IPY_MODEL_e8dd4ee075934a4c838805c792b8388d",
      "value": "881/881[00:00&lt;00:00,109kB/s]"
     }
    },
    "5832ed82d90b49c78f54f3c419f1a28a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5853b4dcee0f4b06a5246c8e8c77d61b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5890eeddb84141e584a051355005f5df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "58daa4586cdb4208b2f1fde49902f160": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e6613567a67d487a8185cb8339fa7718",
      "placeholder": "",
      "style": "IPY_MODEL_aa460ec2afc34bce87fc59e579df46d9",
      "value": "Map:100%"
     }
    },
    "5960c93f7b0d4c6787f965f7107ff93f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f4394ab82f354fa4a9bbacfd160e8cb7",
      "placeholder": "",
      "style": "IPY_MODEL_d83c7b46881a4f25b9dc9b87eb71a88d",
      "value": "Map:100%"
     }
    },
    "5979af5630e14f7e9d697dfab810aeea": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "59f0f24f337948e29e64927b99f6ffd1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a024042a937346688ca077f96c03976c",
      "max": 1000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7a93bcf9c5944603bd37d292218f8778",
      "value": 1000
     }
    },
    "5b75a88a94d64f28a6fc464dd7322b80": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5c27f5c86a55417dbc599e81a3b8aa31": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6d40ec6e27c040be9f77d6c2725f23e9",
      "placeholder": "",
      "style": "IPY_MODEL_6d66a7c5716e4b57b07cd90c3cfb66cf",
      "value": "2.20k/?[00:00&lt;00:00,219kB/s]"
     }
    },
    "5c3e40bb909e4b309276b9bc235e1542": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5cf313235acc40a58c2010bc07300b58": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_809aeb4e2ac743e79cbf55d90da5837f",
       "IPY_MODEL_67275b0cb4b54ad5bb77dc05bb44c2f8",
       "IPY_MODEL_878796fe91084abeb9059b25b6d2d002"
      ],
      "layout": "IPY_MODEL_80e20344a8a5404a8cf73081ecf08fb9"
     }
    },
    "5e3ba0c221f34110b1d6aaa6e73551fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f071980ea3c948a6b0a14054d8929b1b",
       "IPY_MODEL_563c21d3975b406ebb20557235a121c8",
       "IPY_MODEL_feca801cb60a45e08d1183d6722ab72f"
      ],
      "layout": "IPY_MODEL_6d6494437590447e94f8d6ea427763ec"
     }
    },
    "5e694842e9b3465c80938d0b6064ac7e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5eba54199b7b46038fb1a6008c3c77fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_530d3effe9514d8c8fda84074f71d674",
      "placeholder": "",
      "style": "IPY_MODEL_0e59e28f31034d9ea5161db8c4b3eee3",
      "value": "Generatingtrainsplit:"
     }
    },
    "61562e24cca94e4a9ca4b86c318b0f0b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6217095e97314cffab56e22676c0bf96": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "623a9670e45c44f0abc2c8342aaaf163": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8b36f248b3404658b841c9557d923cc2",
      "placeholder": "",
      "style": "IPY_MODEL_41523a7b42fc4b8c9d192bebec3c1308",
      "value": "173/173[00:00&lt;00:00,21.7kB/s]"
     }
    },
    "62980aa1a2f34bc398649d4af689bf2c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_32f171f700be4dbbb95619e5e9d1e889",
      "placeholder": "",
      "style": "IPY_MODEL_8e7d3cafc5ee45399f9b98d3a2164124",
      "value": "spiece.model:100%"
     }
    },
    "63123de74eda4806a2141a0592dce588": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_100393dabcac4e608d69753bffac2257",
      "placeholder": "",
      "style": "IPY_MODEL_ad11629f387047b4a4b60d46e9f21582",
      "value": "2.54k/?[00:00&lt;00:00,256kB/s]"
     }
    },
    "6341f62e59794ed49e6de95a585f7738": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "63fede34b64c4444a80da131daa85877": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "640a14f4409e4611b77d784f338fa648": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ec66a4eda9774f80be9c716ab6a282b9",
       "IPY_MODEL_d7aa12e1a6e74a9dad9a67bf614b2084",
       "IPY_MODEL_f78c5037438a4940b72dd463f2f97cc2"
      ],
      "layout": "IPY_MODEL_1b5c7716a9844ae0964f9ec264b7c0e0"
     }
    },
    "64bbfd1afe8442efad81c2d55c04bf4f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d4b78cc5fe9c4353bde17035673b5566",
       "IPY_MODEL_f2da3245ca7348ad9bba5b6cdc2a8a74",
       "IPY_MODEL_d1bedec004b84a7e86ba3119365798c3"
      ],
      "layout": "IPY_MODEL_39c2066f8e6146969c30a22deb18b84d"
     }
    },
    "64fbfbae3a0c493da9ca7c7568bc5d1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bcd507a4c186499c94de78dfb6212696",
       "IPY_MODEL_01920cac1ebe40d98bde90bfec4feba9",
       "IPY_MODEL_9c931e5e49584999b6b2c3db3bf10d6c"
      ],
      "layout": "IPY_MODEL_2806406def4e4d498f44fb6aee9d95f6"
     }
    },
    "65311a5a4c2b4ebf88971ca3b1ff4c2a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c9a08413736d4689b6fc2391eb4bf7dc",
      "max": 8124,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_54ef07bdd9684240be90db973e3439ef",
      "value": 8124
     }
    },
    "65f10e5c3c5746a5a2486316189c9b87": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "67275b0cb4b54ad5bb77dc05bb44c2f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0a079e8f9bdc486eb34001fb90c5c964",
      "max": 881,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2ddf0e585b0a49269f7e1c7f2ccd152c",
      "value": 881
     }
    },
    "6769ae54a1864f4286cf58be7a192106": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "697189ee43704e57ba015ff4466432d8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "69ebd0e06e7d46d08d8f033c2721eeb0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6a9085093bb6426d9633252e3f7eaa36": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c9cb9441676140aba7849967d2e3f74b",
      "placeholder": "",
      "style": "IPY_MODEL_f772240ae2404a66882307e96f9cc286",
      "value": "81382/81382[00:12&lt;00:00,7173.08examples/s]"
     }
    },
    "6c314a90029440bebc7a36aa7f63a2a7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "6c37466dd08c4e069fac4dad0759c44d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6c3aaa63ac234eed91b8bbfc47340fe7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6d40ec6e27c040be9f77d6c2725f23e9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6d6494437590447e94f8d6ea427763ec": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6d66a7c5716e4b57b07cd90c3cfb66cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6f575cc95c3c4724a52a938b409dc4bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6f76f5cc6da7476e97290f197978bbf6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6fc3cdf4bfa4466eb871d728ae6a1a9f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "72fbf7d8c57944668080102dc818ac57": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "77463bb1c1b74777b29b3a07cec07675": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_02857d4860ae4e55ae0b76b4b514987c",
      "placeholder": "",
      "style": "IPY_MODEL_a718fd6162524e1ca3c8ce2489bceed8",
      "value": "tokenizer_config.json:"
     }
    },
    "78a39a9d1ea24eba88fd0909273100ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "794f1bef6a2741068c3878dfb9161a39": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7a729bba8cec41a8939c01e74e3417cb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7a93bcf9c5944603bd37d292218f8778": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7aa582ee9421467db98487fe678dbbfe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_970755e5da1b4f109959982044dc41e8",
      "placeholder": "",
      "style": "IPY_MODEL_a077d12a2065457c9b70d37715c4b29c",
      "value": "792k/792k[00:00&lt;00:00,1.60MB/s]"
     }
    },
    "7ac5008f255641dc8fc5c26a752467b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7b2dcb0a9ca24b09b5969dc66aff52b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_84f7a4a62a094d8697e92877af99ef8e",
      "placeholder": "",
      "style": "IPY_MODEL_72fbf7d8c57944668080102dc818ac57",
      "value": "tokenizer_config.json:100%"
     }
    },
    "7b6323389024427a8817d365d5307315": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2619875c75d64ea8ab1fecb4db03b4fa",
      "placeholder": "",
      "style": "IPY_MODEL_7b8f7726e22b4d7c83ca959e6707641d",
      "value": "53803/0[00:01&lt;00:00,56404.52examples/s]"
     }
    },
    "7b8f7726e22b4d7c83ca959e6707641d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7bcdb3cbc63749649ad531b769605009": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7d6038359c2f4a6599d63688f1e07c38": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7e7d4dda1faf4d29b04bfe81536ac13f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "809aeb4e2ac743e79cbf55d90da5837f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_04aeed22e5db4250a6f5f9065a2c5987",
      "placeholder": "",
      "style": "IPY_MODEL_5e694842e9b3465c80938d0b6064ac7e",
      "value": "config.json:100%"
     }
    },
    "80e20344a8a5404a8cf73081ecf08fb9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "816123a1d6394df0b25c1e15e223142b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d0b1ccbf04c244199b5fb2d68d7baf44",
      "max": 4305025,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9637af73dfed47b5a2d822a531785dc7",
      "value": 4305025
     }
    },
    "8189b59d902a4b5a8fa04bc164f5de1c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2f14ce19688f4a608ad348477a092947",
       "IPY_MODEL_1c84ae399704403b8308b241687f977b",
       "IPY_MODEL_532c763831fb4aad9de2e6b310ceeacc"
      ],
      "layout": "IPY_MODEL_b7593ff51a4e4c199afb6f509833c591"
     }
    },
    "8229cf259a47476e8a98e558b80e5d9e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "822f6cb310144f8cae38cbe0dd126142": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "82c72b74d13c401694edd910d86db0e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "84334b32d531485baa04537cdcc837fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2258518d213342258ea5f78d8e05c1c1",
      "placeholder": "",
      "style": "IPY_MODEL_e896481db0674ebd8ea69c17b6c1f8a6",
      "value": "Map:100%"
     }
    },
    "84f7a4a62a094d8697e92877af99ef8e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "855d8d32957e462da4176ab721965b78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_65f10e5c3c5746a5a2486316189c9b87",
      "placeholder": "",
      "style": "IPY_MODEL_d9273cbf5fb44a9b997d21876ac65da3",
      "value": "special_tokens_map.json:"
     }
    },
    "85f6c2184b2949e083e1ef7b29771720": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_10ac4e23283b419b9a03f3a8f46f0eb2",
      "max": 412,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7e7d4dda1faf4d29b04bfe81536ac13f",
      "value": 412
     }
    },
    "860e72673b2942c3b7995ffc835228dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_90d536cee008470eb133c4449c79dfc2",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_52eb91254f8746e4808d52fcd57d44f3",
      "value": 1
     }
    },
    "873469a2c0a54c5fad999326c3ca2392": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "878796fe91084abeb9059b25b6d2d002": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_52ba9b011df94da08c4b60f37c228b4c",
      "placeholder": "",
      "style": "IPY_MODEL_cb00b67cd3394338940a4630df671f90",
      "value": "881/881[00:00&lt;00:00,111kB/s]"
     }
    },
    "87c3f45b89ed46758dfc338d2403724d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "881a33352aa7466e97cd9071633fa330": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "88365c36fd76430aa19bf68b2a4f7fdd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f65df6ae10ab46c8895b6e033cc0a3e9",
      "placeholder": "",
      "style": "IPY_MODEL_5853b4dcee0f4b06a5246c8e8c77d61b",
      "value": "4.31M/4.31M[00:01&lt;00:00,2.78MB/s]"
     }
    },
    "891ccf2daed244639133ebc7eb386f9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_77463bb1c1b74777b29b3a07cec07675",
       "IPY_MODEL_43587b1be55d444e9c39dfc32f07124a",
       "IPY_MODEL_01950bc3bd0a4de08630aeda16c95f9a"
      ],
      "layout": "IPY_MODEL_b452f3b15b2141798f8e6f0e8b54c82d"
     }
    },
    "89ba717e73134964aa5359b627f40711": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bff9d52432f2470b8f2fc93404dd641a",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_37a5e6c415f34650bbb0d57e176f1db6",
      "value": 1
     }
    },
    "8b36f248b3404658b841c9557d923cc2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8bd0b3b56c404af9893293abbe91e6ac": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8be499a3ae1f42e483aa0b2add2b00a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8d37e37957ce4891a173bde89e6b16f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d62709daea2c47bcbe9ae05ef0f3a912",
      "max": 16331564,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_57e09e8d1d85451eacbfe32a426f7099",
      "value": 16331564
     }
    },
    "8e43a9e9a43d4ae29c509701386fb25d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8e7d3cafc5ee45399f9b98d3a2164124": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8eb90bf0892e40cb9b5c47ee11522cda": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8f33e346105f4250b15b3bf0700e37e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f09b46772aca4927a71c74439a6bbdf0",
      "placeholder": "",
      "style": "IPY_MODEL_2aae7c78807d46ae97d90f56bd9acd4a",
      "value": "spm.model:100%"
     }
    },
    "8f5cc81e85354d08855771a58001210b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8fcead28a2f1431bb923ebbb68115a03": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0a9b204adbad4e1d907d5457193caab0",
      "max": 4305025,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a5c9932f43574514bd7cdb1fdccf67da",
      "value": 4305025
     }
    },
    "90d536cee008470eb133c4449c79dfc2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "91d6fd92c1ca4fa59d4e5db4dcdefa61": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "92845f841c634c8a934513521a8d2ad7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9389e3244a3842f9ba0319bf58013456": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "959c8398f4c54e6b8b60b83eb8cca2a4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "959f27fbb9b242c284ced60d5d8900b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9637af73dfed47b5a2d822a531785dc7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "970755e5da1b4f109959982044dc41e8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9777bd95e1514975939e8af269a5d7ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_353fbbb877844d7d8838b1746e407e75",
      "placeholder": "",
      "style": "IPY_MODEL_0ef097e84fc2481fb349709e6bdcb04b",
      "value": "1.11G/1.11G[00:21&lt;00:00,245MB/s]"
     }
    },
    "97d0135de8b34f138567daf1cc6811c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "998006532d4d49b29a940c287e4a98d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2f7ec7e46a924843b84f2c7615fd4139",
       "IPY_MODEL_ff3c8da9641f49678a34289a9923f4a3",
       "IPY_MODEL_6a9085093bb6426d9633252e3f7eaa36"
      ],
      "layout": "IPY_MODEL_c258279eab47410ba683a41f9ca16484"
     }
    },
    "9ae63a008edf4361bbd8c39a0970bd2c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_af84263cf2c64ba2bc9f4fdbc41f453b",
      "placeholder": "",
      "style": "IPY_MODEL_b24dcdfb28254872a9d21fbb4225c2c8",
      "value": "config.json:100%"
     }
    },
    "9ba6b10842e94e9d8ab5876d941883b3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9c931e5e49584999b6b2c3db3bf10d6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_02c2d8eacc5447289fd34a336ddc1bff",
      "placeholder": "",
      "style": "IPY_MODEL_3916013e311d4a91b70a0b943062db19",
      "value": "23.0/23.0[00:00&lt;00:00,3.15kB/s]"
     }
    },
    "9cea7cf2f45f4690b66f2779308bce42": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_005c11b2cc59423fab3e376344613c33",
      "max": 881,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6769ae54a1864f4286cf58be7a192106",
      "value": 881
     }
    },
    "9d07a07ca29c405c95332524cf1571ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9389e3244a3842f9ba0319bf58013456",
      "max": 412,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0b7922fe580849059b8ae91f287981c7",
      "value": 412
     }
    },
    "9dcfc5d4066045e8958863d716241ac2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "9e25167ed313465fb5b63a3498c18c18": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f09aa5b5ebe4413cab9f4038e16a6798",
      "placeholder": "",
      "style": "IPY_MODEL_fc2afb9f5b04420e8c4caecc501bdfea",
      "value": "spiece.model:100%"
     }
    },
    "9ee76951e8d44cae8ef9b57b8c91a7df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6fc3cdf4bfa4466eb871d728ae6a1a9f",
      "max": 173,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cb458d6e35b34db8950afa2868aca009",
      "value": 173
     }
    },
    "9f9b18307012407b8188550ab3ca8189": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a024042a937346688ca077f96c03976c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a077d12a2065457c9b70d37715c4b29c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a1a33862d7ed467180bca4b9afcc5f58": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_188600cef0944bb0a02e2cdb0eda1f21",
       "IPY_MODEL_8fcead28a2f1431bb923ebbb68115a03",
       "IPY_MODEL_0b6611ef8f804cb4926eacf1e719fe95"
      ],
      "layout": "IPY_MODEL_e9002886b5254eb786a2265da8590096"
     }
    },
    "a401e116741c4ec097ec55779b795da9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a4c6f882b3e24d2d886787165f49da74": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7b2dcb0a9ca24b09b5969dc66aff52b1",
       "IPY_MODEL_85f6c2184b2949e083e1ef7b29771720",
       "IPY_MODEL_44bf605cc82242fa9bc81c5d13f44601"
      ],
      "layout": "IPY_MODEL_ffa2127ddd7e47e4ab92d3fe7345903d"
     }
    },
    "a5449cf8527b4becaf39aa6feccafd66": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a5c9932f43574514bd7cdb1fdccf67da": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a718fd6162524e1ca3c8ce2489bceed8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a941b6381a14462886456a69b2df1226": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "aa460ec2afc34bce87fc59e579df46d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ac795aea753540c1a81cadce33dceba2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ac9247ed2a5643229573a85a4343c119": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "acb48db41dde44bfbdc2f502ece268aa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ad11629f387047b4a4b60d46e9f21582": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "adbc2faeb9494f83be702f45ce85c7cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ae456161b1f04b11a1bd48765661e93c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "af84263cf2c64ba2bc9f4fdbc41f453b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af9b3d1bb4eb43d581ba72bc65deb03a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "afb829b5c7c442528ab2c888ab7c4680": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2f912d085af142a1930d92544443ac24",
      "placeholder": "",
      "style": "IPY_MODEL_6f76f5cc6da7476e97290f197978bbf6",
      "value": "53803/53803[00:08&lt;00:00,5353.17examples/s]"
     }
    },
    "b1798c3e869f4f6a821a9192e0786664": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b24dcdfb28254872a9d21fbb4225c2c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b334b3c373484d37a28ee82030bd1078": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b3fb94d69b5e4f649fb779c4c2ebad92": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b452f3b15b2141798f8e6f0e8b54c82d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b4964a29afd041a6a6e61c71fa9bfee3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b49f22c174f34e6f9a2834b42d26ddda": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b61321499721409bab975e2c13ab1aa5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b7593ff51a4e4c199afb6f509833c591": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b9046cc01eaa42ec8bb44d7c4d76298e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_48f23b89cc7e489887193874080d4216",
       "IPY_MODEL_17e12a31f2e64de1a588af29004bcc8f",
       "IPY_MODEL_ebc89632f52746f5bf00a564d55c1fec"
      ],
      "layout": "IPY_MODEL_697189ee43704e57ba015ff4466432d8"
     }
    },
    "bb34d3b0de404b198c8adfe832b0751c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2f7989bb88e14e239521a89cdab46ff3",
      "placeholder": "",
      "style": "IPY_MODEL_822f6cb310144f8cae38cbe0dd126142",
      "value": "model.safetensors:100%"
     }
    },
    "bcc03a3abf53493ca2be69fd313463b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8f5cc81e85354d08855771a58001210b",
      "placeholder": "",
      "style": "IPY_MODEL_0793cea924de49c99aa39bda433510f4",
      "value": "1.11G/1.11G[00:08&lt;00:00,545MB/s]"
     }
    },
    "bcd507a4c186499c94de78dfb6212696": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b1798c3e869f4f6a821a9192e0786664",
      "placeholder": "",
      "style": "IPY_MODEL_164f378b54af46bd9bd7bc2c48e7f992",
      "value": "added_tokens.json:100%"
     }
    },
    "bd7862a7c229464cbf46b9994331806a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be5051318cba4400ac017516409c2df7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "be68f3d800544720b51e3e1cfc8ed9df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_62980aa1a2f34bc398649d4af689bf2c",
       "IPY_MODEL_c15f1387de0a46ac88aa0a1158ffb857",
       "IPY_MODEL_d30baf8f0e094f359a983cb675f0b758"
      ],
      "layout": "IPY_MODEL_ac795aea753540c1a81cadce33dceba2"
     }
    },
    "bf5a5b26b150489db52645048f1a7685": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bf7ca9bdcb4a40ed8731e2353c66be94": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bff9d52432f2470b8f2fc93404dd641a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "c05117d4662a4315990ff3c411da953e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c15f1387de0a46ac88aa0a1158ffb857": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6c37466dd08c4e069fac4dad0759c44d",
      "max": 791656,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_40fddb9cbf1842638691e9294ab42ed6",
      "value": 791656
     }
    },
    "c2435c0e93234f9cacfb0d87f7f15cfc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_42ae9d039f0a413c94c7980c330940e7",
       "IPY_MODEL_3dfd3f2b375e45b5b0e9f75eb5d7a105",
       "IPY_MODEL_7b6323389024427a8817d365d5307315"
      ],
      "layout": "IPY_MODEL_c05117d4662a4315990ff3c411da953e"
     }
    },
    "c258279eab47410ba683a41f9ca16484": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c35651cd316046f8bd8665f90cbfee76": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c4296ab1bfe04b11a6199c387751859f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_34ac89955fc1450c8f321844cb1289ed",
      "placeholder": "",
      "style": "IPY_MODEL_959f27fbb9b242c284ced60d5d8900b1",
      "value": "81382/81382[00:43&lt;00:00,1235.89examples/s]"
     }
    },
    "c45eada58df445faa5b2c68ec1856cf0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ca29feb83d6b474f94928e70e40b2613",
       "IPY_MODEL_59f0f24f337948e29e64927b99f6ffd1",
       "IPY_MODEL_dc0e2516ea88418fbd23d32ee63d75ee"
      ],
      "layout": "IPY_MODEL_57d5995774f04cd18d4732dfde5c8477"
     }
    },
    "c493c31533a14204a6edc757ce2e0c06": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c5f0c23f7c8e4068b1bce8b650f21e04": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5193b7782e04421585ab62e6525eec97",
       "IPY_MODEL_d3beaf4480444ef98c51800b26f6da85",
       "IPY_MODEL_63123de74eda4806a2141a0592dce588"
      ],
      "layout": "IPY_MODEL_7bcdb3cbc63749649ad531b769605009"
     }
    },
    "c60368f5ad77441bb195ab9291f91d12": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c661b5b6e80a4547b9358620141d356b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9dcfc5d4066045e8958863d716241ac2",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ec98f7abc67a452f869524e15546d462",
      "value": 1
     }
    },
    "c731b5a456874971b3136eb4b7b72902": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c8e21ff9da4b431da32866a33acd8b16": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c8fbb7336c964ac09dc001b25b78d2ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c9a08413736d4689b6fc2391eb4bf7dc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c9bb4c82728743ffbc53ac2534a67582": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "c9cb9441676140aba7849967d2e3f74b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ca29feb83d6b474f94928e70e40b2613": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a5449cf8527b4becaf39aa6feccafd66",
      "placeholder": "",
      "style": "IPY_MODEL_fa0bde3a677b4376b22c6d0fe366198a",
      "value": "Map:100%"
     }
    },
    "cb00b67cd3394338940a4630df671f90": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cb26e43816114201b9b1a744042cf613": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cb458d6e35b34db8950afa2868aca009": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cbc84aacf8034928ad05ae622ad146c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c9bb4c82728743ffbc53ac2534a67582",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_873469a2c0a54c5fad999326c3ca2392",
      "value": 1
     }
    },
    "cc01dbfc108b470ea4808e2ccb768917": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ccb9c16435664590b6cb8f56b4047069": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3a146b212ed84b51bb3b6b978d2a58bf",
       "IPY_MODEL_8d37e37957ce4891a173bde89e6b16f9",
       "IPY_MODEL_2419285ec414421dbbe2a4964b180c70"
      ],
      "layout": "IPY_MODEL_5832ed82d90b49c78f54f3c419f1a28a"
     }
    },
    "ceafd9537463401280fa75e3ad837c9a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cf6568e952234f6cb19318cce74ec5f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d0b1ccbf04c244199b5fb2d68d7baf44": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d1bedec004b84a7e86ba3119365798c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7a729bba8cec41a8939c01e74e3417cb",
      "placeholder": "",
      "style": "IPY_MODEL_97d0135de8b34f138567daf1cc6811c4",
      "value": "23.0/23.0[00:00&lt;00:00,2.97kB/s]"
     }
    },
    "d257d31e3c0c48d096168360ba872a4c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d30baf8f0e094f359a983cb675f0b758": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_06c14377b1624679a17e57248c8be499",
      "placeholder": "",
      "style": "IPY_MODEL_69ebd0e06e7d46d08d8f033c2721eeb0",
      "value": "792k/792k[00:01&lt;00:00,699kB/s]"
     }
    },
    "d3beaf4480444ef98c51800b26f6da85": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dd8f3c2aca8342d7827072147cda6623",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_41c2ab2ac1074d168301ed892df14ace",
      "value": 1
     }
    },
    "d43eec1f7b314b2a8f1e7ca16724512a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d4b78cc5fe9c4353bde17035673b5566": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9ba6b10842e94e9d8ab5876d941883b3",
      "placeholder": "",
      "style": "IPY_MODEL_1c2e7d47491f4bb99a39a85a0dbdd7f5",
      "value": "added_tokens.json:100%"
     }
    },
    "d5769859b0034ce584e147fd3d905855": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d62709daea2c47bcbe9ae05ef0f3a912": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d6cf00fbc50a499cb815a96408666cb1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8e43a9e9a43d4ae29c509701386fb25d",
      "placeholder": "",
      "style": "IPY_MODEL_c731b5a456874971b3136eb4b7b72902",
      "value": "Generatingtrainsplit:"
     }
    },
    "d7aa12e1a6e74a9dad9a67bf614b2084": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_24a7efe1bc0b497380f0df58f1798b54",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f33f5894a82f4771a9fac7723e893678",
      "value": 1
     }
    },
    "d83c7b46881a4f25b9dc9b87eb71a88d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d8c73ca29dca4e3da889198fe0ba7831": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_400bc57d8b8b45e582b19dd974cffb37",
      "placeholder": "",
      "style": "IPY_MODEL_d5769859b0034ce584e147fd3d905855",
      "value": "412/412[00:00&lt;00:00,36.9kB/s]"
     }
    },
    "d9273cbf5fb44a9b997d21876ac65da3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d99a57b6fff049b9b251e2d1268bdf17": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_dc2aced280584650873c1767466a306d",
       "IPY_MODEL_de5e40fa137049859b884f895c0aea06",
       "IPY_MODEL_623a9670e45c44f0abc2c8342aaaf163"
      ],
      "layout": "IPY_MODEL_5b75a88a94d64f28a6fc464dd7322b80"
     }
    },
    "dc0e2516ea88418fbd23d32ee63d75ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_16a72e2157084a3d9eb25dc2a3bc0c5b",
      "placeholder": "",
      "style": "IPY_MODEL_7ac5008f255641dc8fc5c26a752467b3",
      "value": "1000/1000[00:00&lt;00:00,5315.87examples/s]"
     }
    },
    "dc2aced280584650873c1767466a306d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f576072157ef4ba799866cd60bc03b02",
      "placeholder": "",
      "style": "IPY_MODEL_c8fbb7336c964ac09dc001b25b78d2ce",
      "value": "special_tokens_map.json:100%"
     }
    },
    "dc96283bafc944459fdc2a9185bae7e4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dd8f3c2aca8342d7827072147cda6623": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "de5e40fa137049859b884f895c0aea06": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5c3e40bb909e4b309276b9bc235e1542",
      "max": 173,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8be499a3ae1f42e483aa0b2add2b00a0",
      "value": 173
     }
    },
    "e3efd86044c74dcd8edd9932c9f64a2b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e6033e5ffd4f43dd8906015aabf5657c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e6613567a67d487a8185cb8339fa7718": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e6eecdb0210e4e6cb9a3b2cbe52d462b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e82c551cc4b942d9ae77af6ac63559ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e848280153794184978daacded1e5327": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e8874c17b6d04f8aa14ea19d2c78d155": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e896481db0674ebd8ea69c17b6c1f8a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e8dd4ee075934a4c838805c792b8388d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e8ed2480d9d143e08df54d71cf56f4c7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e9002886b5254eb786a2265da8590096": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e927bb1c3d13419095849a8d66263edc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ea684ce24bf14bbdade11ca65a3c763a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d6cf00fbc50a499cb815a96408666cb1",
       "IPY_MODEL_cbc84aacf8034928ad05ae622ad146c2",
       "IPY_MODEL_38ce96b9dbe94ff7a8295dffca552ff3"
      ],
      "layout": "IPY_MODEL_8bd0b3b56c404af9893293abbe91e6ac"
     }
    },
    "ebc89632f52746f5bf00a564d55c1fec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dc96283bafc944459fdc2a9185bae7e4",
      "placeholder": "",
      "style": "IPY_MODEL_f5ccc9ce200644979bd28f59108e9f62",
      "value": "8124/8124[00:01&lt;00:00,7627.66examples/s]"
     }
    },
    "ec2991ffa0b2447ebef695409e3bbb06": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c60368f5ad77441bb195ab9291f91d12",
      "placeholder": "",
      "style": "IPY_MODEL_87c3f45b89ed46758dfc338d2403724d",
      "value": "8124/8124[00:01&lt;00:00,7883.60examples/s]"
     }
    },
    "ec66a4eda9774f80be9c716ab6a282b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_50fa33f38aa64171aa7584c2835ffdce",
      "placeholder": "",
      "style": "IPY_MODEL_d43eec1f7b314b2a8f1e7ca16724512a",
      "value": "tokenizer.json:"
     }
    },
    "ec7a896e91c64eb19fb025c51bd52aee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ec98f7abc67a452f869524e15546d462": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ecab88c53400495aa431a50d4106ebfe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7d6038359c2f4a6599d63688f1e07c38",
      "placeholder": "",
      "style": "IPY_MODEL_ae456161b1f04b11a1bd48765661e93c",
      "value": "81382/0[00:00&lt;00:00,176227.14examples/s]"
     }
    },
    "edaac1fa6a894c0eb48cbf808736245d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ef1477585cf8432ea25696a87cf9612c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f071980ea3c948a6b0a14054d8929b1b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e8ed2480d9d143e08df54d71cf56f4c7",
      "placeholder": "",
      "style": "IPY_MODEL_fa02292d27cb4d5d9415b8332c6d7402",
      "value": "tokenizer.json:"
     }
    },
    "f09aa5b5ebe4413cab9f4038e16a6798": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f09b46772aca4927a71c74439a6bbdf0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f2da3245ca7348ad9bba5b6cdc2a8a74": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_11ea0f8322594bdaa4ef72c03e6ded87",
      "max": 23,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e927bb1c3d13419095849a8d66263edc",
      "value": 23
     }
    },
    "f33f5894a82f4771a9fac7723e893678": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f3f8ea85f0654d33b73de468a9bbef17": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f4394ab82f354fa4a9bbacfd160e8cb7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f576072157ef4ba799866cd60bc03b02": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f5ccc9ce200644979bd28f59108e9f62": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f5da34916279459bad312762d416c9c4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f5de05e112814202944d4e9106be2083": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5eba54199b7b46038fb1a6008c3c77fc",
       "IPY_MODEL_89ba717e73134964aa5359b627f40711",
       "IPY_MODEL_ecab88c53400495aa431a50d4106ebfe"
      ],
      "layout": "IPY_MODEL_517217c05785432f8edfa4220b2623c7"
     }
    },
    "f65df6ae10ab46c8895b6e033cc0a3e9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f772240ae2404a66882307e96f9cc286": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f78c5037438a4940b72dd463f2f97cc2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_881a33352aa7466e97cd9071633fa330",
      "placeholder": "",
      "style": "IPY_MODEL_6c3aaa63ac234eed91b8bbfc47340fe7",
      "value": "2.42M/?[00:00&lt;00:00,6.88MB/s]"
     }
    },
    "fa02292d27cb4d5d9415b8332c6d7402": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fa0bde3a677b4376b22c6d0fe366198a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fabef84864ed4788b5994aaf4d1542b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bb34d3b0de404b198c8adfe832b0751c",
       "IPY_MODEL_ff644624b03e44c3ad78d416d6ced567",
       "IPY_MODEL_bcc03a3abf53493ca2be69fd313463b4"
      ],
      "layout": "IPY_MODEL_8229cf259a47476e8a98e558b80e5d9e"
     }
    },
    "fb1a8c27574d4fa0ba775aaab5688d98": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fc2afb9f5b04420e8c4caecc501bdfea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fc98284225944bba8f0238b27731307c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_84334b32d531485baa04537cdcc837fa",
       "IPY_MODEL_2e6316c662bd428f9b733cbfe5aa4b9c",
       "IPY_MODEL_19722962d598405482928975278cad5b"
      ],
      "layout": "IPY_MODEL_ceafd9537463401280fa75e3ad837c9a"
     }
    },
    "fd979112742e49c590519bf88a3700e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b49f22c174f34e6f9a2834b42d26ddda",
      "placeholder": "",
      "style": "IPY_MODEL_5890eeddb84141e584a051355005f5df",
      "value": "model.safetensors:100%"
     }
    },
    "feca801cb60a45e08d1183d6722ab72f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e6eecdb0210e4e6cb9a3b2cbe52d462b",
      "placeholder": "",
      "style": "IPY_MODEL_d257d31e3c0c48d096168360ba872a4c",
      "value": "2.42M/?[00:00&lt;00:00,49.8MB/s]"
     }
    },
    "ff3c8da9641f49678a34289a9923f4a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b61321499721409bab975e2c13ab1aa5",
      "max": 81382,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cf6568e952234f6cb19318cce74ec5f8",
      "value": 81382
     }
    },
    "ff644624b03e44c3ad78d416d6ced567": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cc01dbfc108b470ea4808e2ccb768917",
      "max": 1112513736,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e8874c17b6d04f8aa14ea19d2c78d155",
      "value": 1112513736
     }
    },
    "ffa2127ddd7e47e4ab92d3fe7345903d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
